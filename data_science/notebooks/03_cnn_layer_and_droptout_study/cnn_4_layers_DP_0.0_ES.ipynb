{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import warnings\r\n",
    "import os\r\n",
    "import cnn_loop\r\n",
    "\r\n",
    "from processing import preprocessing\r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 of data loaded\n"
     ]
    }
   ],
   "source": [
    "data_df = preprocessing.load_dataset(num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('/datasets/UrbanSound8K/processed/mean_mfcc_data.json')\n",
    "data_df = preprocessing.filter_mfccs(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocessing.create_training_data(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5861, 40, 173, 1)\n",
      "X_test shape: (1466, 40, 173, 1)\n",
      "y_train shape: (5861, 10)\n",
      "y_test shape: (1466, 10)\n",
      "num_outputs:  10\n"
     ]
    }
   ],
   "source": [
    "num_outputs = data_df['label'].unique().shape[0]  # labels = 10\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"num_outputs: \", num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_loop.CNN(num_outputs, num_models=4, DP_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f18d8006240>,\n",
       " 'model_2': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f18d7ff0748>,\n",
       " 'model_3': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f18d806db70>,\n",
       " 'model_4': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f18d806c5c0>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUMMARY FOR MODEL  1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 10.7319 - accuracy: 0.1248\n",
      "Pre-training accuracy: 12.4829%\n",
      "\n",
      " SUMMARY FOR MODEL  2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 2,490\n",
      "Trainable params: 2,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 11.3253 - accuracy: 0.1119\n",
      "Pre-training accuracy: 11.1869%\n",
      "\n",
      " SUMMARY FOR MODEL  3\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 11,066\n",
      "Trainable params: 11,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 6.5330 - accuracy: 0.1241\n",
      "Pre-training accuracy: 12.4147%\n",
      "\n",
      " SUMMARY FOR MODEL  4\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 19, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 7.2901 - accuracy: 0.0750\n",
      "Pre-training accuracy: 7.5034%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.initialize(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for model  1  has started.\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 7.7919 - accuracy: 0.1259\n",
      "Epoch 00001: val_loss improved from inf to 5.22198, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 7.7919 - accuracy: 0.1259 - val_loss: 5.2220 - val_accuracy: 0.1132\n",
      "Epoch 2/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 4.3604 - accuracy: 0.1252\n",
      "Epoch 00002: val_loss improved from 5.22198 to 3.48068, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 4.2030 - accuracy: 0.1218 - val_loss: 3.4807 - val_accuracy: 0.1201\n",
      "Epoch 3/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 3.1025 - accuracy: 0.1109\n",
      "Epoch 00003: val_loss improved from 3.48068 to 2.72698, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.0595 - accuracy: 0.1135 - val_loss: 2.7270 - val_accuracy: 0.1289\n",
      "Epoch 4/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.6104 - accuracy: 0.1275\n",
      "Epoch 00004: val_loss improved from 2.72698 to 2.44191, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.5837 - accuracy: 0.1256 - val_loss: 2.4419 - val_accuracy: 0.1153\n",
      "Epoch 5/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.3855 - accuracy: 0.1112\n",
      "Epoch 00005: val_loss improved from 2.44191 to 2.33030, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.3826 - accuracy: 0.1089 - val_loss: 2.3303 - val_accuracy: 0.1105\n",
      "Epoch 6/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.2756 - accuracy: 0.1281\n",
      "Epoch 00006: val_loss improved from 2.33030 to 2.24719, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.2713 - accuracy: 0.1310 - val_loss: 2.2472 - val_accuracy: 0.1364\n",
      "Epoch 7/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.2008 - accuracy: 0.1571\n",
      "Epoch 00007: val_loss improved from 2.24719 to 2.18711, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.1903 - accuracy: 0.1570 - val_loss: 2.1871 - val_accuracy: 0.1419\n",
      "Epoch 8/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.1378 - accuracy: 0.1719\n",
      "Epoch 00008: val_loss improved from 2.18711 to 2.13886, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.1282 - accuracy: 0.1754 - val_loss: 2.1389 - val_accuracy: 0.1555\n",
      "Epoch 9/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.0792 - accuracy: 0.1830\n",
      "Epoch 00009: val_loss improved from 2.13886 to 2.09917, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.0808 - accuracy: 0.1844 - val_loss: 2.0992 - val_accuracy: 0.1719\n",
      "Epoch 10/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 2.0483 - accuracy: 0.2027\n",
      "Epoch 00010: val_loss improved from 2.09917 to 2.06575, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.0407 - accuracy: 0.2037 - val_loss: 2.0658 - val_accuracy: 0.1869\n",
      "Epoch 11/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 2.0129 - accuracy: 0.2266\n",
      "Epoch 00011: val_loss improved from 2.06575 to 2.03348, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.0069 - accuracy: 0.2268 - val_loss: 2.0335 - val_accuracy: 0.2026\n",
      "Epoch 12/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.9787 - accuracy: 0.2420\n",
      "Epoch 00012: val_loss improved from 2.03348 to 2.00684, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9771 - accuracy: 0.2394 - val_loss: 2.0068 - val_accuracy: 0.2265\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9501 - accuracy: 0.2576\n",
      "Epoch 00013: val_loss improved from 2.00684 to 1.98185, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9501 - accuracy: 0.2576 - val_loss: 1.9819 - val_accuracy: 0.2333\n",
      "Epoch 14/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.9361 - accuracy: 0.2671\n",
      "Epoch 00014: val_loss improved from 1.98185 to 1.95905, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9258 - accuracy: 0.2665 - val_loss: 1.9590 - val_accuracy: 0.2422\n",
      "Epoch 15/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.9049 - accuracy: 0.2728\n",
      "Epoch 00015: val_loss improved from 1.95905 to 1.93714, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9046 - accuracy: 0.2749 - val_loss: 1.9371 - val_accuracy: 0.2442\n",
      "Epoch 16/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.8868 - accuracy: 0.2834\n",
      "Epoch 00016: val_loss improved from 1.93714 to 1.92227, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8839 - accuracy: 0.2873 - val_loss: 1.9223 - val_accuracy: 0.2694\n",
      "Epoch 17/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.8645 - accuracy: 0.3059\n",
      "Epoch 00017: val_loss improved from 1.92227 to 1.90303, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.8674 - accuracy: 0.3034 - val_loss: 1.9030 - val_accuracy: 0.2619\n",
      "Epoch 18/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.8435 - accuracy: 0.3035\n",
      "Epoch 00018: val_loss improved from 1.90303 to 1.89376, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8520 - accuracy: 0.3035 - val_loss: 1.8938 - val_accuracy: 0.2797\n",
      "Epoch 19/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.8381 - accuracy: 0.3145\n",
      "Epoch 00019: val_loss improved from 1.89376 to 1.87695, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8391 - accuracy: 0.3112 - val_loss: 1.8769 - val_accuracy: 0.2742\n",
      "Epoch 20/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.8275 - accuracy: 0.3250\n",
      "Epoch 00020: val_loss improved from 1.87695 to 1.86836, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8298 - accuracy: 0.3213 - val_loss: 1.8684 - val_accuracy: 0.2769\n",
      "Epoch 21/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.8147 - accuracy: 0.3265\n",
      "Epoch 00021: val_loss improved from 1.86836 to 1.85877, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8171 - accuracy: 0.3250 - val_loss: 1.8588 - val_accuracy: 0.2858\n",
      "Epoch 22/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7999 - accuracy: 0.3316\n",
      "Epoch 00022: val_loss improved from 1.85877 to 1.85014, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8082 - accuracy: 0.3319 - val_loss: 1.8501 - val_accuracy: 0.2981\n",
      "Epoch 23/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.8063 - accuracy: 0.3296\n",
      "Epoch 00023: val_loss improved from 1.85014 to 1.84051, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8014 - accuracy: 0.3322 - val_loss: 1.8405 - val_accuracy: 0.2947\n",
      "Epoch 24/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7938 - accuracy: 0.3343\n",
      "Epoch 00024: val_loss improved from 1.84051 to 1.83959, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7931 - accuracy: 0.3399 - val_loss: 1.8396 - val_accuracy: 0.3049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7885 - accuracy: 0.3417\n",
      "Epoch 00025: val_loss improved from 1.83959 to 1.83058, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7871 - accuracy: 0.3419 - val_loss: 1.8306 - val_accuracy: 0.2988\n",
      "Epoch 26/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7698 - accuracy: 0.3451\n",
      "Epoch 00026: val_loss improved from 1.83058 to 1.82465, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7813 - accuracy: 0.3450 - val_loss: 1.8246 - val_accuracy: 0.3090\n",
      "Epoch 27/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7723 - accuracy: 0.3491\n",
      "Epoch 00027: val_loss improved from 1.82465 to 1.81955, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7757 - accuracy: 0.3487 - val_loss: 1.8196 - val_accuracy: 0.3165\n",
      "Epoch 28/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7701 - accuracy: 0.3509\n",
      "Epoch 00028: val_loss improved from 1.81955 to 1.81571, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7722 - accuracy: 0.3503 - val_loss: 1.8157 - val_accuracy: 0.3090\n",
      "Epoch 29/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7711 - accuracy: 0.3479\n",
      "Epoch 00029: val_loss improved from 1.81571 to 1.81215, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7679 - accuracy: 0.3445 - val_loss: 1.8122 - val_accuracy: 0.3138\n",
      "Epoch 30/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7642 - accuracy: 0.3546\n",
      "Epoch 00030: val_loss improved from 1.81215 to 1.80862, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7626 - accuracy: 0.3556 - val_loss: 1.8086 - val_accuracy: 0.3090\n",
      "Epoch 31/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7589 - accuracy: 0.3659\n",
      "Epoch 00031: val_loss improved from 1.80862 to 1.80343, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7594 - accuracy: 0.3590 - val_loss: 1.8034 - val_accuracy: 0.3247\n",
      "Epoch 32/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7577 - accuracy: 0.3588\n",
      "Epoch 00032: val_loss improved from 1.80343 to 1.80087, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7539 - accuracy: 0.3571 - val_loss: 1.8009 - val_accuracy: 0.3186\n",
      "Epoch 33/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7589 - accuracy: 0.3600\n",
      "Epoch 00033: val_loss improved from 1.80087 to 1.79571, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7527 - accuracy: 0.3617 - val_loss: 1.7957 - val_accuracy: 0.3281\n",
      "Epoch 34/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7466 - accuracy: 0.3713\n",
      "Epoch 00034: val_loss improved from 1.79571 to 1.79163, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7487 - accuracy: 0.3679 - val_loss: 1.7916 - val_accuracy: 0.3377\n",
      "Epoch 35/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7495 - accuracy: 0.3571\n",
      "Epoch 00035: val_loss did not improve from 1.79163\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7463 - accuracy: 0.3581 - val_loss: 1.7939 - val_accuracy: 0.3295\n",
      "Epoch 36/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7494 - accuracy: 0.3604\n",
      "Epoch 00036: val_loss improved from 1.79163 to 1.78955, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7444 - accuracy: 0.3598 - val_loss: 1.7895 - val_accuracy: 0.3349\n",
      "Epoch 37/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7433 - accuracy: 0.3610\n",
      "Epoch 00037: val_loss improved from 1.78955 to 1.78724, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7399 - accuracy: 0.3643 - val_loss: 1.7872 - val_accuracy: 0.3377\n",
      "Epoch 38/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.7470 - accuracy: 0.3643\n",
      "Epoch 00038: val_loss improved from 1.78724 to 1.78594, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7380 - accuracy: 0.3694 - val_loss: 1.7859 - val_accuracy: 0.3452\n",
      "Epoch 39/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7391 - accuracy: 0.3651\n",
      "Epoch 00039: val_loss improved from 1.78594 to 1.78318, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7357 - accuracy: 0.3679 - val_loss: 1.7832 - val_accuracy: 0.3377\n",
      "Epoch 40/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7237 - accuracy: 0.3666\n",
      "Epoch 00040: val_loss improved from 1.78318 to 1.77993, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7325 - accuracy: 0.3634 - val_loss: 1.7799 - val_accuracy: 0.3274\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7305 - accuracy: 0.3667\n",
      "Epoch 00041: val_loss improved from 1.77993 to 1.77879, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7305 - accuracy: 0.3667 - val_loss: 1.7788 - val_accuracy: 0.3288\n",
      "Epoch 42/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7346 - accuracy: 0.3685\n",
      "Epoch 00042: val_loss improved from 1.77879 to 1.77725, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7295 - accuracy: 0.3697 - val_loss: 1.7773 - val_accuracy: 0.3322\n",
      "Epoch 43/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7278 - accuracy: 0.3670\n",
      "Epoch 00043: val_loss improved from 1.77725 to 1.77186, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7259 - accuracy: 0.3684 - val_loss: 1.7719 - val_accuracy: 0.3411\n",
      "Epoch 44/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7398 - accuracy: 0.3618\n",
      "Epoch 00044: val_loss did not improve from 1.77186\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7255 - accuracy: 0.3655 - val_loss: 1.7736 - val_accuracy: 0.3363\n",
      "Epoch 45/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7223 - accuracy: 0.3684\n",
      "Epoch 00045: val_loss improved from 1.77186 to 1.77012, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7227 - accuracy: 0.3723 - val_loss: 1.7701 - val_accuracy: 0.3465\n",
      "Epoch 46/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7231 - accuracy: 0.3688\n",
      "Epoch 00046: val_loss improved from 1.77012 to 1.76918, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7221 - accuracy: 0.3696 - val_loss: 1.7692 - val_accuracy: 0.3417\n",
      "Epoch 47/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7199 - accuracy: 0.3717\n",
      "Epoch 00047: val_loss improved from 1.76918 to 1.76601, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7199 - accuracy: 0.3713 - val_loss: 1.7660 - val_accuracy: 0.3513\n",
      "Epoch 48/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7135 - accuracy: 0.3694\n",
      "Epoch 00048: val_loss improved from 1.76601 to 1.76523, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7188 - accuracy: 0.3706 - val_loss: 1.7652 - val_accuracy: 0.3431\n",
      "Epoch 49/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7078 - accuracy: 0.3711\n",
      "Epoch 00049: val_loss improved from 1.76523 to 1.76361, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7169 - accuracy: 0.3689 - val_loss: 1.7636 - val_accuracy: 0.3397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7157 - accuracy: 0.3577\n",
      "Epoch 00050: val_loss improved from 1.76361 to 1.76039, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7163 - accuracy: 0.3622 - val_loss: 1.7604 - val_accuracy: 0.3363\n",
      "Epoch 51/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7119 - accuracy: 0.3674\n",
      "Epoch 00051: val_loss did not improve from 1.76039\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7149 - accuracy: 0.3692 - val_loss: 1.7649 - val_accuracy: 0.3424\n",
      "Epoch 52/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7091 - accuracy: 0.3723\n",
      "Epoch 00052: val_loss improved from 1.76039 to 1.75897, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7164 - accuracy: 0.3692 - val_loss: 1.7590 - val_accuracy: 0.3431\n",
      "Epoch 53/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7202 - accuracy: 0.3674\n",
      "Epoch 00053: val_loss did not improve from 1.75897\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7110 - accuracy: 0.3731 - val_loss: 1.7604 - val_accuracy: 0.3458\n",
      "Epoch 54/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7142 - accuracy: 0.3680\n",
      "Epoch 00054: val_loss did not improve from 1.75897\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7101 - accuracy: 0.3704 - val_loss: 1.7635 - val_accuracy: 0.3377\n",
      "Epoch 55/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7096 - accuracy: 0.3756\n",
      "Epoch 00055: val_loss improved from 1.75897 to 1.75887, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7092 - accuracy: 0.3742 - val_loss: 1.7589 - val_accuracy: 0.3540\n",
      "Epoch 56/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7113 - accuracy: 0.3743\n",
      "Epoch 00056: val_loss improved from 1.75887 to 1.75195, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7078 - accuracy: 0.3743 - val_loss: 1.7520 - val_accuracy: 0.3472\n",
      "Epoch 57/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7067 - accuracy: 0.3822\n",
      "Epoch 00057: val_loss did not improve from 1.75195\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7066 - accuracy: 0.3796 - val_loss: 1.7532 - val_accuracy: 0.3431\n",
      "Epoch 58/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7029 - accuracy: 0.3738\n",
      "Epoch 00058: val_loss did not improve from 1.75195\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7051 - accuracy: 0.3743 - val_loss: 1.7552 - val_accuracy: 0.3445\n",
      "Epoch 59/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6991 - accuracy: 0.3785\n",
      "Epoch 00059: val_loss improved from 1.75195 to 1.74832, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7036 - accuracy: 0.3743 - val_loss: 1.7483 - val_accuracy: 0.3561\n",
      "Epoch 60/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7050 - accuracy: 0.3757\n",
      "Epoch 00060: val_loss did not improve from 1.74832\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7009 - accuracy: 0.3779 - val_loss: 1.7520 - val_accuracy: 0.3513\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7011 - accuracy: 0.3769\n",
      "Epoch 00061: val_loss improved from 1.74832 to 1.74550, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7011 - accuracy: 0.3769 - val_loss: 1.7455 - val_accuracy: 0.3554\n",
      "Epoch 62/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7040 - accuracy: 0.3758\n",
      "Epoch 00062: val_loss did not improve from 1.74550\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6992 - accuracy: 0.3766 - val_loss: 1.7485 - val_accuracy: 0.3458\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6990 - accuracy: 0.3749\n",
      "Epoch 00063: val_loss improved from 1.74550 to 1.74266, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6990 - accuracy: 0.3749 - val_loss: 1.7427 - val_accuracy: 0.3540\n",
      "Epoch 64/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6991 - accuracy: 0.3780\n",
      "Epoch 00064: val_loss did not improve from 1.74266\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6970 - accuracy: 0.3784 - val_loss: 1.7440 - val_accuracy: 0.3520\n",
      "Epoch 65/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6944 - accuracy: 0.3826\n",
      "Epoch 00065: val_loss improved from 1.74266 to 1.74110, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6954 - accuracy: 0.3849 - val_loss: 1.7411 - val_accuracy: 0.3499\n",
      "Epoch 66/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6971 - accuracy: 0.3830\n",
      "Epoch 00066: val_loss did not improve from 1.74110\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6959 - accuracy: 0.3827 - val_loss: 1.7431 - val_accuracy: 0.3513\n",
      "Epoch 67/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7034 - accuracy: 0.3793\n",
      "Epoch 00067: val_loss improved from 1.74110 to 1.73821, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6939 - accuracy: 0.3824 - val_loss: 1.7382 - val_accuracy: 0.3649\n",
      "Epoch 68/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6905 - accuracy: 0.3853\n",
      "Epoch 00068: val_loss did not improve from 1.73821\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6920 - accuracy: 0.3839 - val_loss: 1.7411 - val_accuracy: 0.3636\n",
      "Epoch 69/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6916 - accuracy: 0.3808\n",
      "Epoch 00069: val_loss did not improve from 1.73821\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6928 - accuracy: 0.3820 - val_loss: 1.7386 - val_accuracy: 0.3465\n",
      "Epoch 70/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6924 - accuracy: 0.3830\n",
      "Epoch 00070: val_loss did not improve from 1.73821\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6900 - accuracy: 0.3844 - val_loss: 1.7427 - val_accuracy: 0.3458\n",
      "Epoch 71/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6894 - accuracy: 0.3824\n",
      "Epoch 00071: val_loss improved from 1.73821 to 1.73477, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6904 - accuracy: 0.3791 - val_loss: 1.7348 - val_accuracy: 0.3506\n",
      "Epoch 72/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6909 - accuracy: 0.3828\n",
      "Epoch 00072: val_loss improved from 1.73477 to 1.73401, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6862 - accuracy: 0.3837 - val_loss: 1.7340 - val_accuracy: 0.3547\n",
      "Epoch 73/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6950 - accuracy: 0.3859\n",
      "Epoch 00073: val_loss did not improve from 1.73401\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6885 - accuracy: 0.3875 - val_loss: 1.7372 - val_accuracy: 0.3595\n",
      "Epoch 74/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.6776 - accuracy: 0.3928\n",
      "Epoch 00074: val_loss improved from 1.73401 to 1.73153, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6834 - accuracy: 0.3883 - val_loss: 1.7315 - val_accuracy: 0.3595\n",
      "Epoch 75/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6790 - accuracy: 0.3881\n",
      "Epoch 00075: val_loss did not improve from 1.73153\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6822 - accuracy: 0.3875 - val_loss: 1.7329 - val_accuracy: 0.3554\n",
      "Epoch 76/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6807 - accuracy: 0.3877\n",
      "Epoch 00076: val_loss improved from 1.73153 to 1.72778, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6826 - accuracy: 0.3808 - val_loss: 1.7278 - val_accuracy: 0.3588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.6840 - accuracy: 0.3904\n",
      "Epoch 00077: val_loss did not improve from 1.72778\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6817 - accuracy: 0.3916 - val_loss: 1.7295 - val_accuracy: 0.3554\n",
      "Epoch 78/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6819 - accuracy: 0.3877\n",
      "Epoch 00078: val_loss did not improve from 1.72778\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6816 - accuracy: 0.3904 - val_loss: 1.7292 - val_accuracy: 0.3690\n",
      "Epoch 79/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6835 - accuracy: 0.3915\n",
      "Epoch 00079: val_loss improved from 1.72778 to 1.72649, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6818 - accuracy: 0.3892 - val_loss: 1.7265 - val_accuracy: 0.3554\n",
      "Epoch 80/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6782 - accuracy: 0.3836\n",
      "Epoch 00080: val_loss improved from 1.72649 to 1.72326, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6770 - accuracy: 0.3854 - val_loss: 1.7233 - val_accuracy: 0.3622\n",
      "Epoch 81/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6776 - accuracy: 0.3958\n",
      "Epoch 00081: val_loss did not improve from 1.72326\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6785 - accuracy: 0.3938 - val_loss: 1.7254 - val_accuracy: 0.3643\n",
      "Epoch 82/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6734 - accuracy: 0.3941\n",
      "Epoch 00082: val_loss improved from 1.72326 to 1.71973, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6768 - accuracy: 0.3924 - val_loss: 1.7197 - val_accuracy: 0.3663\n",
      "Epoch 83/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6797 - accuracy: 0.3877\n",
      "Epoch 00083: val_loss did not improve from 1.71973\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6742 - accuracy: 0.3921 - val_loss: 1.7204 - val_accuracy: 0.3595\n",
      "Epoch 84/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6715 - accuracy: 0.3962\n",
      "Epoch 00084: val_loss improved from 1.71973 to 1.71886, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6719 - accuracy: 0.3952 - val_loss: 1.7189 - val_accuracy: 0.3656\n",
      "Epoch 85/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6679 - accuracy: 0.3962\n",
      "Epoch 00085: val_loss did not improve from 1.71886\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6689 - accuracy: 0.3958 - val_loss: 1.7200 - val_accuracy: 0.3718\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6705 - accuracy: 0.3916\n",
      "Epoch 00086: val_loss did not improve from 1.71886\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6705 - accuracy: 0.3916 - val_loss: 1.7223 - val_accuracy: 0.3649\n",
      "Epoch 87/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6742 - accuracy: 0.3976\n",
      "Epoch 00087: val_loss improved from 1.71886 to 1.71669, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6681 - accuracy: 0.4013 - val_loss: 1.7167 - val_accuracy: 0.3615\n",
      "Epoch 88/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.6659 - accuracy: 0.3973\n",
      "Epoch 00088: val_loss improved from 1.71669 to 1.71516, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6659 - accuracy: 0.3989 - val_loss: 1.7152 - val_accuracy: 0.3574\n",
      "Epoch 89/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6732 - accuracy: 0.3921\n",
      "Epoch 00089: val_loss improved from 1.71516 to 1.71208, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6666 - accuracy: 0.3934 - val_loss: 1.7121 - val_accuracy: 0.3608\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6628 - accuracy: 0.3953\n",
      "Epoch 00090: val_loss improved from 1.71208 to 1.71093, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6628 - accuracy: 0.3953 - val_loss: 1.7109 - val_accuracy: 0.3861\n",
      "Epoch 91/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6609 - accuracy: 0.3972\n",
      "Epoch 00091: val_loss improved from 1.71093 to 1.70890, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6641 - accuracy: 0.3977 - val_loss: 1.7089 - val_accuracy: 0.3840\n",
      "Epoch 92/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6630 - accuracy: 0.4023\n",
      "Epoch 00092: val_loss did not improve from 1.70890\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6626 - accuracy: 0.3999 - val_loss: 1.7157 - val_accuracy: 0.3472\n",
      "Epoch 93/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6596 - accuracy: 0.3970\n",
      "Epoch 00093: val_loss did not improve from 1.70890\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6613 - accuracy: 0.3967 - val_loss: 1.7103 - val_accuracy: 0.3683\n",
      "Epoch 94/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6614 - accuracy: 0.3972\n",
      "Epoch 00094: val_loss improved from 1.70890 to 1.70351, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6604 - accuracy: 0.3965 - val_loss: 1.7035 - val_accuracy: 0.3683\n",
      "Epoch 95/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6549 - accuracy: 0.4015\n",
      "Epoch 00095: val_loss improved from 1.70351 to 1.70234, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6573 - accuracy: 0.3996 - val_loss: 1.7023 - val_accuracy: 0.3834\n",
      "Epoch 96/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6537 - accuracy: 0.4087\n",
      "Epoch 00096: val_loss did not improve from 1.70234\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6580 - accuracy: 0.4042 - val_loss: 1.7042 - val_accuracy: 0.3608\n",
      "Epoch 97/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6504 - accuracy: 0.4036\n",
      "Epoch 00097: val_loss did not improve from 1.70234\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6540 - accuracy: 0.4033 - val_loss: 1.7085 - val_accuracy: 0.3745\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6521 - accuracy: 0.4023\n",
      "Epoch 00098: val_loss did not improve from 1.70234\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6521 - accuracy: 0.4023 - val_loss: 1.7025 - val_accuracy: 0.3649\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6522 - accuracy: 0.4074\n",
      "Epoch 00099: val_loss improved from 1.70234 to 1.69978, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6522 - accuracy: 0.4074 - val_loss: 1.6998 - val_accuracy: 0.3704\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6495 - accuracy: 0.4047\n",
      "Epoch 00100: val_loss improved from 1.69978 to 1.69702, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6495 - accuracy: 0.4047 - val_loss: 1.6970 - val_accuracy: 0.3608\n",
      "Epoch 101/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6528 - accuracy: 0.3993\n",
      "Epoch 00101: val_loss did not improve from 1.69702\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6490 - accuracy: 0.3999 - val_loss: 1.6982 - val_accuracy: 0.3595\n",
      "Epoch 102/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6438 - accuracy: 0.4130\n",
      "Epoch 00102: val_loss improved from 1.69702 to 1.69046, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6476 - accuracy: 0.4064 - val_loss: 1.6905 - val_accuracy: 0.3697\n",
      "Epoch 103/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6443 - accuracy: 0.4073\n",
      "Epoch 00103: val_loss did not improve from 1.69046\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6482 - accuracy: 0.4051 - val_loss: 1.6911 - val_accuracy: 0.3868\n",
      "Epoch 104/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6491 - accuracy: 0.4046\n",
      "Epoch 00104: val_loss improved from 1.69046 to 1.68857, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6450 - accuracy: 0.4062 - val_loss: 1.6886 - val_accuracy: 0.3738\n",
      "Epoch 105/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6456 - accuracy: 0.4019\n",
      "Epoch 00105: val_loss improved from 1.68857 to 1.68652, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6425 - accuracy: 0.4090 - val_loss: 1.6865 - val_accuracy: 0.3806\n",
      "Epoch 106/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6570 - accuracy: 0.4058\n",
      "Epoch 00106: val_loss improved from 1.68652 to 1.68594, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6406 - accuracy: 0.4100 - val_loss: 1.6859 - val_accuracy: 0.3895\n",
      "Epoch 107/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6411 - accuracy: 0.4102\n",
      "Epoch 00107: val_loss improved from 1.68594 to 1.68459, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6393 - accuracy: 0.4097 - val_loss: 1.6846 - val_accuracy: 0.3929\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6363 - accuracy: 0.4102\n",
      "Epoch 00108: val_loss did not improve from 1.68459\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6363 - accuracy: 0.4102 - val_loss: 1.6861 - val_accuracy: 0.3840\n",
      "Epoch 109/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6362 - accuracy: 0.4164\n",
      "Epoch 00109: val_loss improved from 1.68459 to 1.68396, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6362 - accuracy: 0.4112 - val_loss: 1.6840 - val_accuracy: 0.3799\n",
      "Epoch 110/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.6348 - accuracy: 0.4104\n",
      "Epoch 00110: val_loss improved from 1.68396 to 1.68303, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6342 - accuracy: 0.4144 - val_loss: 1.6830 - val_accuracy: 0.3861\n",
      "Epoch 111/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6372 - accuracy: 0.4190\n",
      "Epoch 00111: val_loss did not improve from 1.68303\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6329 - accuracy: 0.4170 - val_loss: 1.6869 - val_accuracy: 0.3724\n",
      "Epoch 112/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6222 - accuracy: 0.4137\n",
      "Epoch 00112: val_loss improved from 1.68303 to 1.68181, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6344 - accuracy: 0.4110 - val_loss: 1.6818 - val_accuracy: 0.3759\n",
      "Epoch 113/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6315 - accuracy: 0.4225\n",
      "Epoch 00113: val_loss did not improve from 1.68181\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6313 - accuracy: 0.4178 - val_loss: 1.6837 - val_accuracy: 0.3772\n",
      "Epoch 114/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6269 - accuracy: 0.4156\n",
      "Epoch 00114: val_loss improved from 1.68181 to 1.67887, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6291 - accuracy: 0.4189 - val_loss: 1.6789 - val_accuracy: 0.3922\n",
      "Epoch 115/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6383 - accuracy: 0.4118\n",
      "Epoch 00115: val_loss improved from 1.67887 to 1.67680, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6283 - accuracy: 0.4160 - val_loss: 1.6768 - val_accuracy: 0.3929\n",
      "Epoch 116/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6173 - accuracy: 0.4254\n",
      "Epoch 00116: val_loss improved from 1.67680 to 1.67537, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6270 - accuracy: 0.4196 - val_loss: 1.6754 - val_accuracy: 0.3793\n",
      "Epoch 117/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.6268 - accuracy: 0.4207\n",
      "Epoch 00117: val_loss did not improve from 1.67537\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6303 - accuracy: 0.4163 - val_loss: 1.6795 - val_accuracy: 0.3670\n",
      "Epoch 118/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6314 - accuracy: 0.4143\n",
      "Epoch 00118: val_loss improved from 1.67537 to 1.67239, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6250 - accuracy: 0.4173 - val_loss: 1.6724 - val_accuracy: 0.3759\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6245 - accuracy: 0.4187\n",
      "Epoch 00119: val_loss did not improve from 1.67239\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6245 - accuracy: 0.4187 - val_loss: 1.6769 - val_accuracy: 0.3847\n",
      "Epoch 120/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6244 - accuracy: 0.4239\n",
      "Epoch 00120: val_loss did not improve from 1.67239\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6205 - accuracy: 0.4245 - val_loss: 1.6755 - val_accuracy: 0.3793\n",
      "Epoch 121/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6143 - accuracy: 0.4250\n",
      "Epoch 00121: val_loss improved from 1.67239 to 1.66571, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6201 - accuracy: 0.4196 - val_loss: 1.6657 - val_accuracy: 0.3929\n",
      "Epoch 122/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6188 - accuracy: 0.4223\n",
      "Epoch 00122: val_loss did not improve from 1.66571\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6177 - accuracy: 0.4262 - val_loss: 1.6678 - val_accuracy: 0.3793\n",
      "Epoch 123/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6173 - accuracy: 0.4278\n",
      "Epoch 00123: val_loss did not improve from 1.66571\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6154 - accuracy: 0.4276 - val_loss: 1.6696 - val_accuracy: 0.3847\n",
      "Epoch 124/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6137 - accuracy: 0.4274\n",
      "Epoch 00124: val_loss did not improve from 1.66571\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6162 - accuracy: 0.4269 - val_loss: 1.6676 - val_accuracy: 0.3799\n",
      "Epoch 125/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6235 - accuracy: 0.4236\n",
      "Epoch 00125: val_loss improved from 1.66571 to 1.66061, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6125 - accuracy: 0.4262 - val_loss: 1.6606 - val_accuracy: 0.4018\n",
      "Epoch 126/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6062 - accuracy: 0.4287\n",
      "Epoch 00126: val_loss improved from 1.66061 to 1.65814, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6101 - accuracy: 0.4301 - val_loss: 1.6581 - val_accuracy: 0.3834\n",
      "Epoch 127/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6104 - accuracy: 0.4252\n",
      "Epoch 00127: val_loss did not improve from 1.65814\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6096 - accuracy: 0.4260 - val_loss: 1.6600 - val_accuracy: 0.3963\n",
      "Epoch 128/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5950 - accuracy: 0.4396\n",
      "Epoch 00128: val_loss did not improve from 1.65814\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6080 - accuracy: 0.4320 - val_loss: 1.6613 - val_accuracy: 0.3827\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6006 - accuracy: 0.4262\n",
      "Epoch 00129: val_loss improved from 1.65814 to 1.65568, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6065 - accuracy: 0.4264 - val_loss: 1.6557 - val_accuracy: 0.3950\n",
      "Epoch 130/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5912 - accuracy: 0.4403\n",
      "Epoch 00130: val_loss did not improve from 1.65568\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6048 - accuracy: 0.4335 - val_loss: 1.6608 - val_accuracy: 0.3881\n",
      "Epoch 131/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6050 - accuracy: 0.4293\n",
      "Epoch 00131: val_loss improved from 1.65568 to 1.65368, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6044 - accuracy: 0.4306 - val_loss: 1.6537 - val_accuracy: 0.3997\n",
      "Epoch 132/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5973 - accuracy: 0.4330\n",
      "Epoch 00132: val_loss improved from 1.65368 to 1.65214, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6018 - accuracy: 0.4330 - val_loss: 1.6521 - val_accuracy: 0.3868\n",
      "Epoch 133/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5977 - accuracy: 0.4282\n",
      "Epoch 00133: val_loss improved from 1.65214 to 1.65163, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6003 - accuracy: 0.4286 - val_loss: 1.6516 - val_accuracy: 0.4052\n",
      "Epoch 134/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6045 - accuracy: 0.4305\n",
      "Epoch 00134: val_loss did not improve from 1.65163\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6007 - accuracy: 0.4341 - val_loss: 1.6543 - val_accuracy: 0.3956\n",
      "Epoch 135/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5986 - accuracy: 0.4315\n",
      "Epoch 00135: val_loss did not improve from 1.65163\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5986 - accuracy: 0.4303 - val_loss: 1.6545 - val_accuracy: 0.3895\n",
      "Epoch 136/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5936 - accuracy: 0.4270\n",
      "Epoch 00136: val_loss improved from 1.65163 to 1.64414, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5968 - accuracy: 0.4327 - val_loss: 1.6441 - val_accuracy: 0.4031\n",
      "Epoch 137/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5912 - accuracy: 0.4422\n",
      "Epoch 00137: val_loss did not improve from 1.64414\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5941 - accuracy: 0.4364 - val_loss: 1.6463 - val_accuracy: 0.3840\n",
      "Epoch 138/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6003 - accuracy: 0.4311\n",
      "Epoch 00138: val_loss did not improve from 1.64414\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5933 - accuracy: 0.4351 - val_loss: 1.6454 - val_accuracy: 0.4059\n",
      "Epoch 139/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5848 - accuracy: 0.4361\n",
      "Epoch 00139: val_loss did not improve from 1.64414\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5922 - accuracy: 0.4349 - val_loss: 1.6455 - val_accuracy: 0.3997\n",
      "Epoch 140/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5923 - accuracy: 0.4301\n",
      "Epoch 00140: val_loss improved from 1.64414 to 1.63950, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5896 - accuracy: 0.4332 - val_loss: 1.6395 - val_accuracy: 0.3950\n",
      "Epoch 141/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5946 - accuracy: 0.4371\n",
      "Epoch 00141: val_loss did not improve from 1.63950\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5883 - accuracy: 0.4429 - val_loss: 1.6471 - val_accuracy: 0.3943\n",
      "Epoch 142/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5984 - accuracy: 0.4361\n",
      "Epoch 00142: val_loss did not improve from 1.63950\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5899 - accuracy: 0.4378 - val_loss: 1.6405 - val_accuracy: 0.3950\n",
      "Epoch 143/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5844 - accuracy: 0.4307\n",
      "Epoch 00143: val_loss did not improve from 1.63950\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5874 - accuracy: 0.4325 - val_loss: 1.6413 - val_accuracy: 0.3956\n",
      "Epoch 144/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5837 - accuracy: 0.4399\n",
      "Epoch 00144: val_loss improved from 1.63950 to 1.63436, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5856 - accuracy: 0.4393 - val_loss: 1.6344 - val_accuracy: 0.4106\n",
      "Epoch 145/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5866 - accuracy: 0.4326\n",
      "Epoch 00145: val_loss did not improve from 1.63436\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5868 - accuracy: 0.4325 - val_loss: 1.6372 - val_accuracy: 0.3950\n",
      "Epoch 146/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5832 - accuracy: 0.4443\n",
      "Epoch 00146: val_loss improved from 1.63436 to 1.63428, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5816 - accuracy: 0.4470 - val_loss: 1.6343 - val_accuracy: 0.4004\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5780 - accuracy: 0.4433\n",
      "Epoch 00147: val_loss improved from 1.63428 to 1.63320, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5780 - accuracy: 0.4433 - val_loss: 1.6332 - val_accuracy: 0.3915\n",
      "Epoch 148/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5766 - accuracy: 0.4453\n",
      "Epoch 00148: val_loss improved from 1.63320 to 1.63267, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5805 - accuracy: 0.4421 - val_loss: 1.6327 - val_accuracy: 0.3977\n",
      "Epoch 149/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5807 - accuracy: 0.4404\n",
      "Epoch 00149: val_loss improved from 1.63267 to 1.62909, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5763 - accuracy: 0.4414 - val_loss: 1.6291 - val_accuracy: 0.4018\n",
      "Epoch 150/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5673 - accuracy: 0.4463\n",
      "Epoch 00150: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5765 - accuracy: 0.4436 - val_loss: 1.6321 - val_accuracy: 0.3963\n",
      "Epoch 151/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5666 - accuracy: 0.4527\n",
      "Epoch 00151: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5720 - accuracy: 0.4509 - val_loss: 1.6292 - val_accuracy: 0.4038\n",
      "Epoch 152/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5628 - accuracy: 0.4461\n",
      "Epoch 00152: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5721 - accuracy: 0.4436 - val_loss: 1.6344 - val_accuracy: 0.3990\n",
      "Epoch 153/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5732 - accuracy: 0.4476\n",
      "Epoch 00153: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5711 - accuracy: 0.4479 - val_loss: 1.6305 - val_accuracy: 0.4018\n",
      "Epoch 154/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5667 - accuracy: 0.4473\n",
      "Epoch 00154: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5698 - accuracy: 0.4446 - val_loss: 1.6317 - val_accuracy: 0.4018\n",
      "Epoch 155/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5763 - accuracy: 0.4507\n",
      "Epoch 00155: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5687 - accuracy: 0.4506 - val_loss: 1.6341 - val_accuracy: 0.3956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5664 - accuracy: 0.4463\n",
      "Epoch 00156: val_loss did not improve from 1.62909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5686 - accuracy: 0.4467 - val_loss: 1.6318 - val_accuracy: 0.4004\n",
      "Epoch 157/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5651 - accuracy: 0.4562\n",
      "Epoch 00157: val_loss improved from 1.62909 to 1.62095, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5681 - accuracy: 0.4494 - val_loss: 1.6209 - val_accuracy: 0.4086\n",
      "Epoch 158/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5757 - accuracy: 0.4482\n",
      "Epoch 00158: val_loss improved from 1.62095 to 1.61669, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5642 - accuracy: 0.4513 - val_loss: 1.6167 - val_accuracy: 0.4127\n",
      "Epoch 159/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5657 - accuracy: 0.4496\n",
      "Epoch 00159: val_loss did not improve from 1.61669\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5653 - accuracy: 0.4491 - val_loss: 1.6183 - val_accuracy: 0.4011\n",
      "Epoch 160/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5571 - accuracy: 0.4515\n",
      "Epoch 00160: val_loss did not improve from 1.61669\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5610 - accuracy: 0.4477 - val_loss: 1.6214 - val_accuracy: 0.4045\n",
      "Epoch 161/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5487 - accuracy: 0.4558\n",
      "Epoch 00161: val_loss did not improve from 1.61669\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5589 - accuracy: 0.4547 - val_loss: 1.6219 - val_accuracy: 0.4086\n",
      "Epoch 162/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5524 - accuracy: 0.4486\n",
      "Epoch 00162: val_loss improved from 1.61669 to 1.61617, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5591 - accuracy: 0.4504 - val_loss: 1.6162 - val_accuracy: 0.3990\n",
      "Epoch 163/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5630 - accuracy: 0.4449\n",
      "Epoch 00163: val_loss did not improve from 1.61617\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5603 - accuracy: 0.4465 - val_loss: 1.6193 - val_accuracy: 0.3936\n",
      "Epoch 164/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5681 - accuracy: 0.4484\n",
      "Epoch 00164: val_loss improved from 1.61617 to 1.61408, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5589 - accuracy: 0.4484 - val_loss: 1.6141 - val_accuracy: 0.4031\n",
      "Epoch 165/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5600 - accuracy: 0.4482\n",
      "Epoch 00165: val_loss did not improve from 1.61408\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5566 - accuracy: 0.4482 - val_loss: 1.6142 - val_accuracy: 0.4154\n",
      "Epoch 166/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5458 - accuracy: 0.4556\n",
      "Epoch 00166: val_loss did not improve from 1.61408\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5530 - accuracy: 0.4530 - val_loss: 1.6148 - val_accuracy: 0.4025\n",
      "Epoch 167/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5544 - accuracy: 0.4509\n",
      "Epoch 00167: val_loss did not improve from 1.61408\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5513 - accuracy: 0.4516 - val_loss: 1.6162 - val_accuracy: 0.4079\n",
      "Epoch 168/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.5494 - accuracy: 0.4502\n",
      "Epoch 00168: val_loss did not improve from 1.61408\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.5521 - accuracy: 0.4503 - val_loss: 1.6160 - val_accuracy: 0.4079\n",
      "Epoch 169/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5525 - accuracy: 0.4478\n",
      "Epoch 00169: val_loss did not improve from 1.61408\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5525 - accuracy: 0.4467 - val_loss: 1.6182 - val_accuracy: 0.4113\n",
      "Epoch 170/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5489 - accuracy: 0.4531\n",
      "Epoch 00170: val_loss improved from 1.61408 to 1.60483, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5521 - accuracy: 0.4521 - val_loss: 1.6048 - val_accuracy: 0.4168\n",
      "Epoch 171/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5474 - accuracy: 0.4488\n",
      "Epoch 00171: val_loss did not improve from 1.60483\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5472 - accuracy: 0.4506 - val_loss: 1.6064 - val_accuracy: 0.4147\n",
      "Epoch 172/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5587 - accuracy: 0.4552\n",
      "Epoch 00172: val_loss did not improve from 1.60483\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5469 - accuracy: 0.4607 - val_loss: 1.6173 - val_accuracy: 0.4065\n",
      "Epoch 173/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5467 - accuracy: 0.4589\n",
      "Epoch 00173: val_loss improved from 1.60483 to 1.60335, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5485 - accuracy: 0.4576 - val_loss: 1.6034 - val_accuracy: 0.4100\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5471 - accuracy: 0.4554\n",
      "Epoch 00174: val_loss improved from 1.60335 to 1.60267, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5471 - accuracy: 0.4554 - val_loss: 1.6027 - val_accuracy: 0.4100\n",
      "Epoch 175/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5423 - accuracy: 0.4480\n",
      "Epoch 00175: val_loss improved from 1.60267 to 1.59975, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5422 - accuracy: 0.4515 - val_loss: 1.5997 - val_accuracy: 0.4120\n",
      "Epoch 176/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.5519 - accuracy: 0.4504\n",
      "Epoch 00176: val_loss did not improve from 1.59975\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5416 - accuracy: 0.4550 - val_loss: 1.6052 - val_accuracy: 0.4134\n",
      "Epoch 177/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5359 - accuracy: 0.4607\n",
      "Epoch 00177: val_loss did not improve from 1.59975\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5432 - accuracy: 0.4540 - val_loss: 1.6058 - val_accuracy: 0.4072\n",
      "Epoch 178/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5367 - accuracy: 0.4560\n",
      "Epoch 00178: val_loss improved from 1.59975 to 1.59868, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5418 - accuracy: 0.4569 - val_loss: 1.5987 - val_accuracy: 0.4154\n",
      "Epoch 179/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5378 - accuracy: 0.4535\n",
      "Epoch 00179: val_loss improved from 1.59868 to 1.59841, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5398 - accuracy: 0.4557 - val_loss: 1.5984 - val_accuracy: 0.4079\n",
      "Epoch 180/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5317 - accuracy: 0.4599\n",
      "Epoch 00180: val_loss improved from 1.59841 to 1.59426, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5349 - accuracy: 0.4596 - val_loss: 1.5943 - val_accuracy: 0.4161\n",
      "Epoch 181/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5387 - accuracy: 0.4535\n",
      "Epoch 00181: val_loss did not improve from 1.59426\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5334 - accuracy: 0.4585 - val_loss: 1.5978 - val_accuracy: 0.4229\n",
      "Epoch 182/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5330 - accuracy: 0.4542\n",
      "Epoch 00182: val_loss improved from 1.59426 to 1.59338, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5360 - accuracy: 0.4552 - val_loss: 1.5934 - val_accuracy: 0.4045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5260 - accuracy: 0.4568\n",
      "Epoch 00183: val_loss improved from 1.59338 to 1.59040, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5344 - accuracy: 0.4573 - val_loss: 1.5904 - val_accuracy: 0.4161\n",
      "Epoch 184/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5448 - accuracy: 0.4510\n",
      "Epoch 00184: val_loss did not improve from 1.59040\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5331 - accuracy: 0.4567 - val_loss: 1.5972 - val_accuracy: 0.4113\n",
      "Epoch 185/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5373 - accuracy: 0.4500\n",
      "Epoch 00185: val_loss did not improve from 1.59040\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5325 - accuracy: 0.4564 - val_loss: 1.5969 - val_accuracy: 0.4072\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5301 - accuracy: 0.4600\n",
      "Epoch 00186: val_loss did not improve from 1.59040\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5301 - accuracy: 0.4600 - val_loss: 1.5939 - val_accuracy: 0.4141\n",
      "Epoch 187/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5325 - accuracy: 0.4605\n",
      "Epoch 00187: val_loss improved from 1.59040 to 1.58711, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5326 - accuracy: 0.4617 - val_loss: 1.5871 - val_accuracy: 0.4045\n",
      "Epoch 188/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5219 - accuracy: 0.4590\n",
      "Epoch 00188: val_loss did not improve from 1.58711\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5292 - accuracy: 0.4588 - val_loss: 1.5883 - val_accuracy: 0.4195\n",
      "Epoch 189/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5287 - accuracy: 0.4657\n",
      "Epoch 00189: val_loss improved from 1.58711 to 1.58479, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5255 - accuracy: 0.4629 - val_loss: 1.5848 - val_accuracy: 0.4181\n",
      "Epoch 190/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5273 - accuracy: 0.4595\n",
      "Epoch 00190: val_loss did not improve from 1.58479\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5267 - accuracy: 0.4612 - val_loss: 1.5854 - val_accuracy: 0.4120\n",
      "Epoch 191/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5333 - accuracy: 0.4572\n",
      "Epoch 00191: val_loss did not improve from 1.58479\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5254 - accuracy: 0.4632 - val_loss: 1.5862 - val_accuracy: 0.4175\n",
      "Epoch 192/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5250 - accuracy: 0.4624\n",
      "Epoch 00192: val_loss did not improve from 1.58479\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5272 - accuracy: 0.4625 - val_loss: 1.5920 - val_accuracy: 0.4195\n",
      "Epoch 193/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5262 - accuracy: 0.4544\n",
      "Epoch 00193: val_loss improved from 1.58479 to 1.58391, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5255 - accuracy: 0.4602 - val_loss: 1.5839 - val_accuracy: 0.4188\n",
      "Epoch 194/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5277 - accuracy: 0.4630\n",
      "Epoch 00194: val_loss improved from 1.58391 to 1.58329, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5237 - accuracy: 0.4620 - val_loss: 1.5833 - val_accuracy: 0.4120\n",
      "Epoch 195/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5227 - accuracy: 0.4642\n",
      "Epoch 00195: val_loss improved from 1.58329 to 1.57958, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5203 - accuracy: 0.4641 - val_loss: 1.5796 - val_accuracy: 0.4222\n",
      "Epoch 196/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5198 - accuracy: 0.4648\n",
      "Epoch 00196: val_loss did not improve from 1.57958\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5190 - accuracy: 0.4636 - val_loss: 1.5813 - val_accuracy: 0.4222\n",
      "Epoch 197/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5168 - accuracy: 0.4622\n",
      "Epoch 00197: val_loss did not improve from 1.57958\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5182 - accuracy: 0.4608 - val_loss: 1.5823 - val_accuracy: 0.4120\n",
      "Epoch 198/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5106 - accuracy: 0.4675\n",
      "Epoch 00198: val_loss did not improve from 1.57958\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5209 - accuracy: 0.4615 - val_loss: 1.5934 - val_accuracy: 0.4168\n",
      "Epoch 199/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5099 - accuracy: 0.4636\n",
      "Epoch 00199: val_loss did not improve from 1.57958\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5165 - accuracy: 0.4603 - val_loss: 1.5856 - val_accuracy: 0.4113\n",
      "Epoch 200/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5210 - accuracy: 0.4517\n",
      "Epoch 00200: val_loss did not improve from 1.57958\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5196 - accuracy: 0.4562 - val_loss: 1.5884 - val_accuracy: 0.4113\n",
      "Epoch 201/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5126 - accuracy: 0.4581\n",
      "Epoch 00201: val_loss did not improve from 1.57958\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5169 - accuracy: 0.4588 - val_loss: 1.5826 - val_accuracy: 0.4181\n",
      "Epoch 202/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5073 - accuracy: 0.4648\n",
      "Epoch 00202: val_loss improved from 1.57958 to 1.57535, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5139 - accuracy: 0.4625 - val_loss: 1.5753 - val_accuracy: 0.4195\n",
      "Epoch 203/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5156 - accuracy: 0.4616\n",
      "Epoch 00203: val_loss did not improve from 1.57535\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5192 - accuracy: 0.4608 - val_loss: 1.5893 - val_accuracy: 0.4113\n",
      "Epoch 204/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5163 - accuracy: 0.4607\n",
      "Epoch 00204: val_loss improved from 1.57535 to 1.57213, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5120 - accuracy: 0.4639 - val_loss: 1.5721 - val_accuracy: 0.4120\n",
      "Epoch 205/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.5113 - accuracy: 0.4688\n",
      "Epoch 00205: val_loss did not improve from 1.57213\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5108 - accuracy: 0.4644 - val_loss: 1.5779 - val_accuracy: 0.4100\n",
      "Epoch 206/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5142 - accuracy: 0.4611\n",
      "Epoch 00206: val_loss did not improve from 1.57213\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5121 - accuracy: 0.4646 - val_loss: 1.5775 - val_accuracy: 0.4168\n",
      "Epoch 207/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5152 - accuracy: 0.4640\n",
      "Epoch 00207: val_loss did not improve from 1.57213\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5125 - accuracy: 0.4648 - val_loss: 1.5816 - val_accuracy: 0.4175\n",
      "Epoch 208/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5129 - accuracy: 0.4585\n",
      "Epoch 00208: val_loss did not improve from 1.57213\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5111 - accuracy: 0.4595 - val_loss: 1.5774 - val_accuracy: 0.4188\n",
      "Epoch 209/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5129 - accuracy: 0.4593\n",
      "Epoch 00209: val_loss improved from 1.57213 to 1.56863, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5099 - accuracy: 0.4644 - val_loss: 1.5686 - val_accuracy: 0.4147\n",
      "Epoch 210/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5043 - accuracy: 0.4706\n",
      "Epoch 00210: val_loss did not improve from 1.56863\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5075 - accuracy: 0.4702 - val_loss: 1.5714 - val_accuracy: 0.4209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5096 - accuracy: 0.4650\n",
      "Epoch 00211: val_loss did not improve from 1.56863\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5063 - accuracy: 0.4690 - val_loss: 1.5750 - val_accuracy: 0.4175\n",
      "Epoch 212/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5049 - accuracy: 0.4700\n",
      "Epoch 00212: val_loss did not improve from 1.56863\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5092 - accuracy: 0.4661 - val_loss: 1.5776 - val_accuracy: 0.4188\n",
      "Epoch 213/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4997 - accuracy: 0.4720\n",
      "Epoch 00213: val_loss did not improve from 1.56863\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5055 - accuracy: 0.4677 - val_loss: 1.5695 - val_accuracy: 0.4202\n",
      "Epoch 214/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5055 - accuracy: 0.4673\n",
      "Epoch 00214: val_loss improved from 1.56863 to 1.56720, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5019 - accuracy: 0.4716 - val_loss: 1.5672 - val_accuracy: 0.4181\n",
      "Epoch 215/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4878 - accuracy: 0.4786\n",
      "Epoch 00215: val_loss did not improve from 1.56720\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5016 - accuracy: 0.4709 - val_loss: 1.5676 - val_accuracy: 0.4106\n",
      "Epoch 216/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5154 - accuracy: 0.4646\n",
      "Epoch 00216: val_loss did not improve from 1.56720\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5048 - accuracy: 0.4678 - val_loss: 1.5681 - val_accuracy: 0.4222\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5020 - accuracy: 0.4678\n",
      "Epoch 00217: val_loss improved from 1.56720 to 1.56537, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5020 - accuracy: 0.4678 - val_loss: 1.5654 - val_accuracy: 0.4209\n",
      "Epoch 218/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4924 - accuracy: 0.4692\n",
      "Epoch 00218: val_loss improved from 1.56537 to 1.56286, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5030 - accuracy: 0.4694 - val_loss: 1.5629 - val_accuracy: 0.4270\n",
      "Epoch 219/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4974 - accuracy: 0.4755\n",
      "Epoch 00219: val_loss did not improve from 1.56286\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5041 - accuracy: 0.4672 - val_loss: 1.5677 - val_accuracy: 0.4216\n",
      "Epoch 220/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5124 - accuracy: 0.4667\n",
      "Epoch 00220: val_loss did not improve from 1.56286\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4995 - accuracy: 0.4735 - val_loss: 1.5649 - val_accuracy: 0.4188\n",
      "Epoch 221/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4996 - accuracy: 0.4685\n",
      "Epoch 00221: val_loss did not improve from 1.56286\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4992 - accuracy: 0.4735 - val_loss: 1.5649 - val_accuracy: 0.4263\n",
      "Epoch 222/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5063 - accuracy: 0.4681\n",
      "Epoch 00222: val_loss improved from 1.56286 to 1.56174, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5006 - accuracy: 0.4702 - val_loss: 1.5617 - val_accuracy: 0.4243\n",
      "Epoch 223/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4986 - accuracy: 0.4751\n",
      "Epoch 00223: val_loss improved from 1.56174 to 1.56041, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4964 - accuracy: 0.4701 - val_loss: 1.5604 - val_accuracy: 0.4236\n",
      "Epoch 224/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4988 - accuracy: 0.4755\n",
      "Epoch 00224: val_loss improved from 1.56041 to 1.55878, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4948 - accuracy: 0.4745 - val_loss: 1.5588 - val_accuracy: 0.4216\n",
      "Epoch 225/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4942 - accuracy: 0.4675\n",
      "Epoch 00225: val_loss did not improve from 1.55878\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4961 - accuracy: 0.4692 - val_loss: 1.5588 - val_accuracy: 0.4216\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4946 - accuracy: 0.4750\n",
      "Epoch 00226: val_loss did not improve from 1.55878\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4946 - accuracy: 0.4750 - val_loss: 1.5657 - val_accuracy: 0.4161\n",
      "Epoch 227/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4965 - accuracy: 0.4704\n",
      "Epoch 00227: val_loss improved from 1.55878 to 1.55742, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4941 - accuracy: 0.4701 - val_loss: 1.5574 - val_accuracy: 0.4195\n",
      "Epoch 228/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4914 - accuracy: 0.4744\n",
      "Epoch 00228: val_loss did not improve from 1.55742\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4920 - accuracy: 0.4745 - val_loss: 1.5597 - val_accuracy: 0.4318\n",
      "Epoch 229/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4924 - accuracy: 0.4762\n",
      "Epoch 00229: val_loss improved from 1.55742 to 1.55703, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4905 - accuracy: 0.4777 - val_loss: 1.5570 - val_accuracy: 0.4263\n",
      "Epoch 230/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4920 - accuracy: 0.4771\n",
      "Epoch 00230: val_loss improved from 1.55703 to 1.55442, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4912 - accuracy: 0.4774 - val_loss: 1.5544 - val_accuracy: 0.4216\n",
      "Epoch 231/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4868 - accuracy: 0.4757\n",
      "Epoch 00231: val_loss did not improve from 1.55442\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4904 - accuracy: 0.4774 - val_loss: 1.5556 - val_accuracy: 0.4202\n",
      "Epoch 232/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4947 - accuracy: 0.4737\n",
      "Epoch 00232: val_loss improved from 1.55442 to 1.55311, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4908 - accuracy: 0.4742 - val_loss: 1.5531 - val_accuracy: 0.4270\n",
      "Epoch 233/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4805 - accuracy: 0.4820\n",
      "Epoch 00233: val_loss did not improve from 1.55311\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4869 - accuracy: 0.4774 - val_loss: 1.5582 - val_accuracy: 0.4229\n",
      "Epoch 234/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.4818 - accuracy: 0.4793\n",
      "Epoch 00234: val_loss did not improve from 1.55311\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4877 - accuracy: 0.4784 - val_loss: 1.5543 - val_accuracy: 0.4243\n",
      "Epoch 235/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4809 - accuracy: 0.4766\n",
      "Epoch 00235: val_loss did not improve from 1.55311\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4881 - accuracy: 0.4733 - val_loss: 1.5556 - val_accuracy: 0.4256\n",
      "Epoch 236/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4863 - accuracy: 0.4796\n",
      "Epoch 00236: val_loss did not improve from 1.55311\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4888 - accuracy: 0.4760 - val_loss: 1.5545 - val_accuracy: 0.4209\n",
      "Epoch 237/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4872 - accuracy: 0.4774\n",
      "Epoch 00237: val_loss improved from 1.55311 to 1.55274, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4852 - accuracy: 0.4752 - val_loss: 1.5527 - val_accuracy: 0.4250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4792 - accuracy: 0.4866\n",
      "Epoch 00238: val_loss improved from 1.55274 to 1.54905, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4851 - accuracy: 0.4834 - val_loss: 1.5490 - val_accuracy: 0.4277\n",
      "Epoch 239/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4726 - accuracy: 0.4778\n",
      "Epoch 00239: val_loss improved from 1.54905 to 1.54718, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4820 - accuracy: 0.4762 - val_loss: 1.5472 - val_accuracy: 0.4297\n",
      "Epoch 240/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4901 - accuracy: 0.4813\n",
      "Epoch 00240: val_loss did not improve from 1.54718\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4840 - accuracy: 0.4832 - val_loss: 1.5538 - val_accuracy: 0.4359\n",
      "Epoch 241/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4852 - accuracy: 0.4743\n",
      "Epoch 00241: val_loss did not improve from 1.54718\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4845 - accuracy: 0.4750 - val_loss: 1.5504 - val_accuracy: 0.4284\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4809 - accuracy: 0.4808\n",
      "Epoch 00242: val_loss did not improve from 1.54718\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4809 - accuracy: 0.4808 - val_loss: 1.5507 - val_accuracy: 0.4291\n",
      "Epoch 243/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4806 - accuracy: 0.4806\n",
      "Epoch 00243: val_loss did not improve from 1.54718\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4802 - accuracy: 0.4811 - val_loss: 1.5476 - val_accuracy: 0.4332\n",
      "Epoch 244/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4774 - accuracy: 0.4788\n",
      "Epoch 00244: val_loss did not improve from 1.54718\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4815 - accuracy: 0.4779 - val_loss: 1.5501 - val_accuracy: 0.4359\n",
      "Epoch 245/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4894 - accuracy: 0.4805\n",
      "Epoch 00245: val_loss improved from 1.54718 to 1.54585, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4799 - accuracy: 0.4827 - val_loss: 1.5459 - val_accuracy: 0.4332\n",
      "Epoch 246/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4862 - accuracy: 0.4782\n",
      "Epoch 00246: val_loss improved from 1.54585 to 1.54424, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4773 - accuracy: 0.4796 - val_loss: 1.5442 - val_accuracy: 0.4291\n",
      "Epoch 247/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4814 - accuracy: 0.4829\n",
      "Epoch 00247: val_loss did not improve from 1.54424\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4777 - accuracy: 0.4823 - val_loss: 1.5467 - val_accuracy: 0.4270\n",
      "Epoch 248/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4825 - accuracy: 0.4827\n",
      "Epoch 00248: val_loss did not improve from 1.54424\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4798 - accuracy: 0.4825 - val_loss: 1.5486 - val_accuracy: 0.4304\n",
      "Epoch 249/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4809 - accuracy: 0.4794\n",
      "Epoch 00249: val_loss did not improve from 1.54424\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4793 - accuracy: 0.4769 - val_loss: 1.5460 - val_accuracy: 0.4441\n",
      "Epoch 250/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4737 - accuracy: 0.4821\n",
      "Epoch 00250: val_loss improved from 1.54424 to 1.54165, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4796 - accuracy: 0.4830 - val_loss: 1.5417 - val_accuracy: 0.4284\n",
      "Epoch 251/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4659 - accuracy: 0.4866\n",
      "Epoch 00251: val_loss improved from 1.54165 to 1.53851, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4749 - accuracy: 0.4822 - val_loss: 1.5385 - val_accuracy: 0.4325\n",
      "Epoch 252/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4789 - accuracy: 0.4792\n",
      "Epoch 00252: val_loss did not improve from 1.53851\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4762 - accuracy: 0.4829 - val_loss: 1.5438 - val_accuracy: 0.4352\n",
      "Epoch 253/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4792 - accuracy: 0.4774\n",
      "Epoch 00253: val_loss did not improve from 1.53851\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4760 - accuracy: 0.4820 - val_loss: 1.5418 - val_accuracy: 0.4352\n",
      "Epoch 254/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4607 - accuracy: 0.4914\n",
      "Epoch 00254: val_loss improved from 1.53851 to 1.53841, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4728 - accuracy: 0.4873 - val_loss: 1.5384 - val_accuracy: 0.4263\n",
      "Epoch 255/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4712 - accuracy: 0.4903\n",
      "Epoch 00255: val_loss did not improve from 1.53841\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4726 - accuracy: 0.4892 - val_loss: 1.5428 - val_accuracy: 0.4345\n",
      "Epoch 256/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4753 - accuracy: 0.4813\n",
      "Epoch 00256: val_loss did not improve from 1.53841\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4714 - accuracy: 0.4825 - val_loss: 1.5475 - val_accuracy: 0.4284\n",
      "Epoch 257/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4905 - accuracy: 0.4870\n",
      "Epoch 00257: val_loss improved from 1.53841 to 1.53394, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4752 - accuracy: 0.4883 - val_loss: 1.5339 - val_accuracy: 0.4366\n",
      "Epoch 258/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4768 - accuracy: 0.4858\n",
      "Epoch 00258: val_loss did not improve from 1.53394\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4719 - accuracy: 0.4873 - val_loss: 1.5354 - val_accuracy: 0.4447\n",
      "Epoch 259/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4712 - accuracy: 0.4839\n",
      "Epoch 00259: val_loss did not improve from 1.53394\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4698 - accuracy: 0.4839 - val_loss: 1.5377 - val_accuracy: 0.4284\n",
      "Epoch 260/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4732 - accuracy: 0.4796\n",
      "Epoch 00260: val_loss did not improve from 1.53394\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4715 - accuracy: 0.4818 - val_loss: 1.5452 - val_accuracy: 0.4229\n",
      "Epoch 261/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4680 - accuracy: 0.4774\n",
      "Epoch 00261: val_loss did not improve from 1.53394\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4720 - accuracy: 0.4794 - val_loss: 1.5349 - val_accuracy: 0.4311\n",
      "Epoch 262/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4631 - accuracy: 0.4831\n",
      "Epoch 00262: val_loss did not improve from 1.53394\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4676 - accuracy: 0.4837 - val_loss: 1.5389 - val_accuracy: 0.4441\n",
      "Epoch 263/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4635 - accuracy: 0.4875\n",
      "Epoch 00263: val_loss did not improve from 1.53394\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4658 - accuracy: 0.4876 - val_loss: 1.5387 - val_accuracy: 0.4447\n",
      "Epoch 264/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4634 - accuracy: 0.4891\n",
      "Epoch 00264: val_loss improved from 1.53394 to 1.53314, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4673 - accuracy: 0.4849 - val_loss: 1.5331 - val_accuracy: 0.4434\n",
      "Epoch 265/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4701 - accuracy: 0.4910\n",
      "Epoch 00265: val_loss improved from 1.53314 to 1.53203, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4656 - accuracy: 0.4909 - val_loss: 1.5320 - val_accuracy: 0.4386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4634 - accuracy: 0.4918\n",
      "Epoch 00266: val_loss did not improve from 1.53203\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4654 - accuracy: 0.4898 - val_loss: 1.5331 - val_accuracy: 0.4379\n",
      "Epoch 267/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4649 - accuracy: 0.4910\n",
      "Epoch 00267: val_loss did not improve from 1.53203\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4695 - accuracy: 0.4910 - val_loss: 1.5366 - val_accuracy: 0.4434\n",
      "Epoch 268/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4676 - accuracy: 0.4821\n",
      "Epoch 00268: val_loss improved from 1.53203 to 1.53125, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4662 - accuracy: 0.4803 - val_loss: 1.5312 - val_accuracy: 0.4434\n",
      "Epoch 269/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4636 - accuracy: 0.4877\n",
      "Epoch 00269: val_loss did not improve from 1.53125\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4636 - accuracy: 0.4902 - val_loss: 1.5340 - val_accuracy: 0.4413\n",
      "Epoch 270/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4616 - accuracy: 0.4942\n",
      "Epoch 00270: val_loss improved from 1.53125 to 1.52651, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4656 - accuracy: 0.4914 - val_loss: 1.5265 - val_accuracy: 0.4413\n",
      "Epoch 271/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4612 - accuracy: 0.4895\n",
      "Epoch 00271: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4621 - accuracy: 0.4871 - val_loss: 1.5297 - val_accuracy: 0.4441\n",
      "Epoch 272/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4607 - accuracy: 0.4885\n",
      "Epoch 00272: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4617 - accuracy: 0.4863 - val_loss: 1.5327 - val_accuracy: 0.4291\n",
      "Epoch 273/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4754 - accuracy: 0.4862\n",
      "Epoch 00273: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4664 - accuracy: 0.4890 - val_loss: 1.5272 - val_accuracy: 0.4332\n",
      "Epoch 274/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4552 - accuracy: 0.5006\n",
      "Epoch 00274: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4601 - accuracy: 0.4941 - val_loss: 1.5302 - val_accuracy: 0.4297\n",
      "Epoch 275/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4629 - accuracy: 0.4881\n",
      "Epoch 00275: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4644 - accuracy: 0.4835 - val_loss: 1.5266 - val_accuracy: 0.4270\n",
      "Epoch 276/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4538 - accuracy: 0.4967\n",
      "Epoch 00276: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4594 - accuracy: 0.4936 - val_loss: 1.5281 - val_accuracy: 0.4570\n",
      "Epoch 277/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4647 - accuracy: 0.4838\n",
      "Epoch 00277: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4626 - accuracy: 0.4856 - val_loss: 1.5338 - val_accuracy: 0.4454\n",
      "Epoch 278/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4535 - accuracy: 0.4905\n",
      "Epoch 00278: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4585 - accuracy: 0.4912 - val_loss: 1.5332 - val_accuracy: 0.4461\n",
      "Epoch 279/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4669 - accuracy: 0.4858\n",
      "Epoch 00279: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4598 - accuracy: 0.4916 - val_loss: 1.5289 - val_accuracy: 0.4359\n",
      "Epoch 280/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4597 - accuracy: 0.4953\n",
      "Epoch 00280: val_loss did not improve from 1.52651\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4627 - accuracy: 0.4909 - val_loss: 1.5348 - val_accuracy: 0.4352\n",
      "Epoch 281/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4523 - accuracy: 0.4899\n",
      "Epoch 00281: val_loss improved from 1.52651 to 1.52489, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4606 - accuracy: 0.4863 - val_loss: 1.5249 - val_accuracy: 0.4502\n",
      "Epoch 282/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4539 - accuracy: 0.4922\n",
      "Epoch 00282: val_loss improved from 1.52489 to 1.52127, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4574 - accuracy: 0.4927 - val_loss: 1.5213 - val_accuracy: 0.4413\n",
      "Epoch 283/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4597 - accuracy: 0.4955\n",
      "Epoch 00283: val_loss did not improve from 1.52127\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4592 - accuracy: 0.4927 - val_loss: 1.5261 - val_accuracy: 0.4352\n",
      "Epoch 284/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4464 - accuracy: 0.4981\n",
      "Epoch 00284: val_loss did not improve from 1.52127\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4536 - accuracy: 0.4933 - val_loss: 1.5280 - val_accuracy: 0.4434\n",
      "Epoch 285/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4497 - accuracy: 0.4984\n",
      "Epoch 00285: val_loss did not improve from 1.52127\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4546 - accuracy: 0.4985 - val_loss: 1.5217 - val_accuracy: 0.4332\n",
      "Epoch 286/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4499 - accuracy: 0.4912\n",
      "Epoch 00286: val_loss did not improve from 1.52127\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4532 - accuracy: 0.4912 - val_loss: 1.5223 - val_accuracy: 0.4386\n",
      "Epoch 287/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4551 - accuracy: 0.4934\n",
      "Epoch 00287: val_loss improved from 1.52127 to 1.52106, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4514 - accuracy: 0.4939 - val_loss: 1.5211 - val_accuracy: 0.4413\n",
      "Epoch 288/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4459 - accuracy: 0.4969\n",
      "Epoch 00288: val_loss improved from 1.52106 to 1.51909, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4509 - accuracy: 0.4948 - val_loss: 1.5191 - val_accuracy: 0.4475\n",
      "Epoch 289/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4473 - accuracy: 0.4969\n",
      "Epoch 00289: val_loss did not improve from 1.51909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4515 - accuracy: 0.4974 - val_loss: 1.5245 - val_accuracy: 0.4407\n",
      "Epoch 290/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4589 - accuracy: 0.4916\n",
      "Epoch 00290: val_loss did not improve from 1.51909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4548 - accuracy: 0.4910 - val_loss: 1.5292 - val_accuracy: 0.4482\n",
      "Epoch 291/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4564 - accuracy: 0.4986\n",
      "Epoch 00291: val_loss did not improve from 1.51909\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4517 - accuracy: 0.4948 - val_loss: 1.5256 - val_accuracy: 0.4447\n",
      "Epoch 292/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4496 - accuracy: 0.4951\n",
      "Epoch 00292: val_loss improved from 1.51909 to 1.51758, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4539 - accuracy: 0.4946 - val_loss: 1.5176 - val_accuracy: 0.4420\n",
      "Epoch 293/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4545 - accuracy: 0.4893\n",
      "Epoch 00293: val_loss did not improve from 1.51758\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4492 - accuracy: 0.4921 - val_loss: 1.5212 - val_accuracy: 0.4441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4445 - accuracy: 0.5056\n",
      "Epoch 00294: val_loss did not improve from 1.51758\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4507 - accuracy: 0.5006 - val_loss: 1.5187 - val_accuracy: 0.4509\n",
      "Epoch 295/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4485 - accuracy: 0.4947\n",
      "Epoch 00295: val_loss did not improve from 1.51758\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4496 - accuracy: 0.4921 - val_loss: 1.5183 - val_accuracy: 0.4604\n",
      "Epoch 296/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4525 - accuracy: 0.4928\n",
      "Epoch 00296: val_loss improved from 1.51758 to 1.51426, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4482 - accuracy: 0.4953 - val_loss: 1.5143 - val_accuracy: 0.4529\n",
      "Epoch 297/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4431 - accuracy: 0.4971\n",
      "Epoch 00297: val_loss did not improve from 1.51426\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4480 - accuracy: 0.4968 - val_loss: 1.5300 - val_accuracy: 0.4502\n",
      "Epoch 298/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4405 - accuracy: 0.5041\n",
      "Epoch 00298: val_loss improved from 1.51426 to 1.51357, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4487 - accuracy: 0.4963 - val_loss: 1.5136 - val_accuracy: 0.4488\n",
      "Epoch 299/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4515 - accuracy: 0.4988\n",
      "Epoch 00299: val_loss did not improve from 1.51357\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4467 - accuracy: 0.4955 - val_loss: 1.5143 - val_accuracy: 0.4536\n",
      "Epoch 300/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4501 - accuracy: 0.4936\n",
      "Epoch 00300: val_loss did not improve from 1.51357\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4455 - accuracy: 0.4975 - val_loss: 1.5165 - val_accuracy: 0.4495\n",
      "Epoch 301/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.4486 - accuracy: 0.4969\n",
      "Epoch 00301: val_loss did not improve from 1.51357\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4478 - accuracy: 0.4975 - val_loss: 1.5155 - val_accuracy: 0.4427\n",
      "Epoch 302/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4476 - accuracy: 0.4936\n",
      "Epoch 00302: val_loss did not improve from 1.51357\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4494 - accuracy: 0.4924 - val_loss: 1.5140 - val_accuracy: 0.4563\n",
      "Epoch 303/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4398 - accuracy: 0.5006\n",
      "Epoch 00303: val_loss improved from 1.51357 to 1.51323, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4447 - accuracy: 0.4967 - val_loss: 1.5132 - val_accuracy: 0.4488\n",
      "Epoch 304/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4475 - accuracy: 0.4990\n",
      "Epoch 00304: val_loss improved from 1.51323 to 1.51240, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4452 - accuracy: 0.4979 - val_loss: 1.5124 - val_accuracy: 0.4393\n",
      "Epoch 305/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4536 - accuracy: 0.4866\n",
      "Epoch 00305: val_loss did not improve from 1.51240\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4485 - accuracy: 0.4909 - val_loss: 1.5256 - val_accuracy: 0.4550\n",
      "Epoch 306/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4527 - accuracy: 0.4965\n",
      "Epoch 00306: val_loss did not improve from 1.51240\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4530 - accuracy: 0.4950 - val_loss: 1.5170 - val_accuracy: 0.4434\n",
      "Epoch 307/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4395 - accuracy: 0.4973\n",
      "Epoch 00307: val_loss did not improve from 1.51240\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4474 - accuracy: 0.4967 - val_loss: 1.5169 - val_accuracy: 0.4543\n",
      "Epoch 308/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4487 - accuracy: 0.5000\n",
      "Epoch 00308: val_loss improved from 1.51240 to 1.51065, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4406 - accuracy: 0.5023 - val_loss: 1.5107 - val_accuracy: 0.4591\n",
      "Epoch 309/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4401 - accuracy: 0.5000\n",
      "Epoch 00309: val_loss did not improve from 1.51065\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4433 - accuracy: 0.5032 - val_loss: 1.5122 - val_accuracy: 0.4577\n",
      "Epoch 310/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4466 - accuracy: 0.4961\n",
      "Epoch 00310: val_loss improved from 1.51065 to 1.50979, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4410 - accuracy: 0.4958 - val_loss: 1.5098 - val_accuracy: 0.4550\n",
      "Epoch 311/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4390 - accuracy: 0.5008\n",
      "Epoch 00311: val_loss did not improve from 1.50979\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4420 - accuracy: 0.4996 - val_loss: 1.5136 - val_accuracy: 0.4502\n",
      "Epoch 312/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4443 - accuracy: 0.4965\n",
      "Epoch 00312: val_loss improved from 1.50979 to 1.50913, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4407 - accuracy: 0.4980 - val_loss: 1.5091 - val_accuracy: 0.4570\n",
      "Epoch 313/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4424 - accuracy: 0.5006\n",
      "Epoch 00313: val_loss improved from 1.50913 to 1.50896, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4397 - accuracy: 0.5009 - val_loss: 1.5090 - val_accuracy: 0.4475\n",
      "Epoch 314/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4452 - accuracy: 0.4957\n",
      "Epoch 00314: val_loss did not improve from 1.50896\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4412 - accuracy: 0.4980 - val_loss: 1.5093 - val_accuracy: 0.4400\n",
      "Epoch 315/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4333 - accuracy: 0.5051\n",
      "Epoch 00315: val_loss improved from 1.50896 to 1.50627, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4394 - accuracy: 0.5008 - val_loss: 1.5063 - val_accuracy: 0.4632\n",
      "Epoch 316/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4256 - accuracy: 0.5072\n",
      "Epoch 00316: val_loss improved from 1.50627 to 1.50398, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4372 - accuracy: 0.5054 - val_loss: 1.5040 - val_accuracy: 0.4550\n",
      "Epoch 317/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4276 - accuracy: 0.5058\n",
      "Epoch 00317: val_loss did not improve from 1.50398\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4390 - accuracy: 0.4987 - val_loss: 1.5198 - val_accuracy: 0.4727\n",
      "Epoch 318/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4449 - accuracy: 0.5002\n",
      "Epoch 00318: val_loss improved from 1.50398 to 1.50305, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4409 - accuracy: 0.5026 - val_loss: 1.5031 - val_accuracy: 0.4550\n",
      "Epoch 319/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4343 - accuracy: 0.5090\n",
      "Epoch 00319: val_loss did not improve from 1.50305\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4368 - accuracy: 0.5037 - val_loss: 1.5056 - val_accuracy: 0.4536\n",
      "Epoch 320/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4402 - accuracy: 0.4947\n",
      "Epoch 00320: val_loss did not improve from 1.50305\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4379 - accuracy: 0.4968 - val_loss: 1.5073 - val_accuracy: 0.4618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4430 - accuracy: 0.5012\n",
      "Epoch 00321: val_loss did not improve from 1.50305\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4388 - accuracy: 0.5028 - val_loss: 1.5042 - val_accuracy: 0.4523\n",
      "Epoch 322/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4335 - accuracy: 0.5031\n",
      "Epoch 00322: val_loss did not improve from 1.50305\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4360 - accuracy: 0.5061 - val_loss: 1.5120 - val_accuracy: 0.4550\n",
      "Epoch 323/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4391 - accuracy: 0.5002\n",
      "Epoch 00323: val_loss improved from 1.50305 to 1.50187, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4372 - accuracy: 0.5011 - val_loss: 1.5019 - val_accuracy: 0.4495\n",
      "Epoch 324/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4358 - accuracy: 0.5023\n",
      "Epoch 00324: val_loss did not improve from 1.50187\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4352 - accuracy: 0.4996 - val_loss: 1.5037 - val_accuracy: 0.4536\n",
      "Epoch 325/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4428 - accuracy: 0.4983\n",
      "Epoch 00325: val_loss did not improve from 1.50187\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4342 - accuracy: 0.5057 - val_loss: 1.5032 - val_accuracy: 0.4543\n",
      "Epoch 326/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4343 - accuracy: 0.4992\n",
      "Epoch 00326: val_loss did not improve from 1.50187\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4343 - accuracy: 0.5016 - val_loss: 1.5019 - val_accuracy: 0.4550\n",
      "Epoch 327/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4323 - accuracy: 0.5037\n",
      "Epoch 00327: val_loss improved from 1.50187 to 1.50150, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4333 - accuracy: 0.5026 - val_loss: 1.5015 - val_accuracy: 0.4495\n",
      "Epoch 328/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4116 - accuracy: 0.5113\n",
      "Epoch 00328: val_loss improved from 1.50150 to 1.49998, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4367 - accuracy: 0.5020 - val_loss: 1.5000 - val_accuracy: 0.4598\n",
      "Epoch 329/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4282 - accuracy: 0.5101\n",
      "Epoch 00329: val_loss improved from 1.49998 to 1.49635, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4361 - accuracy: 0.5066 - val_loss: 1.4963 - val_accuracy: 0.4536\n",
      "Epoch 330/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4226 - accuracy: 0.5008\n",
      "Epoch 00330: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4321 - accuracy: 0.4989 - val_loss: 1.5070 - val_accuracy: 0.4488\n",
      "Epoch 331/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4326 - accuracy: 0.5033\n",
      "Epoch 00331: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4326 - accuracy: 0.5064 - val_loss: 1.5017 - val_accuracy: 0.4447\n",
      "Epoch 332/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4213 - accuracy: 0.5045\n",
      "Epoch 00332: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4308 - accuracy: 0.5040 - val_loss: 1.5033 - val_accuracy: 0.4638\n",
      "Epoch 333/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4346 - accuracy: 0.5056\n",
      "Epoch 00333: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4329 - accuracy: 0.5055 - val_loss: 1.4992 - val_accuracy: 0.4550\n",
      "Epoch 334/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4165 - accuracy: 0.5134\n",
      "Epoch 00334: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4295 - accuracy: 0.5054 - val_loss: 1.5017 - val_accuracy: 0.4652\n",
      "Epoch 335/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4347 - accuracy: 0.5008\n",
      "Epoch 00335: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4303 - accuracy: 0.5021 - val_loss: 1.5025 - val_accuracy: 0.4598\n",
      "Epoch 336/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4323 - accuracy: 0.5002\n",
      "Epoch 00336: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4309 - accuracy: 0.5032 - val_loss: 1.5028 - val_accuracy: 0.4625\n",
      "Epoch 337/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4246 - accuracy: 0.5060\n",
      "Epoch 00337: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4271 - accuracy: 0.5083 - val_loss: 1.4987 - val_accuracy: 0.4707\n",
      "Epoch 338/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4210 - accuracy: 0.5097\n",
      "Epoch 00338: val_loss did not improve from 1.49635\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4285 - accuracy: 0.5054 - val_loss: 1.5010 - val_accuracy: 0.4679\n",
      "Epoch 339/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4232 - accuracy: 0.5029\n",
      "Epoch 00339: val_loss improved from 1.49635 to 1.49315, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4282 - accuracy: 0.5030 - val_loss: 1.4931 - val_accuracy: 0.4577\n",
      "Epoch 340/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4258 - accuracy: 0.5099\n",
      "Epoch 00340: val_loss did not improve from 1.49315\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4251 - accuracy: 0.5062 - val_loss: 1.4941 - val_accuracy: 0.4632\n",
      "Epoch 341/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4340 - accuracy: 0.5026\n",
      "Epoch 00341: val_loss did not improve from 1.49315\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4277 - accuracy: 0.5078 - val_loss: 1.4936 - val_accuracy: 0.4604\n",
      "Epoch 342/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4235 - accuracy: 0.5047\n",
      "Epoch 00342: val_loss did not improve from 1.49315\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4263 - accuracy: 0.5032 - val_loss: 1.4936 - val_accuracy: 0.4488\n",
      "Epoch 343/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4197 - accuracy: 0.5113\n",
      "Epoch 00343: val_loss improved from 1.49315 to 1.49150, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4260 - accuracy: 0.5062 - val_loss: 1.4915 - val_accuracy: 0.4679\n",
      "Epoch 344/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4233 - accuracy: 0.5053\n",
      "Epoch 00344: val_loss did not improve from 1.49150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4289 - accuracy: 0.5057 - val_loss: 1.4959 - val_accuracy: 0.4516\n",
      "Epoch 345/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4235 - accuracy: 0.5045\n",
      "Epoch 00345: val_loss did not improve from 1.49150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4288 - accuracy: 0.5052 - val_loss: 1.5074 - val_accuracy: 0.4625\n",
      "Epoch 346/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4214 - accuracy: 0.5136\n",
      "Epoch 00346: val_loss did not improve from 1.49150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4274 - accuracy: 0.5091 - val_loss: 1.4975 - val_accuracy: 0.4488\n",
      "Epoch 347/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4267 - accuracy: 0.5051\n",
      "Epoch 00347: val_loss did not improve from 1.49150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4254 - accuracy: 0.5073 - val_loss: 1.4945 - val_accuracy: 0.4700\n",
      "Epoch 348/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4217 - accuracy: 0.5158\n",
      "Epoch 00348: val_loss improved from 1.49150 to 1.48984, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4261 - accuracy: 0.5120 - val_loss: 1.4898 - val_accuracy: 0.4557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4139 - accuracy: 0.5090\n",
      "Epoch 00349: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4216 - accuracy: 0.5088 - val_loss: 1.4960 - val_accuracy: 0.4666\n",
      "Epoch 350/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4255 - accuracy: 0.5051\n",
      "Epoch 00350: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4278 - accuracy: 0.5038 - val_loss: 1.4922 - val_accuracy: 0.4618\n",
      "Epoch 351/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4222 - accuracy: 0.5082\n",
      "Epoch 00351: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4250 - accuracy: 0.5045 - val_loss: 1.4955 - val_accuracy: 0.4509\n",
      "Epoch 352/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4195 - accuracy: 0.5041\n",
      "Epoch 00352: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4233 - accuracy: 0.5045 - val_loss: 1.4932 - val_accuracy: 0.4673\n",
      "Epoch 353/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4300 - accuracy: 0.5014\n",
      "Epoch 00353: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4219 - accuracy: 0.5083 - val_loss: 1.4939 - val_accuracy: 0.4625\n",
      "Epoch 354/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4225 - accuracy: 0.5101\n",
      "Epoch 00354: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4252 - accuracy: 0.5071 - val_loss: 1.4947 - val_accuracy: 0.4659\n",
      "Epoch 355/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4152 - accuracy: 0.5130\n",
      "Epoch 00355: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4224 - accuracy: 0.5093 - val_loss: 1.4906 - val_accuracy: 0.4543\n",
      "Epoch 356/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4188 - accuracy: 0.5074\n",
      "Epoch 00356: val_loss did not improve from 1.48984\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4235 - accuracy: 0.5042 - val_loss: 1.4968 - val_accuracy: 0.4679\n",
      "Epoch 357/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4258 - accuracy: 0.5088\n",
      "Epoch 00357: val_loss improved from 1.48984 to 1.48624, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4210 - accuracy: 0.5084 - val_loss: 1.4862 - val_accuracy: 0.4673\n",
      "Epoch 358/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4179 - accuracy: 0.5014\n",
      "Epoch 00358: val_loss did not improve from 1.48624\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4201 - accuracy: 0.5055 - val_loss: 1.4918 - val_accuracy: 0.4693\n",
      "Epoch 359/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4045 - accuracy: 0.5171\n",
      "Epoch 00359: val_loss did not improve from 1.48624\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4187 - accuracy: 0.5124 - val_loss: 1.4903 - val_accuracy: 0.4543\n",
      "Epoch 360/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4218 - accuracy: 0.5066\n",
      "Epoch 00360: val_loss did not improve from 1.48624\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4207 - accuracy: 0.5069 - val_loss: 1.4892 - val_accuracy: 0.4707\n",
      "Epoch 361/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4178 - accuracy: 0.5111\n",
      "Epoch 00361: val_loss did not improve from 1.48624\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4179 - accuracy: 0.5103 - val_loss: 1.4933 - val_accuracy: 0.4638\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4177 - accuracy: 0.5117\n",
      "Epoch 00362: val_loss did not improve from 1.48624\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4177 - accuracy: 0.5117 - val_loss: 1.4878 - val_accuracy: 0.4652\n",
      "Epoch 363/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4114 - accuracy: 0.5136\n",
      "Epoch 00363: val_loss improved from 1.48624 to 1.48471, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4157 - accuracy: 0.5084 - val_loss: 1.4847 - val_accuracy: 0.4652\n",
      "Epoch 364/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4123 - accuracy: 0.5080\n",
      "Epoch 00364: val_loss improved from 1.48471 to 1.48435, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4182 - accuracy: 0.5081 - val_loss: 1.4844 - val_accuracy: 0.4659\n",
      "Epoch 365/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4077 - accuracy: 0.5121\n",
      "Epoch 00365: val_loss did not improve from 1.48435\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4176 - accuracy: 0.5102 - val_loss: 1.4865 - val_accuracy: 0.4652\n",
      "Epoch 366/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4227 - accuracy: 0.5109\n",
      "Epoch 00366: val_loss did not improve from 1.48435\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4159 - accuracy: 0.5127 - val_loss: 1.4846 - val_accuracy: 0.4673\n",
      "Epoch 367/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4170 - accuracy: 0.5037\n",
      "Epoch 00367: val_loss did not improve from 1.48435\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4169 - accuracy: 0.5064 - val_loss: 1.4886 - val_accuracy: 0.4679\n",
      "Epoch 00367: early stopping\n",
      "Training for model  1  completed in time:  0:01:48.897237 seconds\n",
      "Training for model  2  has started.\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.6655 - accuracy: 0.1580\n",
      "Epoch 00001: val_loss improved from inf to 2.03346, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 3.6655 - accuracy: 0.1580 - val_loss: 2.0335 - val_accuracy: 0.2217\n",
      "Epoch 2/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.8427 - accuracy: 0.3090\n",
      "Epoch 00002: val_loss improved from 2.03346 to 1.71934, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.8269 - accuracy: 0.3180 - val_loss: 1.7193 - val_accuracy: 0.3779\n",
      "Epoch 3/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6583 - accuracy: 0.4224\n",
      "Epoch 00003: val_loss improved from 1.71934 to 1.64698, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.6538 - accuracy: 0.4245 - val_loss: 1.6470 - val_accuracy: 0.4243\n",
      "Epoch 4/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5795 - accuracy: 0.4618\n",
      "Epoch 00004: val_loss improved from 1.64698 to 1.61055, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5825 - accuracy: 0.4562 - val_loss: 1.6105 - val_accuracy: 0.4216\n",
      "Epoch 5/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5389 - accuracy: 0.4714\n",
      "Epoch 00005: val_loss improved from 1.61055 to 1.56707, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.5351 - accuracy: 0.4750 - val_loss: 1.5671 - val_accuracy: 0.4632\n",
      "Epoch 6/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.4918 - accuracy: 0.4959\n",
      "Epoch 00006: val_loss improved from 1.56707 to 1.54379, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4994 - accuracy: 0.4929 - val_loss: 1.5438 - val_accuracy: 0.4707\n",
      "Epoch 7/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4689 - accuracy: 0.5027\n",
      "Epoch 00007: val_loss improved from 1.54379 to 1.50743, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4693 - accuracy: 0.5008 - val_loss: 1.5074 - val_accuracy: 0.4802\n",
      "Epoch 8/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4463 - accuracy: 0.5119\n",
      "Epoch 00008: val_loss improved from 1.50743 to 1.48321, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4408 - accuracy: 0.5148 - val_loss: 1.4832 - val_accuracy: 0.4870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4195 - accuracy: 0.5170\n",
      "Epoch 00009: val_loss improved from 1.48321 to 1.46000, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4195 - accuracy: 0.5170 - val_loss: 1.4600 - val_accuracy: 0.5027\n",
      "Epoch 10/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4030 - accuracy: 0.5160\n",
      "Epoch 00010: val_loss improved from 1.46000 to 1.44447, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4006 - accuracy: 0.5206 - val_loss: 1.4445 - val_accuracy: 0.4918\n",
      "Epoch 11/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.3792 - accuracy: 0.5266\n",
      "Epoch 00011: val_loss improved from 1.44447 to 1.42946, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3856 - accuracy: 0.5245 - val_loss: 1.4295 - val_accuracy: 0.5102\n",
      "Epoch 12/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.3617 - accuracy: 0.5319\n",
      "Epoch 00012: val_loss improved from 1.42946 to 1.42028, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3596 - accuracy: 0.5335 - val_loss: 1.4203 - val_accuracy: 0.5089\n",
      "Epoch 13/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.3471 - accuracy: 0.5456\n",
      "Epoch 00013: val_loss improved from 1.42028 to 1.39450, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3423 - accuracy: 0.5465 - val_loss: 1.3945 - val_accuracy: 0.5184\n",
      "Epoch 14/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.3297 - accuracy: 0.5436\n",
      "Epoch 00014: val_loss improved from 1.39450 to 1.37845, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3331 - accuracy: 0.5441 - val_loss: 1.3785 - val_accuracy: 0.5191\n",
      "Epoch 15/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.3166 - accuracy: 0.5482\n",
      "Epoch 00015: val_loss improved from 1.37845 to 1.35700, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3180 - accuracy: 0.5463 - val_loss: 1.3570 - val_accuracy: 0.5286\n",
      "Epoch 16/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2992 - accuracy: 0.5543\n",
      "Epoch 00016: val_loss improved from 1.35700 to 1.34221, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3010 - accuracy: 0.5533 - val_loss: 1.3422 - val_accuracy: 0.5293\n",
      "Epoch 17/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.2765 - accuracy: 0.5639\n",
      "Epoch 00017: val_loss improved from 1.34221 to 1.32561, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.2805 - accuracy: 0.5600 - val_loss: 1.3256 - val_accuracy: 0.5368\n",
      "Epoch 18/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2702 - accuracy: 0.5582\n",
      "Epoch 00018: val_loss did not improve from 1.32561\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.2712 - accuracy: 0.5613 - val_loss: 1.3370 - val_accuracy: 0.5232\n",
      "Epoch 19/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.2666 - accuracy: 0.5564\n",
      "Epoch 00019: val_loss improved from 1.32561 to 1.31119, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.2669 - accuracy: 0.5583 - val_loss: 1.3112 - val_accuracy: 0.5341\n",
      "Epoch 20/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.2407 - accuracy: 0.5734\n",
      "Epoch 00020: val_loss improved from 1.31119 to 1.29621, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2457 - accuracy: 0.5724 - val_loss: 1.2962 - val_accuracy: 0.5580\n",
      "Epoch 21/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.2350 - accuracy: 0.5732\n",
      "Epoch 00021: val_loss improved from 1.29621 to 1.27817, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2331 - accuracy: 0.5748 - val_loss: 1.2782 - val_accuracy: 0.5566\n",
      "Epoch 22/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2231 - accuracy: 0.5798\n",
      "Epoch 00022: val_loss improved from 1.27817 to 1.27773, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.2209 - accuracy: 0.5796 - val_loss: 1.2777 - val_accuracy: 0.5546\n",
      "Epoch 23/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.2143 - accuracy: 0.5804\n",
      "Epoch 00023: val_loss did not improve from 1.27773\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.2107 - accuracy: 0.5810 - val_loss: 1.2790 - val_accuracy: 0.5341\n",
      "Epoch 24/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.2023 - accuracy: 0.5838\n",
      "Epoch 00024: val_loss improved from 1.27773 to 1.24813, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.2031 - accuracy: 0.5833 - val_loss: 1.2481 - val_accuracy: 0.5593\n",
      "Epoch 25/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.1908 - accuracy: 0.5890\n",
      "Epoch 00025: val_loss improved from 1.24813 to 1.23564, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1868 - accuracy: 0.5864 - val_loss: 1.2356 - val_accuracy: 0.5621\n",
      "Epoch 26/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.1763 - accuracy: 0.5948\n",
      "Epoch 00026: val_loss improved from 1.23564 to 1.22373, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1814 - accuracy: 0.5949 - val_loss: 1.2237 - val_accuracy: 0.5723\n",
      "Epoch 27/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1725 - accuracy: 0.5923\n",
      "Epoch 00027: val_loss improved from 1.22373 to 1.22255, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1725 - accuracy: 0.5932 - val_loss: 1.2226 - val_accuracy: 0.5757\n",
      "Epoch 28/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1568 - accuracy: 0.5945\n",
      "Epoch 00028: val_loss improved from 1.22255 to 1.21378, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1621 - accuracy: 0.5932 - val_loss: 1.2138 - val_accuracy: 0.5825\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1663 - accuracy: 0.5967\n",
      "Epoch 00029: val_loss improved from 1.21378 to 1.20463, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1663 - accuracy: 0.5967 - val_loss: 1.2046 - val_accuracy: 0.5921\n",
      "Epoch 30/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1463 - accuracy: 0.6081\n",
      "Epoch 00030: val_loss did not improve from 1.20463\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.1538 - accuracy: 0.6040 - val_loss: 1.2184 - val_accuracy: 0.5764\n",
      "Epoch 31/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.1340 - accuracy: 0.6023\n",
      "Epoch 00031: val_loss improved from 1.20463 to 1.18801, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1427 - accuracy: 0.5984 - val_loss: 1.1880 - val_accuracy: 0.5819\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1330 - accuracy: 0.6108\n",
      "Epoch 00032: val_loss did not improve from 1.18801\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1330 - accuracy: 0.6108 - val_loss: 1.1892 - val_accuracy: 0.5914\n",
      "Epoch 33/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1253 - accuracy: 0.6122\n",
      "Epoch 00033: val_loss improved from 1.18801 to 1.16717, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1237 - accuracy: 0.6127 - val_loss: 1.1672 - val_accuracy: 0.5907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1203 - accuracy: 0.6127\n",
      "Epoch 00034: val_loss improved from 1.16717 to 1.16586, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1185 - accuracy: 0.6158 - val_loss: 1.1659 - val_accuracy: 0.6050\n",
      "Epoch 35/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1085 - accuracy: 0.6165\n",
      "Epoch 00035: val_loss improved from 1.16586 to 1.14991, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1105 - accuracy: 0.6168 - val_loss: 1.1499 - val_accuracy: 0.5989\n",
      "Epoch 36/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.1068 - accuracy: 0.6234\n",
      "Epoch 00036: val_loss improved from 1.14991 to 1.13938, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1020 - accuracy: 0.6262 - val_loss: 1.1394 - val_accuracy: 0.6146\n",
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1043 - accuracy: 0.6252\n",
      "Epoch 00037: val_loss did not improve from 1.13938\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0977 - accuracy: 0.6277 - val_loss: 1.1541 - val_accuracy: 0.6126\n",
      "Epoch 38/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.0936 - accuracy: 0.6293\n",
      "Epoch 00038: val_loss did not improve from 1.13938\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0895 - accuracy: 0.6286 - val_loss: 1.1464 - val_accuracy: 0.5975\n",
      "Epoch 39/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.0648 - accuracy: 0.6387\n",
      "Epoch 00039: val_loss improved from 1.13938 to 1.13284, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0814 - accuracy: 0.6340 - val_loss: 1.1328 - val_accuracy: 0.6146\n",
      "Epoch 40/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0718 - accuracy: 0.6354\n",
      "Epoch 00040: val_loss improved from 1.13284 to 1.13041, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0719 - accuracy: 0.6371 - val_loss: 1.1304 - val_accuracy: 0.6194\n",
      "Epoch 41/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.0720 - accuracy: 0.6242\n",
      "Epoch 00041: val_loss improved from 1.13041 to 1.12203, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0702 - accuracy: 0.6292 - val_loss: 1.1220 - val_accuracy: 0.6276\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0604 - accuracy: 0.6395\n",
      "Epoch 00042: val_loss improved from 1.12203 to 1.11304, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0604 - accuracy: 0.6395 - val_loss: 1.1130 - val_accuracy: 0.6180\n",
      "Epoch 43/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0556 - accuracy: 0.6468\n",
      "Epoch 00043: val_loss did not improve from 1.11304\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0561 - accuracy: 0.6472 - val_loss: 1.1259 - val_accuracy: 0.5941\n",
      "Epoch 44/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.0521 - accuracy: 0.6421\n",
      "Epoch 00044: val_loss improved from 1.11304 to 1.09278, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0642 - accuracy: 0.6364 - val_loss: 1.0928 - val_accuracy: 0.6248\n",
      "Epoch 45/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0461 - accuracy: 0.6469\n",
      "Epoch 00045: val_loss improved from 1.09278 to 1.07933, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0501 - accuracy: 0.6463 - val_loss: 1.0793 - val_accuracy: 0.6351\n",
      "Epoch 46/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.0363 - accuracy: 0.6518\n",
      "Epoch 00046: val_loss did not improve from 1.07933\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0405 - accuracy: 0.6506 - val_loss: 1.0955 - val_accuracy: 0.6201\n",
      "Epoch 47/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.0356 - accuracy: 0.6488\n",
      "Epoch 00047: val_loss improved from 1.07933 to 1.07191, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0332 - accuracy: 0.6530 - val_loss: 1.0719 - val_accuracy: 0.6405\n",
      "Epoch 48/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.0274 - accuracy: 0.6590\n",
      "Epoch 00048: val_loss improved from 1.07191 to 1.06382, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0273 - accuracy: 0.6584 - val_loss: 1.0638 - val_accuracy: 0.6446\n",
      "Epoch 49/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.0175 - accuracy: 0.6568\n",
      "Epoch 00049: val_loss did not improve from 1.06382\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0235 - accuracy: 0.6548 - val_loss: 1.0913 - val_accuracy: 0.6337\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0224 - accuracy: 0.6594\n",
      "Epoch 00050: val_loss did not improve from 1.06382\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.0264 - accuracy: 0.6567 - val_loss: 1.0734 - val_accuracy: 0.6221\n",
      "Epoch 51/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0258 - accuracy: 0.6566\n",
      "Epoch 00051: val_loss improved from 1.06382 to 1.06159, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0212 - accuracy: 0.6562 - val_loss: 1.0616 - val_accuracy: 0.6364\n",
      "Epoch 52/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.0174 - accuracy: 0.6555\n",
      "Epoch 00052: val_loss improved from 1.06159 to 1.04754, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0191 - accuracy: 0.6553 - val_loss: 1.0475 - val_accuracy: 0.6528\n",
      "Epoch 53/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9969 - accuracy: 0.6739\n",
      "Epoch 00053: val_loss did not improve from 1.04754\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9983 - accuracy: 0.6739 - val_loss: 1.0589 - val_accuracy: 0.6419\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9999 - accuracy: 0.6668\n",
      "Epoch 00054: val_loss did not improve from 1.04754\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9999 - accuracy: 0.6668 - val_loss: 1.0548 - val_accuracy: 0.6473\n",
      "Epoch 55/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0000 - accuracy: 0.6683\n",
      "Epoch 00055: val_loss improved from 1.04754 to 1.03923, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9999 - accuracy: 0.6693 - val_loss: 1.0392 - val_accuracy: 0.6542\n",
      "Epoch 56/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9867 - accuracy: 0.6717\n",
      "Epoch 00056: val_loss did not improve from 1.03923\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9886 - accuracy: 0.6714 - val_loss: 1.0413 - val_accuracy: 0.6460\n",
      "Epoch 57/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9879 - accuracy: 0.6713\n",
      "Epoch 00057: val_loss did not improve from 1.03923\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.9909 - accuracy: 0.6707 - val_loss: 1.0793 - val_accuracy: 0.6255\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0020 - accuracy: 0.6644\n",
      "Epoch 00058: val_loss improved from 1.03923 to 1.03860, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0020 - accuracy: 0.6644 - val_loss: 1.0386 - val_accuracy: 0.6480\n",
      "Epoch 59/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9907 - accuracy: 0.6672\n",
      "Epoch 00059: val_loss improved from 1.03860 to 1.03531, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9865 - accuracy: 0.6692 - val_loss: 1.0353 - val_accuracy: 0.6596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9835 - accuracy: 0.6736\n",
      "Epoch 00060: val_loss improved from 1.03531 to 1.02470, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9852 - accuracy: 0.6722 - val_loss: 1.0247 - val_accuracy: 0.6548\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9781 - accuracy: 0.6775\n",
      "Epoch 00061: val_loss improved from 1.02470 to 1.01982, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9781 - accuracy: 0.6775 - val_loss: 1.0198 - val_accuracy: 0.6535\n",
      "Epoch 62/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.9802 - accuracy: 0.6711\n",
      "Epoch 00062: val_loss did not improve from 1.01982\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9734 - accuracy: 0.6755 - val_loss: 1.0368 - val_accuracy: 0.6480\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9704 - accuracy: 0.6735\n",
      "Epoch 00063: val_loss improved from 1.01982 to 1.01212, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9725 - accuracy: 0.6741 - val_loss: 1.0121 - val_accuracy: 0.6698\n",
      "Epoch 64/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9685 - accuracy: 0.6795\n",
      "Epoch 00064: val_loss did not improve from 1.01212\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9629 - accuracy: 0.6823 - val_loss: 1.0185 - val_accuracy: 0.6630\n",
      "Epoch 65/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9616 - accuracy: 0.6852\n",
      "Epoch 00065: val_loss improved from 1.01212 to 1.00012, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9592 - accuracy: 0.6869 - val_loss: 1.0001 - val_accuracy: 0.6746\n",
      "Epoch 66/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9567 - accuracy: 0.6830\n",
      "Epoch 00066: val_loss did not improve from 1.00012\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9564 - accuracy: 0.6830 - val_loss: 1.0051 - val_accuracy: 0.6726\n",
      "Epoch 67/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9423 - accuracy: 0.6935\n",
      "Epoch 00067: val_loss improved from 1.00012 to 0.99526, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9482 - accuracy: 0.6913 - val_loss: 0.9953 - val_accuracy: 0.6780\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9553 - accuracy: 0.6852\n",
      "Epoch 00068: val_loss did not improve from 0.99526\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9553 - accuracy: 0.6852 - val_loss: 1.0116 - val_accuracy: 0.6685\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9667 - accuracy: 0.6815\n",
      "Epoch 00069: val_loss did not improve from 0.99526\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9629 - accuracy: 0.6820 - val_loss: 1.0087 - val_accuracy: 0.6685\n",
      "Epoch 70/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9635 - accuracy: 0.6735\n",
      "Epoch 00070: val_loss did not improve from 0.99526\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9602 - accuracy: 0.6751 - val_loss: 1.0164 - val_accuracy: 0.6637\n",
      "Epoch 71/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9413 - accuracy: 0.6905\n",
      "Epoch 00071: val_loss improved from 0.99526 to 0.98338, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9415 - accuracy: 0.6919 - val_loss: 0.9834 - val_accuracy: 0.6801\n",
      "Epoch 72/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.9334 - accuracy: 0.6972\n",
      "Epoch 00072: val_loss did not improve from 0.98338\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9333 - accuracy: 0.6965 - val_loss: 0.9901 - val_accuracy: 0.6623\n",
      "Epoch 73/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9246 - accuracy: 0.6928\n",
      "Epoch 00073: val_loss did not improve from 0.98338\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9341 - accuracy: 0.6900 - val_loss: 0.9861 - val_accuracy: 0.6664\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9308 - accuracy: 0.6934\n",
      "Epoch 00074: val_loss improved from 0.98338 to 0.98062, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9308 - accuracy: 0.6934 - val_loss: 0.9806 - val_accuracy: 0.6842\n",
      "Epoch 75/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9212 - accuracy: 0.6992\n",
      "Epoch 00075: val_loss improved from 0.98062 to 0.97811, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9274 - accuracy: 0.6990 - val_loss: 0.9781 - val_accuracy: 0.6842\n",
      "Epoch 76/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.9277 - accuracy: 0.6931\n",
      "Epoch 00076: val_loss improved from 0.97811 to 0.97197, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9253 - accuracy: 0.6953 - val_loss: 0.9720 - val_accuracy: 0.6760\n",
      "Epoch 77/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9268 - accuracy: 0.6900\n",
      "Epoch 00077: val_loss improved from 0.97197 to 0.96728, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9218 - accuracy: 0.6941 - val_loss: 0.9673 - val_accuracy: 0.6705\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9218 - accuracy: 0.6970\n",
      "Epoch 00078: val_loss did not improve from 0.96728\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9218 - accuracy: 0.6970 - val_loss: 0.9685 - val_accuracy: 0.6746\n",
      "Epoch 79/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.9191 - accuracy: 0.7048\n",
      "Epoch 00079: val_loss did not improve from 0.96728\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9174 - accuracy: 0.7014 - val_loss: 0.9675 - val_accuracy: 0.6760\n",
      "Epoch 80/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9161 - accuracy: 0.7012\n",
      "Epoch 00080: val_loss improved from 0.96728 to 0.96488, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9117 - accuracy: 0.7021 - val_loss: 0.9649 - val_accuracy: 0.6842\n",
      "Epoch 81/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9071 - accuracy: 0.7026\n",
      "Epoch 00081: val_loss improved from 0.96488 to 0.95677, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9146 - accuracy: 0.6997 - val_loss: 0.9568 - val_accuracy: 0.6835\n",
      "Epoch 82/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.9135 - accuracy: 0.7000\n",
      "Epoch 00082: val_loss did not improve from 0.95677\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9139 - accuracy: 0.6990 - val_loss: 0.9955 - val_accuracy: 0.6658\n",
      "Epoch 83/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9055 - accuracy: 0.6965\n",
      "Epoch 00083: val_loss did not improve from 0.95677\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.9101 - accuracy: 0.6960 - val_loss: 0.9639 - val_accuracy: 0.6767\n",
      "Epoch 84/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9130 - accuracy: 0.6951\n",
      "Epoch 00084: val_loss did not improve from 0.95677\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9111 - accuracy: 0.6951 - val_loss: 0.9621 - val_accuracy: 0.6808\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9041 - accuracy: 0.7021\n",
      "Epoch 00085: val_loss improved from 0.95677 to 0.95452, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9041 - accuracy: 0.7021 - val_loss: 0.9545 - val_accuracy: 0.6876\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9031 - accuracy: 0.7033\n",
      "Epoch 00086: val_loss improved from 0.95452 to 0.94664, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9052 - accuracy: 0.7031 - val_loss: 0.9466 - val_accuracy: 0.6828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8901 - accuracy: 0.7087\n",
      "Epoch 00087: val_loss did not improve from 0.94664\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8945 - accuracy: 0.7070 - val_loss: 0.9524 - val_accuracy: 0.6760\n",
      "Epoch 88/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8895 - accuracy: 0.7093\n",
      "Epoch 00088: val_loss did not improve from 0.94664\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8909 - accuracy: 0.7076 - val_loss: 0.9483 - val_accuracy: 0.6930\n",
      "Epoch 89/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8974 - accuracy: 0.7024\n",
      "Epoch 00089: val_loss improved from 0.94664 to 0.94189, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8944 - accuracy: 0.7030 - val_loss: 0.9419 - val_accuracy: 0.6937\n",
      "Epoch 90/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.9092 - accuracy: 0.6974\n",
      "Epoch 00090: val_loss did not improve from 0.94189\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8965 - accuracy: 0.7014 - val_loss: 0.9428 - val_accuracy: 0.6842\n",
      "Epoch 91/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8811 - accuracy: 0.7108\n",
      "Epoch 00091: val_loss improved from 0.94189 to 0.93785, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8849 - accuracy: 0.7081 - val_loss: 0.9379 - val_accuracy: 0.6924\n",
      "Epoch 92/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8855 - accuracy: 0.7065\n",
      "Epoch 00092: val_loss did not improve from 0.93785\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8848 - accuracy: 0.7074 - val_loss: 0.9420 - val_accuracy: 0.6917\n",
      "Epoch 93/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8834 - accuracy: 0.7100\n",
      "Epoch 00093: val_loss improved from 0.93785 to 0.93250, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8824 - accuracy: 0.7093 - val_loss: 0.9325 - val_accuracy: 0.6917\n",
      "Epoch 94/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8932 - accuracy: 0.7042\n",
      "Epoch 00094: val_loss did not improve from 0.93250\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8828 - accuracy: 0.7062 - val_loss: 0.9348 - val_accuracy: 0.6910\n",
      "Epoch 95/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8790 - accuracy: 0.7080\n",
      "Epoch 00095: val_loss improved from 0.93250 to 0.93249, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8814 - accuracy: 0.7062 - val_loss: 0.9325 - val_accuracy: 0.7005\n",
      "Epoch 96/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8727 - accuracy: 0.7084\n",
      "Epoch 00096: val_loss did not improve from 0.93249\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8798 - accuracy: 0.7082 - val_loss: 0.9502 - val_accuracy: 0.6821\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8815 - accuracy: 0.7089\n",
      "Epoch 00097: val_loss improved from 0.93249 to 0.92829, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8815 - accuracy: 0.7089 - val_loss: 0.9283 - val_accuracy: 0.6924\n",
      "Epoch 98/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8818 - accuracy: 0.7109\n",
      "Epoch 00098: val_loss improved from 0.92829 to 0.92764, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8788 - accuracy: 0.7103 - val_loss: 0.9276 - val_accuracy: 0.6910\n",
      "Epoch 99/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8813 - accuracy: 0.7015\n",
      "Epoch 00099: val_loss improved from 0.92764 to 0.91684, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8797 - accuracy: 0.7041 - val_loss: 0.9168 - val_accuracy: 0.7053\n",
      "Epoch 100/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8648 - accuracy: 0.7189\n",
      "Epoch 00100: val_loss did not improve from 0.91684\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8702 - accuracy: 0.7142 - val_loss: 0.9181 - val_accuracy: 0.6985\n",
      "Epoch 101/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8732 - accuracy: 0.7104\n",
      "Epoch 00101: val_loss did not improve from 0.91684\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8703 - accuracy: 0.7103 - val_loss: 0.9226 - val_accuracy: 0.7026\n",
      "Epoch 102/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8685 - accuracy: 0.7047\n",
      "Epoch 00102: val_loss improved from 0.91684 to 0.91253, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8706 - accuracy: 0.7043 - val_loss: 0.9125 - val_accuracy: 0.6992\n",
      "Epoch 103/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8625 - accuracy: 0.7120\n",
      "Epoch 00103: val_loss improved from 0.91253 to 0.90951, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8595 - accuracy: 0.7169 - val_loss: 0.9095 - val_accuracy: 0.6944\n",
      "Epoch 104/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8546 - accuracy: 0.7193\n",
      "Epoch 00104: val_loss did not improve from 0.90951\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8624 - accuracy: 0.7168 - val_loss: 0.9156 - val_accuracy: 0.7012\n",
      "Epoch 105/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8640 - accuracy: 0.7126\n",
      "Epoch 00105: val_loss improved from 0.90951 to 0.90572, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8635 - accuracy: 0.7134 - val_loss: 0.9057 - val_accuracy: 0.6978\n",
      "Epoch 106/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8789 - accuracy: 0.7111\n",
      "Epoch 00106: val_loss did not improve from 0.90572\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8613 - accuracy: 0.7188 - val_loss: 0.9149 - val_accuracy: 0.6958\n",
      "Epoch 107/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8730 - accuracy: 0.7173\n",
      "Epoch 00107: val_loss did not improve from 0.90572\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8687 - accuracy: 0.7173 - val_loss: 0.9291 - val_accuracy: 0.6903\n",
      "Epoch 108/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8520 - accuracy: 0.7160\n",
      "Epoch 00108: val_loss improved from 0.90572 to 0.90515, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8527 - accuracy: 0.7169 - val_loss: 0.9051 - val_accuracy: 0.7026\n",
      "Epoch 109/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8513 - accuracy: 0.7174\n",
      "Epoch 00109: val_loss did not improve from 0.90515\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8591 - accuracy: 0.7132 - val_loss: 0.9114 - val_accuracy: 0.6896\n",
      "Epoch 110/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8581 - accuracy: 0.7186\n",
      "Epoch 00110: val_loss did not improve from 0.90515\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.8518 - accuracy: 0.7210 - val_loss: 0.9123 - val_accuracy: 0.7005\n",
      "Epoch 111/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8551 - accuracy: 0.7117\n",
      "Epoch 00111: val_loss did not improve from 0.90515\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8525 - accuracy: 0.7125 - val_loss: 0.9133 - val_accuracy: 0.6849\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8460 - accuracy: 0.7217\n",
      "Epoch 00112: val_loss improved from 0.90515 to 0.90203, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8494 - accuracy: 0.7202 - val_loss: 0.9020 - val_accuracy: 0.6930\n",
      "Epoch 113/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8440 - accuracy: 0.7244\n",
      "Epoch 00113: val_loss did not improve from 0.90203\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8433 - accuracy: 0.7234 - val_loss: 0.9134 - val_accuracy: 0.6889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8502 - accuracy: 0.7184\n",
      "Epoch 00114: val_loss improved from 0.90203 to 0.88628, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8467 - accuracy: 0.7176 - val_loss: 0.8863 - val_accuracy: 0.7087\n",
      "Epoch 115/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8516 - accuracy: 0.7227\n",
      "Epoch 00115: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8461 - accuracy: 0.7222 - val_loss: 0.9123 - val_accuracy: 0.7074\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8485 - accuracy: 0.7210\n",
      "Epoch 00116: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8552 - accuracy: 0.7161 - val_loss: 0.9002 - val_accuracy: 0.7005\n",
      "Epoch 117/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8472 - accuracy: 0.7202\n",
      "Epoch 00117: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8449 - accuracy: 0.7202 - val_loss: 0.8901 - val_accuracy: 0.7053\n",
      "Epoch 118/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8403 - accuracy: 0.7259\n",
      "Epoch 00118: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8438 - accuracy: 0.7222 - val_loss: 0.8888 - val_accuracy: 0.7019\n",
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8337 - accuracy: 0.7301\n",
      "Epoch 00119: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8374 - accuracy: 0.7272 - val_loss: 0.8961 - val_accuracy: 0.7026\n",
      "Epoch 120/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8312 - accuracy: 0.7245\n",
      "Epoch 00120: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8363 - accuracy: 0.7238 - val_loss: 0.8913 - val_accuracy: 0.7053\n",
      "Epoch 121/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8435 - accuracy: 0.7201\n",
      "Epoch 00121: val_loss did not improve from 0.88628\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8378 - accuracy: 0.7212 - val_loss: 0.8947 - val_accuracy: 0.7012\n",
      "Epoch 122/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8317 - accuracy: 0.7209\n",
      "Epoch 00122: val_loss improved from 0.88628 to 0.87158, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8359 - accuracy: 0.7204 - val_loss: 0.8716 - val_accuracy: 0.7101\n",
      "Epoch 123/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8381 - accuracy: 0.7163\n",
      "Epoch 00123: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8373 - accuracy: 0.7171 - val_loss: 0.8783 - val_accuracy: 0.7040\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.7292\n",
      "Epoch 00124: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8258 - accuracy: 0.7292 - val_loss: 0.8960 - val_accuracy: 0.6903\n",
      "Epoch 125/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8356 - accuracy: 0.7191\n",
      "Epoch 00125: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8345 - accuracy: 0.7207 - val_loss: 0.8739 - val_accuracy: 0.7115\n",
      "Epoch 126/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8252 - accuracy: 0.7303\n",
      "Epoch 00126: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8248 - accuracy: 0.7282 - val_loss: 0.8730 - val_accuracy: 0.7121\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8258 - accuracy: 0.7243\n",
      "Epoch 00127: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8216 - accuracy: 0.7262 - val_loss: 0.8777 - val_accuracy: 0.7005\n",
      "Epoch 128/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8290 - accuracy: 0.7219\n",
      "Epoch 00128: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8296 - accuracy: 0.7222 - val_loss: 0.8813 - val_accuracy: 0.7026\n",
      "Epoch 129/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8313 - accuracy: 0.7244\n",
      "Epoch 00129: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8237 - accuracy: 0.7265 - val_loss: 0.8878 - val_accuracy: 0.6951\n",
      "Epoch 130/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8219 - accuracy: 0.7314\n",
      "Epoch 00130: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8227 - accuracy: 0.7285 - val_loss: 0.8764 - val_accuracy: 0.6999\n",
      "Epoch 131/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8164 - accuracy: 0.7294\n",
      "Epoch 00131: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8156 - accuracy: 0.7314 - val_loss: 0.8734 - val_accuracy: 0.7121\n",
      "Epoch 132/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8124 - accuracy: 0.7307\n",
      "Epoch 00132: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8168 - accuracy: 0.7275 - val_loss: 0.8774 - val_accuracy: 0.7040\n",
      "Epoch 133/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8177 - accuracy: 0.7282\n",
      "Epoch 00133: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8230 - accuracy: 0.7265 - val_loss: 0.9221 - val_accuracy: 0.6760\n",
      "Epoch 134/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8329 - accuracy: 0.7208\n",
      "Epoch 00134: val_loss did not improve from 0.87158\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8265 - accuracy: 0.7210 - val_loss: 0.8868 - val_accuracy: 0.6992\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8197 - accuracy: 0.7262\n",
      "Epoch 00135: val_loss improved from 0.87158 to 0.86647, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8187 - accuracy: 0.7258 - val_loss: 0.8665 - val_accuracy: 0.7108\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8215 - accuracy: 0.7236\n",
      "Epoch 00136: val_loss did not improve from 0.86647\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8215 - accuracy: 0.7236 - val_loss: 0.8729 - val_accuracy: 0.7019\n",
      "Epoch 137/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8111 - accuracy: 0.7309\n",
      "Epoch 00137: val_loss improved from 0.86647 to 0.86330, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8170 - accuracy: 0.7287 - val_loss: 0.8633 - val_accuracy: 0.7005\n",
      "Epoch 138/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8215 - accuracy: 0.7281\n",
      "Epoch 00138: val_loss improved from 0.86330 to 0.85770, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8171 - accuracy: 0.7299 - val_loss: 0.8577 - val_accuracy: 0.7169\n",
      "Epoch 139/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.8124 - accuracy: 0.7327\n",
      "Epoch 00139: val_loss did not improve from 0.85770\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8098 - accuracy: 0.7340 - val_loss: 0.8622 - val_accuracy: 0.7053\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8089 - accuracy: 0.7340\n",
      "Epoch 00140: val_loss did not improve from 0.85770\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8089 - accuracy: 0.7340 - val_loss: 0.8654 - val_accuracy: 0.7121\n",
      "Epoch 141/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8021 - accuracy: 0.7355\n",
      "Epoch 00141: val_loss improved from 0.85770 to 0.85136, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8034 - accuracy: 0.7337 - val_loss: 0.8514 - val_accuracy: 0.7156\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.7338\n",
      "Epoch 00142: val_loss did not improve from 0.85136\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8020 - accuracy: 0.7338 - val_loss: 0.8590 - val_accuracy: 0.7149\n",
      "Epoch 143/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7949 - accuracy: 0.7407\n",
      "Epoch 00143: val_loss improved from 0.85136 to 0.84724, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8009 - accuracy: 0.7398 - val_loss: 0.8472 - val_accuracy: 0.7196\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8025 - accuracy: 0.7309\n",
      "Epoch 00144: val_loss improved from 0.84724 to 0.84425, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8025 - accuracy: 0.7309 - val_loss: 0.8442 - val_accuracy: 0.7149\n",
      "Epoch 145/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7977 - accuracy: 0.7321\n",
      "Epoch 00145: val_loss did not improve from 0.84425\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7973 - accuracy: 0.7323 - val_loss: 0.8581 - val_accuracy: 0.7128\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.7338\n",
      "Epoch 00146: val_loss did not improve from 0.84425\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8024 - accuracy: 0.7338 - val_loss: 0.8719 - val_accuracy: 0.6992\n",
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7937 - accuracy: 0.7351\n",
      "Epoch 00147: val_loss did not improve from 0.84425\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7997 - accuracy: 0.7349 - val_loss: 0.8633 - val_accuracy: 0.7026\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7964 - accuracy: 0.7412\n",
      "Epoch 00148: val_loss did not improve from 0.84425\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7964 - accuracy: 0.7412 - val_loss: 0.8472 - val_accuracy: 0.7169\n",
      "Epoch 149/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7960 - accuracy: 0.7373\n",
      "Epoch 00149: val_loss did not improve from 0.84425\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7950 - accuracy: 0.7379 - val_loss: 0.8470 - val_accuracy: 0.7142\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8013 - accuracy: 0.7294\n",
      "Epoch 00150: val_loss improved from 0.84425 to 0.84179, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8013 - accuracy: 0.7294 - val_loss: 0.8418 - val_accuracy: 0.7108\n",
      "Epoch 151/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7894 - accuracy: 0.7388\n",
      "Epoch 00151: val_loss did not improve from 0.84179\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7918 - accuracy: 0.7371 - val_loss: 0.8601 - val_accuracy: 0.7108\n",
      "Epoch 152/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8080 - accuracy: 0.7311\n",
      "Epoch 00152: val_loss improved from 0.84179 to 0.83555, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8016 - accuracy: 0.7321 - val_loss: 0.8355 - val_accuracy: 0.7237\n",
      "Epoch 153/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7872 - accuracy: 0.7413\n",
      "Epoch 00153: val_loss did not improve from 0.83555\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7840 - accuracy: 0.7408 - val_loss: 0.8560 - val_accuracy: 0.7101\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7945 - accuracy: 0.7417\n",
      "Epoch 00154: val_loss did not improve from 0.83555\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7945 - accuracy: 0.7417 - val_loss: 0.8461 - val_accuracy: 0.7196\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7880 - accuracy: 0.7401\n",
      "Epoch 00155: val_loss did not improve from 0.83555\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7880 - accuracy: 0.7401 - val_loss: 0.8562 - val_accuracy: 0.7087\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7917 - accuracy: 0.7328\n",
      "Epoch 00156: val_loss did not improve from 0.83555\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7917 - accuracy: 0.7328 - val_loss: 0.8415 - val_accuracy: 0.7203\n",
      "Epoch 157/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7977 - accuracy: 0.7323\n",
      "Epoch 00157: val_loss did not improve from 0.83555\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7938 - accuracy: 0.7335 - val_loss: 0.8408 - val_accuracy: 0.7169\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7826 - accuracy: 0.7396\n",
      "Epoch 00158: val_loss did not improve from 0.83555\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7826 - accuracy: 0.7396 - val_loss: 0.8433 - val_accuracy: 0.7210\n",
      "Epoch 159/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7942 - accuracy: 0.7414\n",
      "Epoch 00159: val_loss improved from 0.83555 to 0.83202, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7898 - accuracy: 0.7408 - val_loss: 0.8320 - val_accuracy: 0.7244\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7843 - accuracy: 0.7395\n",
      "Epoch 00160: val_loss did not improve from 0.83202\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7843 - accuracy: 0.7395 - val_loss: 0.8593 - val_accuracy: 0.7040\n",
      "Epoch 161/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7910 - accuracy: 0.7362\n",
      "Epoch 00161: val_loss did not improve from 0.83202\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7926 - accuracy: 0.7372 - val_loss: 0.8632 - val_accuracy: 0.7087\n",
      "Epoch 162/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7861 - accuracy: 0.7401\n",
      "Epoch 00162: val_loss improved from 0.83202 to 0.82304, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7865 - accuracy: 0.7376 - val_loss: 0.8230 - val_accuracy: 0.7265\n",
      "Epoch 163/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7723 - accuracy: 0.7418\n",
      "Epoch 00163: val_loss did not improve from 0.82304\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7753 - accuracy: 0.7417 - val_loss: 0.8462 - val_accuracy: 0.7224\n",
      "Epoch 164/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7810 - accuracy: 0.7442\n",
      "Epoch 00164: val_loss did not improve from 0.82304\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7815 - accuracy: 0.7436 - val_loss: 0.8294 - val_accuracy: 0.7162\n",
      "Epoch 165/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7749 - accuracy: 0.7394\n",
      "Epoch 00165: val_loss improved from 0.82304 to 0.81768, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7722 - accuracy: 0.7413 - val_loss: 0.8177 - val_accuracy: 0.7271\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7816 - accuracy: 0.7388\n",
      "Epoch 00166: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7755 - accuracy: 0.7419 - val_loss: 0.8454 - val_accuracy: 0.7135\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7758 - accuracy: 0.7398\n",
      "Epoch 00167: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7758 - accuracy: 0.7398 - val_loss: 0.8403 - val_accuracy: 0.7244\n",
      "Epoch 168/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7889 - accuracy: 0.7368\n",
      "Epoch 00168: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7894 - accuracy: 0.7354 - val_loss: 0.8752 - val_accuracy: 0.6971\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7857 - accuracy: 0.7390\n",
      "Epoch 00169: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7857 - accuracy: 0.7390 - val_loss: 0.8330 - val_accuracy: 0.7203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7609 - accuracy: 0.7502\n",
      "Epoch 00170: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7682 - accuracy: 0.7458 - val_loss: 0.8196 - val_accuracy: 0.7299\n",
      "Epoch 171/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7736 - accuracy: 0.7431\n",
      "Epoch 00171: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7710 - accuracy: 0.7446 - val_loss: 0.8207 - val_accuracy: 0.7278\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7726 - accuracy: 0.7473\n",
      "Epoch 00172: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7726 - accuracy: 0.7473 - val_loss: 0.8243 - val_accuracy: 0.7237\n",
      "Epoch 173/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7856 - accuracy: 0.7342\n",
      "Epoch 00173: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7911 - accuracy: 0.7308 - val_loss: 0.8358 - val_accuracy: 0.7258\n",
      "Epoch 174/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7748 - accuracy: 0.7401\n",
      "Epoch 00174: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7722 - accuracy: 0.7405 - val_loss: 0.8283 - val_accuracy: 0.7285\n",
      "Epoch 175/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7648 - accuracy: 0.7482\n",
      "Epoch 00175: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7670 - accuracy: 0.7451 - val_loss: 0.8179 - val_accuracy: 0.7237\n",
      "Epoch 176/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7707 - accuracy: 0.7387\n",
      "Epoch 00176: val_loss did not improve from 0.81768\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7709 - accuracy: 0.7391 - val_loss: 0.8358 - val_accuracy: 0.7306\n",
      "Epoch 177/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7629 - accuracy: 0.7468\n",
      "Epoch 00177: val_loss improved from 0.81768 to 0.80936, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7643 - accuracy: 0.7442 - val_loss: 0.8094 - val_accuracy: 0.7312\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.7495\n",
      "Epoch 00178: val_loss did not improve from 0.80936\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7594 - accuracy: 0.7495 - val_loss: 0.8242 - val_accuracy: 0.7237\n",
      "Epoch 179/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7578 - accuracy: 0.7512\n",
      "Epoch 00179: val_loss did not improve from 0.80936\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7633 - accuracy: 0.7500 - val_loss: 0.8214 - val_accuracy: 0.7326\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7604 - accuracy: 0.7507\n",
      "Epoch 00180: val_loss did not improve from 0.80936\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7604 - accuracy: 0.7507 - val_loss: 0.8209 - val_accuracy: 0.7271\n",
      "Epoch 181/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7649 - accuracy: 0.7455\n",
      "Epoch 00181: val_loss did not improve from 0.80936\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7631 - accuracy: 0.7451 - val_loss: 0.8141 - val_accuracy: 0.7237\n",
      "Epoch 182/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7666 - accuracy: 0.7461\n",
      "Epoch 00182: val_loss improved from 0.80936 to 0.80589, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7626 - accuracy: 0.7482 - val_loss: 0.8059 - val_accuracy: 0.7347\n",
      "Epoch 183/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7676 - accuracy: 0.7470\n",
      "Epoch 00183: val_loss did not improve from 0.80589\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7662 - accuracy: 0.7471 - val_loss: 0.8317 - val_accuracy: 0.7135\n",
      "Epoch 184/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7530 - accuracy: 0.7481\n",
      "Epoch 00184: val_loss did not improve from 0.80589\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7593 - accuracy: 0.7471 - val_loss: 0.8098 - val_accuracy: 0.7360\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7625 - accuracy: 0.7386\n",
      "Epoch 00185: val_loss did not improve from 0.80589\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7625 - accuracy: 0.7386 - val_loss: 0.8324 - val_accuracy: 0.7244\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7619 - accuracy: 0.7441\n",
      "Epoch 00186: val_loss did not improve from 0.80589\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7619 - accuracy: 0.7441 - val_loss: 0.8125 - val_accuracy: 0.7333\n",
      "Epoch 187/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7420 - accuracy: 0.7590\n",
      "Epoch 00187: val_loss improved from 0.80589 to 0.80298, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7523 - accuracy: 0.7524 - val_loss: 0.8030 - val_accuracy: 0.7306\n",
      "Epoch 188/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7526 - accuracy: 0.7502\n",
      "Epoch 00188: val_loss improved from 0.80298 to 0.79853, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7556 - accuracy: 0.7480 - val_loss: 0.7985 - val_accuracy: 0.7381\n",
      "Epoch 189/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7491 - accuracy: 0.7489\n",
      "Epoch 00189: val_loss did not improve from 0.79853\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7567 - accuracy: 0.7471 - val_loss: 0.8107 - val_accuracy: 0.7312\n",
      "Epoch 190/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7530 - accuracy: 0.7514\n",
      "Epoch 00190: val_loss did not improve from 0.79853\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7515 - accuracy: 0.7519 - val_loss: 0.8050 - val_accuracy: 0.7367\n",
      "Epoch 191/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7470 - accuracy: 0.7495\n",
      "Epoch 00191: val_loss did not improve from 0.79853\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7469 - accuracy: 0.7504 - val_loss: 0.8003 - val_accuracy: 0.7367\n",
      "Epoch 192/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7511 - accuracy: 0.7472\n",
      "Epoch 00192: val_loss improved from 0.79853 to 0.79699, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7513 - accuracy: 0.7482 - val_loss: 0.7970 - val_accuracy: 0.7319\n",
      "Epoch 193/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7606 - accuracy: 0.7447\n",
      "Epoch 00193: val_loss improved from 0.79699 to 0.79357, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7530 - accuracy: 0.7480 - val_loss: 0.7936 - val_accuracy: 0.7347\n",
      "Epoch 194/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7509 - accuracy: 0.7465\n",
      "Epoch 00194: val_loss improved from 0.79357 to 0.79178, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7447 - accuracy: 0.7506 - val_loss: 0.7918 - val_accuracy: 0.7381\n",
      "Epoch 195/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7374 - accuracy: 0.7507\n",
      "Epoch 00195: val_loss did not improve from 0.79178\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7458 - accuracy: 0.7485 - val_loss: 0.8030 - val_accuracy: 0.7367\n",
      "Epoch 196/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7601 - accuracy: 0.7481\n",
      "Epoch 00196: val_loss did not improve from 0.79178\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7569 - accuracy: 0.7488 - val_loss: 0.8020 - val_accuracy: 0.7299\n",
      "Epoch 197/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7523 - accuracy: 0.7480\n",
      "Epoch 00197: val_loss did not improve from 0.79178\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7481 - accuracy: 0.7517 - val_loss: 0.8072 - val_accuracy: 0.7203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7478 - accuracy: 0.7521\n",
      "Epoch 00198: val_loss did not improve from 0.79178\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7478 - accuracy: 0.7521 - val_loss: 0.8070 - val_accuracy: 0.7306\n",
      "Epoch 199/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7457 - accuracy: 0.7514\n",
      "Epoch 00199: val_loss did not improve from 0.79178\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7421 - accuracy: 0.7517 - val_loss: 0.8006 - val_accuracy: 0.7278\n",
      "Epoch 200/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7405 - accuracy: 0.7553\n",
      "Epoch 00200: val_loss did not improve from 0.79178\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7449 - accuracy: 0.7536 - val_loss: 0.7936 - val_accuracy: 0.7360\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7378 - accuracy: 0.7517\n",
      "Epoch 00201: val_loss improved from 0.79178 to 0.78844, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7378 - accuracy: 0.7517 - val_loss: 0.7884 - val_accuracy: 0.7360\n",
      "Epoch 202/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7341 - accuracy: 0.7593\n",
      "Epoch 00202: val_loss did not improve from 0.78844\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7324 - accuracy: 0.7594 - val_loss: 0.7914 - val_accuracy: 0.7326\n",
      "Epoch 203/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7356 - accuracy: 0.7518\n",
      "Epoch 00203: val_loss improved from 0.78844 to 0.78411, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7432 - accuracy: 0.7495 - val_loss: 0.7841 - val_accuracy: 0.7428\n",
      "Epoch 204/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7420 - accuracy: 0.7494\n",
      "Epoch 00204: val_loss improved from 0.78411 to 0.77955, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7377 - accuracy: 0.7528 - val_loss: 0.7796 - val_accuracy: 0.7374\n",
      "Epoch 205/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7281 - accuracy: 0.7570\n",
      "Epoch 00205: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7383 - accuracy: 0.7543 - val_loss: 0.7890 - val_accuracy: 0.7360\n",
      "Epoch 206/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7334 - accuracy: 0.7549\n",
      "Epoch 00206: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7456 - accuracy: 0.7483 - val_loss: 0.7940 - val_accuracy: 0.7374\n",
      "Epoch 207/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7575 - accuracy: 0.7416\n",
      "Epoch 00207: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7519 - accuracy: 0.7436 - val_loss: 0.7938 - val_accuracy: 0.7306\n",
      "Epoch 208/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7335 - accuracy: 0.7561\n",
      "Epoch 00208: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7359 - accuracy: 0.7567 - val_loss: 0.7986 - val_accuracy: 0.7271\n",
      "Epoch 209/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7344 - accuracy: 0.7527\n",
      "Epoch 00209: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7364 - accuracy: 0.7516 - val_loss: 0.7825 - val_accuracy: 0.7353\n",
      "Epoch 210/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7305 - accuracy: 0.7612\n",
      "Epoch 00210: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7283 - accuracy: 0.7608 - val_loss: 0.7919 - val_accuracy: 0.7312\n",
      "Epoch 211/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7319 - accuracy: 0.7574\n",
      "Epoch 00211: val_loss did not improve from 0.77955\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7467 - accuracy: 0.7524 - val_loss: 0.7922 - val_accuracy: 0.7374\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7372 - accuracy: 0.7550\n",
      "Epoch 00212: val_loss improved from 0.77955 to 0.77645, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7372 - accuracy: 0.7550 - val_loss: 0.7764 - val_accuracy: 0.7360\n",
      "Epoch 213/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7386 - accuracy: 0.7502\n",
      "Epoch 00213: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7343 - accuracy: 0.7528 - val_loss: 0.7805 - val_accuracy: 0.7387\n",
      "Epoch 214/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7327 - accuracy: 0.7555\n",
      "Epoch 00214: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7300 - accuracy: 0.7557 - val_loss: 0.7791 - val_accuracy: 0.7442\n",
      "Epoch 215/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7467 - accuracy: 0.7566\n",
      "Epoch 00215: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7348 - accuracy: 0.7557 - val_loss: 0.7835 - val_accuracy: 0.7360\n",
      "Epoch 216/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7337 - accuracy: 0.7568\n",
      "Epoch 00216: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7320 - accuracy: 0.7575 - val_loss: 0.7955 - val_accuracy: 0.7251\n",
      "Epoch 217/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7362 - accuracy: 0.7502\n",
      "Epoch 00217: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7369 - accuracy: 0.7500 - val_loss: 0.7771 - val_accuracy: 0.7360\n",
      "Epoch 218/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7394 - accuracy: 0.7580\n",
      "Epoch 00218: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7252 - accuracy: 0.7601 - val_loss: 0.8088 - val_accuracy: 0.7142\n",
      "Epoch 219/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7335 - accuracy: 0.7556\n",
      "Epoch 00219: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7342 - accuracy: 0.7540 - val_loss: 0.7848 - val_accuracy: 0.7374\n",
      "Epoch 220/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7421 - accuracy: 0.7521\n",
      "Epoch 00220: val_loss did not improve from 0.77645\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7398 - accuracy: 0.7526 - val_loss: 0.7859 - val_accuracy: 0.7401\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7315 - accuracy: 0.7565\n",
      "Epoch 00221: val_loss improved from 0.77645 to 0.77104, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7315 - accuracy: 0.7565 - val_loss: 0.7710 - val_accuracy: 0.7422\n",
      "Epoch 222/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7344 - accuracy: 0.7512\n",
      "Epoch 00222: val_loss did not improve from 0.77104\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7382 - accuracy: 0.7499 - val_loss: 0.8198 - val_accuracy: 0.7190\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7379 - accuracy: 0.7560\n",
      "Epoch 00223: val_loss did not improve from 0.77104\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7379 - accuracy: 0.7560 - val_loss: 0.7922 - val_accuracy: 0.7271\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.7589\n",
      "Epoch 00224: val_loss improved from 0.77104 to 0.76830, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7196 - accuracy: 0.7589 - val_loss: 0.7683 - val_accuracy: 0.7374\n",
      "Epoch 225/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7209 - accuracy: 0.7611\n",
      "Epoch 00225: val_loss did not improve from 0.76830\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7286 - accuracy: 0.7562 - val_loss: 0.7684 - val_accuracy: 0.7428\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.7260 - accuracy: 0.7557\n",
      "Epoch 00226: val_loss did not improve from 0.76830\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7260 - accuracy: 0.7557 - val_loss: 0.8238 - val_accuracy: 0.7156\n",
      "Epoch 227/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7425 - accuracy: 0.7483\n",
      "Epoch 00227: val_loss did not improve from 0.76830\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7385 - accuracy: 0.7497 - val_loss: 0.7919 - val_accuracy: 0.7347\n",
      "Epoch 228/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7252 - accuracy: 0.7488\n",
      "Epoch 00228: val_loss did not improve from 0.76830\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7298 - accuracy: 0.7509 - val_loss: 0.7697 - val_accuracy: 0.7394\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7225 - accuracy: 0.7593\n",
      "Epoch 00229: val_loss improved from 0.76830 to 0.76770, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7225 - accuracy: 0.7593 - val_loss: 0.7677 - val_accuracy: 0.7394\n",
      "Epoch 230/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7221 - accuracy: 0.7541\n",
      "Epoch 00230: val_loss did not improve from 0.76770\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7262 - accuracy: 0.7548 - val_loss: 0.7769 - val_accuracy: 0.7387\n",
      "Epoch 231/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7103 - accuracy: 0.7602\n",
      "Epoch 00231: val_loss improved from 0.76770 to 0.75592, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7158 - accuracy: 0.7587 - val_loss: 0.7559 - val_accuracy: 0.7456\n",
      "Epoch 232/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7163 - accuracy: 0.7561\n",
      "Epoch 00232: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7118 - accuracy: 0.7581 - val_loss: 0.7684 - val_accuracy: 0.7428\n",
      "Epoch 233/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7034 - accuracy: 0.7660\n",
      "Epoch 00233: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7119 - accuracy: 0.7620 - val_loss: 0.7611 - val_accuracy: 0.7401\n",
      "Epoch 234/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7101 - accuracy: 0.7642\n",
      "Epoch 00234: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7125 - accuracy: 0.7634 - val_loss: 0.7597 - val_accuracy: 0.7408\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7194 - accuracy: 0.7611\n",
      "Epoch 00235: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7194 - accuracy: 0.7611 - val_loss: 0.7678 - val_accuracy: 0.7367\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7081 - accuracy: 0.7632\n",
      "Epoch 00236: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7081 - accuracy: 0.7632 - val_loss: 0.7672 - val_accuracy: 0.7360\n",
      "Epoch 237/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7156 - accuracy: 0.7584\n",
      "Epoch 00237: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7174 - accuracy: 0.7611 - val_loss: 0.7755 - val_accuracy: 0.7299\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.7605\n",
      "Epoch 00238: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7196 - accuracy: 0.7605 - val_loss: 0.7584 - val_accuracy: 0.7415\n",
      "Epoch 239/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7089 - accuracy: 0.7634\n",
      "Epoch 00239: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7107 - accuracy: 0.7634 - val_loss: 0.7939 - val_accuracy: 0.7285\n",
      "Epoch 240/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7239 - accuracy: 0.7617\n",
      "Epoch 00240: val_loss did not improve from 0.75592\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7141 - accuracy: 0.7651 - val_loss: 0.7712 - val_accuracy: 0.7415\n",
      "Epoch 241/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7135 - accuracy: 0.7607\n",
      "Epoch 00241: val_loss improved from 0.75592 to 0.75296, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7170 - accuracy: 0.7591 - val_loss: 0.7530 - val_accuracy: 0.7462\n",
      "Epoch 242/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7054 - accuracy: 0.7621\n",
      "Epoch 00242: val_loss did not improve from 0.75296\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7138 - accuracy: 0.7594 - val_loss: 0.7546 - val_accuracy: 0.7456\n",
      "Epoch 243/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7151 - accuracy: 0.7598\n",
      "Epoch 00243: val_loss did not improve from 0.75296\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7097 - accuracy: 0.7613 - val_loss: 0.7693 - val_accuracy: 0.7408\n",
      "Epoch 244/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7131 - accuracy: 0.7629\n",
      "Epoch 00244: val_loss did not improve from 0.75296\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7098 - accuracy: 0.7635 - val_loss: 0.7571 - val_accuracy: 0.7428\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.7649\n",
      "Epoch 00245: val_loss improved from 0.75296 to 0.74749, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7053 - accuracy: 0.7649 - val_loss: 0.7475 - val_accuracy: 0.7462\n",
      "Epoch 246/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7014 - accuracy: 0.7607\n",
      "Epoch 00246: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7025 - accuracy: 0.7628 - val_loss: 0.7687 - val_accuracy: 0.7340\n",
      "Epoch 247/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7132 - accuracy: 0.7576\n",
      "Epoch 00247: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7124 - accuracy: 0.7574 - val_loss: 0.7533 - val_accuracy: 0.7401\n",
      "Epoch 248/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7080 - accuracy: 0.7639\n",
      "Epoch 00248: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7021 - accuracy: 0.7657 - val_loss: 0.7547 - val_accuracy: 0.7408\n",
      "Epoch 249/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7117 - accuracy: 0.7604\n",
      "Epoch 00249: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7114 - accuracy: 0.7610 - val_loss: 0.7554 - val_accuracy: 0.7449\n",
      "Epoch 250/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7008 - accuracy: 0.7652\n",
      "Epoch 00250: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6981 - accuracy: 0.7627 - val_loss: 0.7510 - val_accuracy: 0.7422\n",
      "Epoch 251/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7134 - accuracy: 0.7603\n",
      "Epoch 00251: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7098 - accuracy: 0.7605 - val_loss: 0.7563 - val_accuracy: 0.7422\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.7627\n",
      "Epoch 00252: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7053 - accuracy: 0.7627 - val_loss: 0.7828 - val_accuracy: 0.7326\n",
      "Epoch 253/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7046 - accuracy: 0.7675\n",
      "Epoch 00253: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7052 - accuracy: 0.7661 - val_loss: 0.7507 - val_accuracy: 0.7469\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.7623\n",
      "Epoch 00254: val_loss did not improve from 0.74749\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7047 - accuracy: 0.7623 - val_loss: 0.7572 - val_accuracy: 0.7401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7052 - accuracy: 0.7627\n",
      "Epoch 00255: val_loss improved from 0.74749 to 0.74327, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7109 - accuracy: 0.7605 - val_loss: 0.7433 - val_accuracy: 0.7544\n",
      "Epoch 256/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7035 - accuracy: 0.7621\n",
      "Epoch 00256: val_loss did not improve from 0.74327\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7041 - accuracy: 0.7613 - val_loss: 0.7512 - val_accuracy: 0.7462\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.7666\n",
      "Epoch 00257: val_loss improved from 0.74327 to 0.74252, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6982 - accuracy: 0.7666 - val_loss: 0.7425 - val_accuracy: 0.7435\n",
      "Epoch 258/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7029 - accuracy: 0.7677\n",
      "Epoch 00258: val_loss did not improve from 0.74252\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6973 - accuracy: 0.7678 - val_loss: 0.7449 - val_accuracy: 0.7469\n",
      "Epoch 259/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6967 - accuracy: 0.7712\n",
      "Epoch 00259: val_loss did not improve from 0.74252\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6933 - accuracy: 0.7721 - val_loss: 0.7564 - val_accuracy: 0.7469\n",
      "Epoch 260/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.7702\n",
      "Epoch 00260: val_loss improved from 0.74252 to 0.74212, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6908 - accuracy: 0.7693 - val_loss: 0.7421 - val_accuracy: 0.7435\n",
      "Epoch 261/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6994 - accuracy: 0.7656\n",
      "Epoch 00261: val_loss did not improve from 0.74212\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7058 - accuracy: 0.7616 - val_loss: 0.7787 - val_accuracy: 0.7456\n",
      "Epoch 262/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7036 - accuracy: 0.7617\n",
      "Epoch 00262: val_loss did not improve from 0.74212\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7018 - accuracy: 0.7620 - val_loss: 0.7539 - val_accuracy: 0.7483\n",
      "Epoch 263/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6975 - accuracy: 0.7674\n",
      "Epoch 00263: val_loss did not improve from 0.74212\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6970 - accuracy: 0.7680 - val_loss: 0.7439 - val_accuracy: 0.7524\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.7674\n",
      "Epoch 00264: val_loss improved from 0.74212 to 0.73717, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6949 - accuracy: 0.7674 - val_loss: 0.7372 - val_accuracy: 0.7531\n",
      "Epoch 265/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6891 - accuracy: 0.7709\n",
      "Epoch 00265: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6921 - accuracy: 0.7674 - val_loss: 0.7380 - val_accuracy: 0.7469\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6984 - accuracy: 0.7635\n",
      "Epoch 00266: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6984 - accuracy: 0.7635 - val_loss: 0.7597 - val_accuracy: 0.7387\n",
      "Epoch 267/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7105 - accuracy: 0.7571\n",
      "Epoch 00267: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7095 - accuracy: 0.7570 - val_loss: 0.7603 - val_accuracy: 0.7381\n",
      "Epoch 268/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7244 - accuracy: 0.7543\n",
      "Epoch 00268: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7171 - accuracy: 0.7575 - val_loss: 0.7599 - val_accuracy: 0.7299\n",
      "Epoch 269/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6907 - accuracy: 0.7677\n",
      "Epoch 00269: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6888 - accuracy: 0.7678 - val_loss: 0.7418 - val_accuracy: 0.7524\n",
      "Epoch 270/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6905 - accuracy: 0.7720\n",
      "Epoch 00270: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6901 - accuracy: 0.7721 - val_loss: 0.7405 - val_accuracy: 0.7551\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.7707\n",
      "Epoch 00271: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6834 - accuracy: 0.7707 - val_loss: 0.7400 - val_accuracy: 0.7462\n",
      "Epoch 272/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6784 - accuracy: 0.7729\n",
      "Epoch 00272: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6855 - accuracy: 0.7707 - val_loss: 0.7417 - val_accuracy: 0.7422\n",
      "Epoch 273/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6901 - accuracy: 0.7674\n",
      "Epoch 00273: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6869 - accuracy: 0.7692 - val_loss: 0.7388 - val_accuracy: 0.7476\n",
      "Epoch 274/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6877 - accuracy: 0.7699\n",
      "Epoch 00274: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6890 - accuracy: 0.7681 - val_loss: 0.7385 - val_accuracy: 0.7497\n",
      "Epoch 275/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6907 - accuracy: 0.7665\n",
      "Epoch 00275: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.7661 - val_loss: 0.7388 - val_accuracy: 0.7524\n",
      "Epoch 276/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6849 - accuracy: 0.7733\n",
      "Epoch 00276: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6891 - accuracy: 0.7714 - val_loss: 0.7380 - val_accuracy: 0.7510\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.7678\n",
      "Epoch 00277: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6876 - accuracy: 0.7678 - val_loss: 0.7379 - val_accuracy: 0.7462\n",
      "Epoch 278/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7040 - accuracy: 0.7615\n",
      "Epoch 00278: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7015 - accuracy: 0.7611 - val_loss: 0.7473 - val_accuracy: 0.7510\n",
      "Epoch 279/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6961 - accuracy: 0.7610\n",
      "Epoch 00279: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6995 - accuracy: 0.7603 - val_loss: 0.7430 - val_accuracy: 0.7422\n",
      "Epoch 280/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6922 - accuracy: 0.7692\n",
      "Epoch 00280: val_loss did not improve from 0.73717\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6928 - accuracy: 0.7688 - val_loss: 0.7401 - val_accuracy: 0.7483\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.7760\n",
      "Epoch 00281: val_loss improved from 0.73717 to 0.72824, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6814 - accuracy: 0.7760 - val_loss: 0.7282 - val_accuracy: 0.7578\n",
      "Epoch 282/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6771 - accuracy: 0.7725\n",
      "Epoch 00282: val_loss did not improve from 0.72824\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6780 - accuracy: 0.7722 - val_loss: 0.7290 - val_accuracy: 0.7531\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6834 - accuracy: 0.7708\n",
      "Epoch 00283: val_loss did not improve from 0.72824\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6838 - accuracy: 0.7715 - val_loss: 0.7370 - val_accuracy: 0.7462\n",
      "Epoch 284/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6857 - accuracy: 0.7730\n",
      "Epoch 00284: val_loss did not improve from 0.72824\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6860 - accuracy: 0.7722 - val_loss: 0.7453 - val_accuracy: 0.7415\n",
      "Epoch 285/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6839 - accuracy: 0.7725\n",
      "Epoch 00285: val_loss did not improve from 0.72824\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6893 - accuracy: 0.7697 - val_loss: 0.7377 - val_accuracy: 0.7469\n",
      "Epoch 286/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6757 - accuracy: 0.7707\n",
      "Epoch 00286: val_loss did not improve from 0.72824\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6864 - accuracy: 0.7669 - val_loss: 0.7446 - val_accuracy: 0.7476\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.7616\n",
      "Epoch 00287: val_loss did not improve from 0.72824\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6958 - accuracy: 0.7616 - val_loss: 0.7323 - val_accuracy: 0.7462\n",
      "Epoch 288/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6797 - accuracy: 0.7741\n",
      "Epoch 00288: val_loss improved from 0.72824 to 0.72818, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6774 - accuracy: 0.7761 - val_loss: 0.7282 - val_accuracy: 0.7517\n",
      "Epoch 289/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6864 - accuracy: 0.7701\n",
      "Epoch 00289: val_loss did not improve from 0.72818\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6800 - accuracy: 0.7731 - val_loss: 0.7304 - val_accuracy: 0.7490\n",
      "Epoch 290/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6664 - accuracy: 0.7743\n",
      "Epoch 00290: val_loss did not improve from 0.72818\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6794 - accuracy: 0.7715 - val_loss: 0.7604 - val_accuracy: 0.7347\n",
      "Epoch 291/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6807 - accuracy: 0.7666\n",
      "Epoch 00291: val_loss improved from 0.72818 to 0.72421, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6789 - accuracy: 0.7688 - val_loss: 0.7242 - val_accuracy: 0.7558\n",
      "Epoch 292/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6715 - accuracy: 0.7805\n",
      "Epoch 00292: val_loss did not improve from 0.72421\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6773 - accuracy: 0.7790 - val_loss: 0.7338 - val_accuracy: 0.7551\n",
      "Epoch 293/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6815 - accuracy: 0.7664\n",
      "Epoch 00293: val_loss did not improve from 0.72421\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6899 - accuracy: 0.7673 - val_loss: 0.7390 - val_accuracy: 0.7517\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.7712\n",
      "Epoch 00294: val_loss improved from 0.72421 to 0.72346, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6767 - accuracy: 0.7712 - val_loss: 0.7235 - val_accuracy: 0.7544\n",
      "Epoch 295/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6760 - accuracy: 0.7770\n",
      "Epoch 00295: val_loss improved from 0.72346 to 0.72254, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6671 - accuracy: 0.7813 - val_loss: 0.7225 - val_accuracy: 0.7572\n",
      "Epoch 296/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6736 - accuracy: 0.7764\n",
      "Epoch 00296: val_loss did not improve from 0.72254\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6849 - accuracy: 0.7721 - val_loss: 0.7445 - val_accuracy: 0.7510\n",
      "Epoch 297/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6863 - accuracy: 0.7734\n",
      "Epoch 00297: val_loss did not improve from 0.72254\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6822 - accuracy: 0.7760 - val_loss: 0.7375 - val_accuracy: 0.7578\n",
      "Epoch 298/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6760 - accuracy: 0.7734\n",
      "Epoch 00298: val_loss improved from 0.72254 to 0.71682, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6773 - accuracy: 0.7722 - val_loss: 0.7168 - val_accuracy: 0.7565\n",
      "Epoch 299/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6595 - accuracy: 0.7771\n",
      "Epoch 00299: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6755 - accuracy: 0.7727 - val_loss: 0.7452 - val_accuracy: 0.7353\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.7707\n",
      "Epoch 00300: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6818 - accuracy: 0.7707 - val_loss: 0.7521 - val_accuracy: 0.7367\n",
      "Epoch 301/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6632 - accuracy: 0.7751\n",
      "Epoch 00301: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6740 - accuracy: 0.7743 - val_loss: 0.7194 - val_accuracy: 0.7551\n",
      "Epoch 302/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6713 - accuracy: 0.7740\n",
      "Epoch 00302: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6786 - accuracy: 0.7738 - val_loss: 0.7356 - val_accuracy: 0.7517\n",
      "Epoch 303/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6806 - accuracy: 0.7703\n",
      "Epoch 00303: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6802 - accuracy: 0.7695 - val_loss: 0.7273 - val_accuracy: 0.7497\n",
      "Epoch 304/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6786 - accuracy: 0.7723\n",
      "Epoch 00304: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6794 - accuracy: 0.7719 - val_loss: 0.7549 - val_accuracy: 0.7387\n",
      "Epoch 305/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6776 - accuracy: 0.7704\n",
      "Epoch 00305: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6767 - accuracy: 0.7700 - val_loss: 0.7271 - val_accuracy: 0.7592\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.7773\n",
      "Epoch 00306: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6667 - accuracy: 0.7773 - val_loss: 0.7280 - val_accuracy: 0.7401\n",
      "Epoch 307/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6733 - accuracy: 0.7679\n",
      "Epoch 00307: val_loss did not improve from 0.71682\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6734 - accuracy: 0.7692 - val_loss: 0.7219 - val_accuracy: 0.7558\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6702 - accuracy: 0.7732\n",
      "Epoch 00308: val_loss improved from 0.71682 to 0.71204, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6702 - accuracy: 0.7732 - val_loss: 0.7120 - val_accuracy: 0.7531\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.7761\n",
      "Epoch 00309: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6667 - accuracy: 0.7761 - val_loss: 0.7338 - val_accuracy: 0.7462\n",
      "Epoch 310/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6669 - accuracy: 0.7749\n",
      "Epoch 00310: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6672 - accuracy: 0.7748 - val_loss: 0.7204 - val_accuracy: 0.7538\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.6582 - accuracy: 0.7806\n",
      "Epoch 00311: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6582 - accuracy: 0.7806 - val_loss: 0.7239 - val_accuracy: 0.7531\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.7734\n",
      "Epoch 00312: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6662 - accuracy: 0.7734 - val_loss: 0.7163 - val_accuracy: 0.7606\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.7760\n",
      "Epoch 00313: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6661 - accuracy: 0.7760 - val_loss: 0.7205 - val_accuracy: 0.7558\n",
      "Epoch 314/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6688 - accuracy: 0.7731\n",
      "Epoch 00314: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6655 - accuracy: 0.7731 - val_loss: 0.7302 - val_accuracy: 0.7544\n",
      "Epoch 315/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6712 - accuracy: 0.7747\n",
      "Epoch 00315: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6733 - accuracy: 0.7726 - val_loss: 0.7174 - val_accuracy: 0.7483\n",
      "Epoch 316/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6721 - accuracy: 0.7771\n",
      "Epoch 00316: val_loss did not improve from 0.71204\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6658 - accuracy: 0.7765 - val_loss: 0.7170 - val_accuracy: 0.7592\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.7789\n",
      "Epoch 00317: val_loss improved from 0.71204 to 0.70778, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6594 - accuracy: 0.7789 - val_loss: 0.7078 - val_accuracy: 0.7599\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.7765\n",
      "Epoch 00318: val_loss did not improve from 0.70778\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6730 - accuracy: 0.7765 - val_loss: 0.7408 - val_accuracy: 0.7456\n",
      "Epoch 319/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6710 - accuracy: 0.7678\n",
      "Epoch 00319: val_loss did not improve from 0.70778\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6725 - accuracy: 0.7688 - val_loss: 0.7287 - val_accuracy: 0.7517\n",
      "Epoch 320/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6663 - accuracy: 0.7791\n",
      "Epoch 00320: val_loss did not improve from 0.70778\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6686 - accuracy: 0.7782 - val_loss: 0.7205 - val_accuracy: 0.7578\n",
      "Epoch 321/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6553 - accuracy: 0.7781\n",
      "Epoch 00321: val_loss improved from 0.70778 to 0.70326, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6597 - accuracy: 0.7775 - val_loss: 0.7033 - val_accuracy: 0.7565\n",
      "Epoch 322/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6579 - accuracy: 0.7809\n",
      "Epoch 00322: val_loss did not improve from 0.70326\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6572 - accuracy: 0.7802 - val_loss: 0.7186 - val_accuracy: 0.7538\n",
      "Epoch 323/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6609 - accuracy: 0.7744\n",
      "Epoch 00323: val_loss improved from 0.70326 to 0.70314, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6630 - accuracy: 0.7753 - val_loss: 0.7031 - val_accuracy: 0.7531\n",
      "Epoch 324/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6597 - accuracy: 0.7768\n",
      "Epoch 00324: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6626 - accuracy: 0.7772 - val_loss: 0.7240 - val_accuracy: 0.7510\n",
      "Epoch 325/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6627 - accuracy: 0.7747\n",
      "Epoch 00325: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6716 - accuracy: 0.7715 - val_loss: 0.7253 - val_accuracy: 0.7517\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.7751\n",
      "Epoch 00326: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6692 - accuracy: 0.7751 - val_loss: 0.7287 - val_accuracy: 0.7462\n",
      "Epoch 327/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6670 - accuracy: 0.7738\n",
      "Epoch 00327: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6610 - accuracy: 0.7772 - val_loss: 0.7253 - val_accuracy: 0.7531\n",
      "Epoch 328/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6579 - accuracy: 0.7803\n",
      "Epoch 00328: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6542 - accuracy: 0.7816 - val_loss: 0.7057 - val_accuracy: 0.7531\n",
      "Epoch 329/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6575 - accuracy: 0.7833\n",
      "Epoch 00329: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6586 - accuracy: 0.7796 - val_loss: 0.7133 - val_accuracy: 0.7517\n",
      "Epoch 330/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6458 - accuracy: 0.7839\n",
      "Epoch 00330: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6467 - accuracy: 0.7833 - val_loss: 0.7069 - val_accuracy: 0.7660\n",
      "Epoch 331/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6536 - accuracy: 0.7785\n",
      "Epoch 00331: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6574 - accuracy: 0.7761 - val_loss: 0.7040 - val_accuracy: 0.7538\n",
      "Epoch 332/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6548 - accuracy: 0.7788\n",
      "Epoch 00332: val_loss did not improve from 0.70314\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6601 - accuracy: 0.7767 - val_loss: 0.7243 - val_accuracy: 0.7599\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6485 - accuracy: 0.7847\n",
      "Epoch 00333: val_loss improved from 0.70314 to 0.69785, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6485 - accuracy: 0.7847 - val_loss: 0.6978 - val_accuracy: 0.7633\n",
      "Epoch 334/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6442 - accuracy: 0.7891\n",
      "Epoch 00334: val_loss did not improve from 0.69785\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6494 - accuracy: 0.7864 - val_loss: 0.7087 - val_accuracy: 0.7647\n",
      "Epoch 335/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6583 - accuracy: 0.7787\n",
      "Epoch 00335: val_loss did not improve from 0.69785\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6529 - accuracy: 0.7821 - val_loss: 0.7108 - val_accuracy: 0.7667\n",
      "Epoch 336/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6511 - accuracy: 0.7850\n",
      "Epoch 00336: val_loss did not improve from 0.69785\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6477 - accuracy: 0.7869 - val_loss: 0.7087 - val_accuracy: 0.7585\n",
      "Epoch 337/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6547 - accuracy: 0.7802\n",
      "Epoch 00337: val_loss did not improve from 0.69785\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6546 - accuracy: 0.7799 - val_loss: 0.7219 - val_accuracy: 0.7619\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.7850\n",
      "Epoch 00338: val_loss improved from 0.69785 to 0.69650, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6509 - accuracy: 0.7850 - val_loss: 0.6965 - val_accuracy: 0.7606\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.7809\n",
      "Epoch 00339: val_loss did not improve from 0.69650\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6494 - accuracy: 0.7809 - val_loss: 0.6980 - val_accuracy: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6559 - accuracy: 0.7770\n",
      "Epoch 00340: val_loss did not improve from 0.69650\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6541 - accuracy: 0.7801 - val_loss: 0.7086 - val_accuracy: 0.7558\n",
      "Epoch 341/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6401 - accuracy: 0.7913\n",
      "Epoch 00341: val_loss did not improve from 0.69650\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6494 - accuracy: 0.7862 - val_loss: 0.7121 - val_accuracy: 0.7640\n",
      "Epoch 342/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6507 - accuracy: 0.7819\n",
      "Epoch 00342: val_loss improved from 0.69650 to 0.69241, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6551 - accuracy: 0.7814 - val_loss: 0.6924 - val_accuracy: 0.7688\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6439 - accuracy: 0.7831\n",
      "Epoch 00343: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6439 - accuracy: 0.7831 - val_loss: 0.7074 - val_accuracy: 0.7674\n",
      "Epoch 344/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6458 - accuracy: 0.7848\n",
      "Epoch 00344: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6531 - accuracy: 0.7819 - val_loss: 0.7065 - val_accuracy: 0.7708\n",
      "Epoch 345/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6418 - accuracy: 0.7816\n",
      "Epoch 00345: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6472 - accuracy: 0.7808 - val_loss: 0.7124 - val_accuracy: 0.7538\n",
      "Epoch 346/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6442 - accuracy: 0.7842\n",
      "Epoch 00346: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6497 - accuracy: 0.7830 - val_loss: 0.7121 - val_accuracy: 0.7531\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6460 - accuracy: 0.7845\n",
      "Epoch 00347: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6460 - accuracy: 0.7845 - val_loss: 0.6985 - val_accuracy: 0.7688\n",
      "Epoch 348/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6503 - accuracy: 0.7830\n",
      "Epoch 00348: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6481 - accuracy: 0.7845 - val_loss: 0.7097 - val_accuracy: 0.7647\n",
      "Epoch 349/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6378 - accuracy: 0.7901\n",
      "Epoch 00349: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6419 - accuracy: 0.7866 - val_loss: 0.6954 - val_accuracy: 0.7613\n",
      "Epoch 350/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6385 - accuracy: 0.7879\n",
      "Epoch 00350: val_loss did not improve from 0.69241\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6362 - accuracy: 0.7883 - val_loss: 0.7031 - val_accuracy: 0.7722\n",
      "Epoch 351/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6326 - accuracy: 0.7938\n",
      "Epoch 00351: val_loss improved from 0.69241 to 0.68722, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6382 - accuracy: 0.7872 - val_loss: 0.6872 - val_accuracy: 0.7681\n",
      "Epoch 352/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6406 - accuracy: 0.7838\n",
      "Epoch 00352: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6364 - accuracy: 0.7854 - val_loss: 0.7023 - val_accuracy: 0.7613\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.7900\n",
      "Epoch 00353: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6371 - accuracy: 0.7900 - val_loss: 0.7178 - val_accuracy: 0.7606\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6503 - accuracy: 0.7802\n",
      "Epoch 00354: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6503 - accuracy: 0.7802 - val_loss: 0.6931 - val_accuracy: 0.7756\n",
      "Epoch 355/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6436 - accuracy: 0.7831\n",
      "Epoch 00355: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6400 - accuracy: 0.7859 - val_loss: 0.7084 - val_accuracy: 0.7640\n",
      "Epoch 356/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6367 - accuracy: 0.7855\n",
      "Epoch 00356: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6419 - accuracy: 0.7813 - val_loss: 0.7154 - val_accuracy: 0.7599\n",
      "Epoch 357/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6415 - accuracy: 0.7830\n",
      "Epoch 00357: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6418 - accuracy: 0.7837 - val_loss: 0.7122 - val_accuracy: 0.7558\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6374 - accuracy: 0.7842\n",
      "Epoch 00358: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6374 - accuracy: 0.7842 - val_loss: 0.6904 - val_accuracy: 0.7688\n",
      "Epoch 359/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6414 - accuracy: 0.7889\n",
      "Epoch 00359: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6376 - accuracy: 0.7883 - val_loss: 0.7127 - val_accuracy: 0.7613\n",
      "Epoch 360/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6427 - accuracy: 0.7867\n",
      "Epoch 00360: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6432 - accuracy: 0.7842 - val_loss: 0.7118 - val_accuracy: 0.7585\n",
      "Epoch 361/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6584 - accuracy: 0.7777\n",
      "Epoch 00361: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6606 - accuracy: 0.7790 - val_loss: 0.7011 - val_accuracy: 0.7606\n",
      "Epoch 362/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6459 - accuracy: 0.7835\n",
      "Epoch 00362: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6475 - accuracy: 0.7830 - val_loss: 0.6896 - val_accuracy: 0.7633\n",
      "Epoch 363/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6378 - accuracy: 0.7881\n",
      "Epoch 00363: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6379 - accuracy: 0.7867 - val_loss: 0.6897 - val_accuracy: 0.7715\n",
      "Epoch 364/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6363 - accuracy: 0.7871\n",
      "Epoch 00364: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6286 - accuracy: 0.7898 - val_loss: 0.6904 - val_accuracy: 0.7694\n",
      "Epoch 365/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6323 - accuracy: 0.7887\n",
      "Epoch 00365: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6315 - accuracy: 0.7905 - val_loss: 0.6887 - val_accuracy: 0.7681\n",
      "Epoch 366/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6365 - accuracy: 0.7848\n",
      "Epoch 00366: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6339 - accuracy: 0.7874 - val_loss: 0.7111 - val_accuracy: 0.7544\n",
      "Epoch 367/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6430 - accuracy: 0.7859\n",
      "Epoch 00367: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6375 - accuracy: 0.7877 - val_loss: 0.6888 - val_accuracy: 0.7688\n",
      "Epoch 368/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6277 - accuracy: 0.7921\n",
      "Epoch 00368: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6260 - accuracy: 0.7927 - val_loss: 0.7041 - val_accuracy: 0.7660\n",
      "Epoch 369/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6576 - accuracy: 0.7773\n",
      "Epoch 00369: val_loss did not improve from 0.68722\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6557 - accuracy: 0.7751 - val_loss: 0.7145 - val_accuracy: 0.7606\n",
      "Epoch 370/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6470 - accuracy: 0.7883\n",
      "Epoch 00370: val_loss improved from 0.68722 to 0.67762, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6503 - accuracy: 0.7855 - val_loss: 0.6776 - val_accuracy: 0.7735\n",
      "Epoch 371/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6337 - accuracy: 0.7898\n",
      "Epoch 00371: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6351 - accuracy: 0.7884 - val_loss: 0.7162 - val_accuracy: 0.7667\n",
      "Epoch 372/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6450 - accuracy: 0.7861\n",
      "Epoch 00372: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6423 - accuracy: 0.7847 - val_loss: 0.6863 - val_accuracy: 0.7729\n",
      "Epoch 373/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6279 - accuracy: 0.7917\n",
      "Epoch 00373: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6307 - accuracy: 0.7898 - val_loss: 0.6817 - val_accuracy: 0.7783\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6410 - accuracy: 0.7886\n",
      "Epoch 00374: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6410 - accuracy: 0.7886 - val_loss: 0.6895 - val_accuracy: 0.7694\n",
      "Epoch 375/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6296 - accuracy: 0.7857\n",
      "Epoch 00375: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6310 - accuracy: 0.7872 - val_loss: 0.6910 - val_accuracy: 0.7626\n",
      "Epoch 376/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6211 - accuracy: 0.7942\n",
      "Epoch 00376: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6284 - accuracy: 0.7900 - val_loss: 0.6967 - val_accuracy: 0.7544\n",
      "Epoch 377/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6333 - accuracy: 0.7919\n",
      "Epoch 00377: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6313 - accuracy: 0.7908 - val_loss: 0.6927 - val_accuracy: 0.7688\n",
      "Epoch 378/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6221 - accuracy: 0.7969\n",
      "Epoch 00378: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6262 - accuracy: 0.7934 - val_loss: 0.6824 - val_accuracy: 0.7708\n",
      "Epoch 379/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6187 - accuracy: 0.7913\n",
      "Epoch 00379: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6202 - accuracy: 0.7905 - val_loss: 0.6845 - val_accuracy: 0.7694\n",
      "Epoch 380/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6146 - accuracy: 0.7954\n",
      "Epoch 00380: val_loss did not improve from 0.67762\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6261 - accuracy: 0.7917 - val_loss: 0.7039 - val_accuracy: 0.7776\n",
      "Epoch 381/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6219 - accuracy: 0.7928\n",
      "Epoch 00381: val_loss improved from 0.67762 to 0.67625, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6219 - accuracy: 0.7927 - val_loss: 0.6762 - val_accuracy: 0.7735\n",
      "Epoch 382/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6233 - accuracy: 0.7934\n",
      "Epoch 00382: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6211 - accuracy: 0.7924 - val_loss: 0.6807 - val_accuracy: 0.7708\n",
      "Epoch 383/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6220 - accuracy: 0.7941\n",
      "Epoch 00383: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6196 - accuracy: 0.7947 - val_loss: 0.6883 - val_accuracy: 0.7742\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.7913\n",
      "Epoch 00384: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6285 - accuracy: 0.7913 - val_loss: 0.6958 - val_accuracy: 0.7708\n",
      "Epoch 385/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6183 - accuracy: 0.7883\n",
      "Epoch 00385: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6265 - accuracy: 0.7877 - val_loss: 0.6807 - val_accuracy: 0.7694\n",
      "Epoch 386/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6226 - accuracy: 0.7975\n",
      "Epoch 00386: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6218 - accuracy: 0.7970 - val_loss: 0.6930 - val_accuracy: 0.7606\n",
      "Epoch 387/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6242 - accuracy: 0.7932\n",
      "Epoch 00387: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6174 - accuracy: 0.7954 - val_loss: 0.6974 - val_accuracy: 0.7790\n",
      "Epoch 388/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6215 - accuracy: 0.7912\n",
      "Epoch 00388: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6216 - accuracy: 0.7918 - val_loss: 0.6905 - val_accuracy: 0.7708\n",
      "Epoch 389/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6395 - accuracy: 0.7871\n",
      "Epoch 00389: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6390 - accuracy: 0.7871 - val_loss: 0.7035 - val_accuracy: 0.7619\n",
      "Epoch 390/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6357 - accuracy: 0.7875\n",
      "Epoch 00390: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6364 - accuracy: 0.7876 - val_loss: 0.6818 - val_accuracy: 0.7742\n",
      "Epoch 391/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6227 - accuracy: 0.7958\n",
      "Epoch 00391: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6189 - accuracy: 0.7949 - val_loss: 0.6873 - val_accuracy: 0.7633\n",
      "Epoch 392/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6154 - accuracy: 0.7957\n",
      "Epoch 00392: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6155 - accuracy: 0.7961 - val_loss: 0.6893 - val_accuracy: 0.7667\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.7961\n",
      "Epoch 00393: val_loss did not improve from 0.67625\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6139 - accuracy: 0.7961 - val_loss: 0.6785 - val_accuracy: 0.7729\n",
      "Epoch 394/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6106 - accuracy: 0.7988\n",
      "Epoch 00394: val_loss improved from 0.67625 to 0.67621, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6142 - accuracy: 0.7982 - val_loss: 0.6762 - val_accuracy: 0.7783\n",
      "Epoch 395/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6200 - accuracy: 0.7933\n",
      "Epoch 00395: val_loss did not improve from 0.67621\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6195 - accuracy: 0.7942 - val_loss: 0.6939 - val_accuracy: 0.7660\n",
      "Epoch 396/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6286 - accuracy: 0.7910\n",
      "Epoch 00396: val_loss did not improve from 0.67621\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6205 - accuracy: 0.7937 - val_loss: 0.6831 - val_accuracy: 0.7722\n",
      "Epoch 397/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6167 - accuracy: 0.7963\n",
      "Epoch 00397: val_loss did not improve from 0.67621\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6195 - accuracy: 0.7983 - val_loss: 0.6858 - val_accuracy: 0.7749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6222 - accuracy: 0.7909\n",
      "Epoch 00398: val_loss did not improve from 0.67621\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6263 - accuracy: 0.7860 - val_loss: 0.6900 - val_accuracy: 0.7694\n",
      "Epoch 399/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6186 - accuracy: 0.7917\n",
      "Epoch 00399: val_loss improved from 0.67621 to 0.67424, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6203 - accuracy: 0.7913 - val_loss: 0.6742 - val_accuracy: 0.7756\n",
      "Epoch 400/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6189 - accuracy: 0.7950\n",
      "Epoch 00400: val_loss improved from 0.67424 to 0.67202, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6172 - accuracy: 0.7954 - val_loss: 0.6720 - val_accuracy: 0.7831\n",
      "Epoch 401/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6161 - accuracy: 0.8002\n",
      "Epoch 00401: val_loss improved from 0.67202 to 0.67017, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6149 - accuracy: 0.7995 - val_loss: 0.6702 - val_accuracy: 0.7804\n",
      "Epoch 402/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6028 - accuracy: 0.8098\n",
      "Epoch 00402: val_loss did not improve from 0.67017\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6131 - accuracy: 0.8031 - val_loss: 0.6815 - val_accuracy: 0.7681\n",
      "Epoch 403/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6140 - accuracy: 0.7957\n",
      "Epoch 00403: val_loss did not improve from 0.67017\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6178 - accuracy: 0.7956 - val_loss: 0.6841 - val_accuracy: 0.7729\n",
      "Epoch 404/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6230 - accuracy: 0.7945\n",
      "Epoch 00404: val_loss did not improve from 0.67017\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6195 - accuracy: 0.7956 - val_loss: 0.6757 - val_accuracy: 0.7688\n",
      "Epoch 405/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6216 - accuracy: 0.7967\n",
      "Epoch 00405: val_loss did not improve from 0.67017\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6157 - accuracy: 0.7968 - val_loss: 0.6844 - val_accuracy: 0.7688\n",
      "Epoch 406/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6196 - accuracy: 0.7944\n",
      "Epoch 00406: val_loss improved from 0.67017 to 0.66701, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6156 - accuracy: 0.7956 - val_loss: 0.6670 - val_accuracy: 0.7729\n",
      "Epoch 407/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6131 - accuracy: 0.7984\n",
      "Epoch 00407: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6155 - accuracy: 0.7975 - val_loss: 0.6945 - val_accuracy: 0.7633\n",
      "Epoch 408/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6436 - accuracy: 0.7850\n",
      "Epoch 00408: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6437 - accuracy: 0.7838 - val_loss: 0.7053 - val_accuracy: 0.7551\n",
      "Epoch 409/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6278 - accuracy: 0.7872\n",
      "Epoch 00409: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6246 - accuracy: 0.7895 - val_loss: 0.7358 - val_accuracy: 0.7613\n",
      "Epoch 410/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6416 - accuracy: 0.7862\n",
      "Epoch 00410: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6447 - accuracy: 0.7874 - val_loss: 0.6763 - val_accuracy: 0.7694\n",
      "Epoch 411/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6146 - accuracy: 0.7951\n",
      "Epoch 00411: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6157 - accuracy: 0.7965 - val_loss: 0.6737 - val_accuracy: 0.7756\n",
      "Epoch 412/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6187 - accuracy: 0.7975\n",
      "Epoch 00412: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6190 - accuracy: 0.7946 - val_loss: 0.6793 - val_accuracy: 0.7660\n",
      "Epoch 413/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6176 - accuracy: 0.7908\n",
      "Epoch 00413: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6213 - accuracy: 0.7901 - val_loss: 0.6987 - val_accuracy: 0.7674\n",
      "Epoch 414/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6196 - accuracy: 0.7902\n",
      "Epoch 00414: val_loss did not improve from 0.66701\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6185 - accuracy: 0.7918 - val_loss: 0.6815 - val_accuracy: 0.7626\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.7958\n",
      "Epoch 00415: val_loss improved from 0.66701 to 0.66240, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6166 - accuracy: 0.7958 - val_loss: 0.6624 - val_accuracy: 0.7729\n",
      "Epoch 416/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6052 - accuracy: 0.8008\n",
      "Epoch 00416: val_loss did not improve from 0.66240\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6054 - accuracy: 0.8007 - val_loss: 0.6708 - val_accuracy: 0.7722\n",
      "Epoch 417/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6015 - accuracy: 0.8028\n",
      "Epoch 00417: val_loss did not improve from 0.66240\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6013 - accuracy: 0.8011 - val_loss: 0.6796 - val_accuracy: 0.7701\n",
      "Epoch 418/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6074 - accuracy: 0.7991\n",
      "Epoch 00418: val_loss improved from 0.66240 to 0.66172, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6089 - accuracy: 0.7992 - val_loss: 0.6617 - val_accuracy: 0.7872\n",
      "Epoch 419/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6050 - accuracy: 0.7973\n",
      "Epoch 00419: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6054 - accuracy: 0.7970 - val_loss: 0.6663 - val_accuracy: 0.7790\n",
      "Epoch 420/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6045 - accuracy: 0.8026\n",
      "Epoch 00420: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6077 - accuracy: 0.8007 - val_loss: 0.6644 - val_accuracy: 0.7769\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.7947\n",
      "Epoch 00421: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6061 - accuracy: 0.7947 - val_loss: 0.6657 - val_accuracy: 0.7838\n",
      "Epoch 422/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5908 - accuracy: 0.8002\n",
      "Epoch 00422: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6063 - accuracy: 0.7958 - val_loss: 0.6774 - val_accuracy: 0.7694\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.7958\n",
      "Epoch 00423: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6134 - accuracy: 0.7958 - val_loss: 0.6659 - val_accuracy: 0.7783\n",
      "Epoch 424/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6066 - accuracy: 0.8029\n",
      "Epoch 00424: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6091 - accuracy: 0.8005 - val_loss: 0.6735 - val_accuracy: 0.7769\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.7968\n",
      "Epoch 00425: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6129 - accuracy: 0.7968 - val_loss: 0.6634 - val_accuracy: 0.7769\n",
      "Epoch 426/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6125 - accuracy: 0.8021\n",
      "Epoch 00426: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6061 - accuracy: 0.8033 - val_loss: 0.6755 - val_accuracy: 0.7838\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.7922\n",
      "Epoch 00427: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6228 - accuracy: 0.7922 - val_loss: 0.6679 - val_accuracy: 0.7742\n",
      "Epoch 428/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6081 - accuracy: 0.7985\n",
      "Epoch 00428: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6090 - accuracy: 0.7987 - val_loss: 0.6643 - val_accuracy: 0.7810\n",
      "Epoch 429/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6010 - accuracy: 0.8022\n",
      "Epoch 00429: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5989 - accuracy: 0.8031 - val_loss: 0.6644 - val_accuracy: 0.7742\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.7959\n",
      "Epoch 00430: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6051 - accuracy: 0.7959 - val_loss: 0.6801 - val_accuracy: 0.7776\n",
      "Epoch 431/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6112 - accuracy: 0.7996\n",
      "Epoch 00431: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6102 - accuracy: 0.7985 - val_loss: 0.6654 - val_accuracy: 0.7749\n",
      "Epoch 432/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6010 - accuracy: 0.8008\n",
      "Epoch 00432: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6013 - accuracy: 0.8011 - val_loss: 0.6955 - val_accuracy: 0.7701\n",
      "Epoch 433/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6184 - accuracy: 0.7919\n",
      "Epoch 00433: val_loss did not improve from 0.66172\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6158 - accuracy: 0.7934 - val_loss: 0.6680 - val_accuracy: 0.7851\n",
      "Epoch 434/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6068 - accuracy: 0.7994\n",
      "Epoch 00434: val_loss improved from 0.66172 to 0.66078, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5990 - accuracy: 0.8009 - val_loss: 0.6608 - val_accuracy: 0.7817\n",
      "Epoch 435/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6089 - accuracy: 0.7998\n",
      "Epoch 00435: val_loss did not improve from 0.66078\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6088 - accuracy: 0.7963 - val_loss: 0.6685 - val_accuracy: 0.7735\n",
      "Epoch 436/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6201 - accuracy: 0.7908\n",
      "Epoch 00436: val_loss did not improve from 0.66078\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6160 - accuracy: 0.7932 - val_loss: 0.6702 - val_accuracy: 0.7742\n",
      "Epoch 437/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6049 - accuracy: 0.7984\n",
      "Epoch 00437: val_loss improved from 0.66078 to 0.65772, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6061 - accuracy: 0.7980 - val_loss: 0.6577 - val_accuracy: 0.7824\n",
      "Epoch 438/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5814 - accuracy: 0.8065\n",
      "Epoch 00438: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.5913 - accuracy: 0.8045 - val_loss: 0.6650 - val_accuracy: 0.7804\n",
      "Epoch 439/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5975 - accuracy: 0.8000\n",
      "Epoch 00439: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5975 - accuracy: 0.7992 - val_loss: 0.6717 - val_accuracy: 0.7722\n",
      "Epoch 440/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6030 - accuracy: 0.8014\n",
      "Epoch 00440: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6008 - accuracy: 0.7997 - val_loss: 0.6621 - val_accuracy: 0.7797\n",
      "Epoch 441/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6011 - accuracy: 0.7988\n",
      "Epoch 00441: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6003 - accuracy: 0.8011 - val_loss: 0.6711 - val_accuracy: 0.7715\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.7947\n",
      "Epoch 00442: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6096 - accuracy: 0.7947 - val_loss: 0.6585 - val_accuracy: 0.7831\n",
      "Epoch 443/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5983 - accuracy: 0.8008\n",
      "Epoch 00443: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6040 - accuracy: 0.7970 - val_loss: 0.6771 - val_accuracy: 0.7885\n",
      "Epoch 444/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6143 - accuracy: 0.7985\n",
      "Epoch 00444: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6122 - accuracy: 0.7970 - val_loss: 0.6628 - val_accuracy: 0.7804\n",
      "Epoch 445/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5953 - accuracy: 0.8058\n",
      "Epoch 00445: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5994 - accuracy: 0.8036 - val_loss: 0.6653 - val_accuracy: 0.7804\n",
      "Epoch 446/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5900 - accuracy: 0.8098\n",
      "Epoch 00446: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5944 - accuracy: 0.8057 - val_loss: 0.6604 - val_accuracy: 0.7756\n",
      "Epoch 447/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5977 - accuracy: 0.7979\n",
      "Epoch 00447: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6004 - accuracy: 0.7965 - val_loss: 0.6876 - val_accuracy: 0.7660\n",
      "Epoch 448/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6057 - accuracy: 0.7992\n",
      "Epoch 00448: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5987 - accuracy: 0.8026 - val_loss: 0.6640 - val_accuracy: 0.7844\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5994 - accuracy: 0.8000\n",
      "Epoch 00449: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5994 - accuracy: 0.8000 - val_loss: 0.6761 - val_accuracy: 0.7769\n",
      "Epoch 450/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6072 - accuracy: 0.7998\n",
      "Epoch 00450: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6011 - accuracy: 0.8053 - val_loss: 0.6768 - val_accuracy: 0.7790\n",
      "Epoch 451/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6019 - accuracy: 0.7979\n",
      "Epoch 00451: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6054 - accuracy: 0.7965 - val_loss: 0.6760 - val_accuracy: 0.7722\n",
      "Epoch 452/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6084 - accuracy: 0.7974\n",
      "Epoch 00452: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6103 - accuracy: 0.7965 - val_loss: 0.6709 - val_accuracy: 0.7756\n",
      "Epoch 453/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5981 - accuracy: 0.8021\n",
      "Epoch 00453: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6002 - accuracy: 0.8021 - val_loss: 0.6787 - val_accuracy: 0.7667\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.8019\n",
      "Epoch 00454: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5952 - accuracy: 0.8019 - val_loss: 0.6750 - val_accuracy: 0.7776\n",
      "Epoch 455/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6095 - accuracy: 0.8004\n",
      "Epoch 00455: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6077 - accuracy: 0.8002 - val_loss: 0.6836 - val_accuracy: 0.7667\n",
      "Epoch 456/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6072 - accuracy: 0.7954\n",
      "Epoch 00456: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6031 - accuracy: 0.7970 - val_loss: 0.6683 - val_accuracy: 0.7729\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6133 - accuracy: 0.7944\n",
      "Epoch 00457: val_loss did not improve from 0.65772\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6133 - accuracy: 0.7944 - val_loss: 0.6939 - val_accuracy: 0.7688\n",
      "Epoch 458/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6132 - accuracy: 0.7947\n",
      "Epoch 00458: val_loss improved from 0.65772 to 0.65299, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6087 - accuracy: 0.7949 - val_loss: 0.6530 - val_accuracy: 0.7783\n",
      "Epoch 459/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5946 - accuracy: 0.8060\n",
      "Epoch 00459: val_loss did not improve from 0.65299\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5937 - accuracy: 0.8062 - val_loss: 0.6561 - val_accuracy: 0.7851\n",
      "Epoch 460/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5973 - accuracy: 0.7988\n",
      "Epoch 00460: val_loss did not improve from 0.65299\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5935 - accuracy: 0.8031 - val_loss: 0.6741 - val_accuracy: 0.7790\n",
      "Epoch 461/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5961 - accuracy: 0.8043\n",
      "Epoch 00461: val_loss did not improve from 0.65299\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5946 - accuracy: 0.8043 - val_loss: 0.6590 - val_accuracy: 0.7769\n",
      "Epoch 462/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6023 - accuracy: 0.8049\n",
      "Epoch 00462: val_loss improved from 0.65299 to 0.65077, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5932 - accuracy: 0.8063 - val_loss: 0.6508 - val_accuracy: 0.7831\n",
      "Epoch 463/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5909 - accuracy: 0.8051\n",
      "Epoch 00463: val_loss improved from 0.65077 to 0.65004, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5895 - accuracy: 0.8045 - val_loss: 0.6500 - val_accuracy: 0.7892\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.7994\n",
      "Epoch 00464: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5981 - accuracy: 0.7994 - val_loss: 0.6646 - val_accuracy: 0.7769\n",
      "Epoch 465/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6007 - accuracy: 0.8051\n",
      "Epoch 00465: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6001 - accuracy: 0.8046 - val_loss: 0.6618 - val_accuracy: 0.7769\n",
      "Epoch 466/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5928 - accuracy: 0.8018\n",
      "Epoch 00466: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5924 - accuracy: 0.8021 - val_loss: 0.6679 - val_accuracy: 0.7858\n",
      "Epoch 467/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5904 - accuracy: 0.8024\n",
      "Epoch 00467: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5891 - accuracy: 0.8038 - val_loss: 0.6744 - val_accuracy: 0.7749\n",
      "Epoch 468/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6013 - accuracy: 0.8012\n",
      "Epoch 00468: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5936 - accuracy: 0.8019 - val_loss: 0.6530 - val_accuracy: 0.7749\n",
      "Epoch 469/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6003 - accuracy: 0.8047\n",
      "Epoch 00469: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5990 - accuracy: 0.8050 - val_loss: 0.6752 - val_accuracy: 0.7756\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.8065\n",
      "Epoch 00470: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5941 - accuracy: 0.8065 - val_loss: 0.6702 - val_accuracy: 0.7735\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.8055\n",
      "Epoch 00471: val_loss did not improve from 0.65004\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5869 - accuracy: 0.8055 - val_loss: 0.6614 - val_accuracy: 0.7790\n",
      "Epoch 472/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5892 - accuracy: 0.7983\n",
      "Epoch 00472: val_loss improved from 0.65004 to 0.64558, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5863 - accuracy: 0.8038 - val_loss: 0.6456 - val_accuracy: 0.7817\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5913 - accuracy: 0.8033\n",
      "Epoch 00473: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5913 - accuracy: 0.8033 - val_loss: 0.6776 - val_accuracy: 0.7838\n",
      "Epoch 474/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5797 - accuracy: 0.8121\n",
      "Epoch 00474: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5862 - accuracy: 0.8101 - val_loss: 0.6666 - val_accuracy: 0.7865\n",
      "Epoch 475/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5947 - accuracy: 0.7949\n",
      "Epoch 00475: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6000 - accuracy: 0.7965 - val_loss: 0.6576 - val_accuracy: 0.7790\n",
      "Epoch 476/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5913 - accuracy: 0.8078\n",
      "Epoch 00476: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5895 - accuracy: 0.8067 - val_loss: 0.6656 - val_accuracy: 0.7824\n",
      "Epoch 477/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5865 - accuracy: 0.8027\n",
      "Epoch 00477: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5839 - accuracy: 0.8043 - val_loss: 0.6695 - val_accuracy: 0.7783\n",
      "Epoch 478/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5961 - accuracy: 0.7996\n",
      "Epoch 00478: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5970 - accuracy: 0.8024 - val_loss: 0.6515 - val_accuracy: 0.7892\n",
      "Epoch 479/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5926 - accuracy: 0.8051\n",
      "Epoch 00479: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5903 - accuracy: 0.8046 - val_loss: 0.6512 - val_accuracy: 0.7831\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.8014\n",
      "Epoch 00480: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5938 - accuracy: 0.8014 - val_loss: 0.6663 - val_accuracy: 0.7933\n",
      "Epoch 481/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5947 - accuracy: 0.8023\n",
      "Epoch 00481: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5913 - accuracy: 0.8038 - val_loss: 0.6597 - val_accuracy: 0.7783\n",
      "Epoch 482/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5978 - accuracy: 0.8002\n",
      "Epoch 00482: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5944 - accuracy: 0.8012 - val_loss: 0.6617 - val_accuracy: 0.7783\n",
      "Epoch 483/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5881 - accuracy: 0.8053\n",
      "Epoch 00483: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5850 - accuracy: 0.8063 - val_loss: 0.6527 - val_accuracy: 0.7865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5804 - accuracy: 0.8067\n",
      "Epoch 00484: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5821 - accuracy: 0.8060 - val_loss: 0.6867 - val_accuracy: 0.7742\n",
      "Epoch 485/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5777 - accuracy: 0.8082\n",
      "Epoch 00485: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5875 - accuracy: 0.8058 - val_loss: 0.6699 - val_accuracy: 0.7817\n",
      "Epoch 486/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5682 - accuracy: 0.8119\n",
      "Epoch 00486: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5819 - accuracy: 0.8116 - val_loss: 0.6525 - val_accuracy: 0.7947\n",
      "Epoch 487/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5880 - accuracy: 0.8071\n",
      "Epoch 00487: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5852 - accuracy: 0.8092 - val_loss: 0.6548 - val_accuracy: 0.7872\n",
      "Epoch 488/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5841 - accuracy: 0.8072\n",
      "Epoch 00488: val_loss did not improve from 0.64558\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5853 - accuracy: 0.8055 - val_loss: 0.6527 - val_accuracy: 0.7858\n",
      "Epoch 489/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5906 - accuracy: 0.8051\n",
      "Epoch 00489: val_loss improved from 0.64558 to 0.64413, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5830 - accuracy: 0.8075 - val_loss: 0.6441 - val_accuracy: 0.7838\n",
      "Epoch 490/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5836 - accuracy: 0.8066\n",
      "Epoch 00490: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5837 - accuracy: 0.8069 - val_loss: 0.6674 - val_accuracy: 0.7892\n",
      "Epoch 491/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5814 - accuracy: 0.8129\n",
      "Epoch 00491: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5846 - accuracy: 0.8127 - val_loss: 0.6497 - val_accuracy: 0.7899\n",
      "Epoch 492/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5809 - accuracy: 0.8107\n",
      "Epoch 00492: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5791 - accuracy: 0.8118 - val_loss: 0.6565 - val_accuracy: 0.7804\n",
      "Epoch 493/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5889 - accuracy: 0.8010\n",
      "Epoch 00493: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.5853 - accuracy: 0.8038 - val_loss: 0.6514 - val_accuracy: 0.7858\n",
      "Epoch 494/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5822 - accuracy: 0.8098\n",
      "Epoch 00494: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5814 - accuracy: 0.8113 - val_loss: 0.6461 - val_accuracy: 0.7906\n",
      "Epoch 495/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5871 - accuracy: 0.8098\n",
      "Epoch 00495: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5885 - accuracy: 0.8087 - val_loss: 0.6587 - val_accuracy: 0.7756\n",
      "Epoch 496/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5917 - accuracy: 0.8039\n",
      "Epoch 00496: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5892 - accuracy: 0.8045 - val_loss: 0.6584 - val_accuracy: 0.7844\n",
      "Epoch 497/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5968 - accuracy: 0.7990\n",
      "Epoch 00497: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5899 - accuracy: 0.8024 - val_loss: 0.6645 - val_accuracy: 0.7817\n",
      "Epoch 498/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5864 - accuracy: 0.8000\n",
      "Epoch 00498: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5925 - accuracy: 0.7985 - val_loss: 0.6502 - val_accuracy: 0.7810\n",
      "Epoch 499/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5881 - accuracy: 0.8028\n",
      "Epoch 00499: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5930 - accuracy: 0.8019 - val_loss: 0.6462 - val_accuracy: 0.7872\n",
      "Epoch 500/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5804 - accuracy: 0.8096\n",
      "Epoch 00500: val_loss did not improve from 0.64413\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5826 - accuracy: 0.8069 - val_loss: 0.6515 - val_accuracy: 0.7831\n",
      "Training for model  2  completed in time:  0:02:59.421081 seconds\n",
      "Training for model  3  has started.\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.7549 - accuracy: 0.1592\n",
      "Epoch 00001: val_loss improved from inf to 2.00899, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 2.7549 - accuracy: 0.1592 - val_loss: 2.0090 - val_accuracy: 0.2046\n",
      "Epoch 2/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.8964 - accuracy: 0.3093\n",
      "Epoch 00002: val_loss improved from 2.00899 to 1.73293, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.8911 - accuracy: 0.3131 - val_loss: 1.7329 - val_accuracy: 0.3990\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5982 - accuracy: 0.4450\n",
      "Epoch 00003: val_loss improved from 1.73293 to 1.50917, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5982 - accuracy: 0.4450 - val_loss: 1.5092 - val_accuracy: 0.4973\n",
      "Epoch 4/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4157 - accuracy: 0.5108\n",
      "Epoch 00004: val_loss improved from 1.50917 to 1.39658, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4123 - accuracy: 0.5110 - val_loss: 1.3966 - val_accuracy: 0.5048\n",
      "Epoch 5/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3124 - accuracy: 0.5346\n",
      "Epoch 00005: val_loss improved from 1.39658 to 1.28170, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3108 - accuracy: 0.5369 - val_loss: 1.2817 - val_accuracy: 0.5430\n",
      "Epoch 6/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.2393 - accuracy: 0.5691\n",
      "Epoch 00006: val_loss improved from 1.28170 to 1.20462, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.2363 - accuracy: 0.5690 - val_loss: 1.2046 - val_accuracy: 0.5791\n",
      "Epoch 7/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1592 - accuracy: 0.6032\n",
      "Epoch 00007: val_loss improved from 1.20462 to 1.15912, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1594 - accuracy: 0.6028 - val_loss: 1.1591 - val_accuracy: 0.6010\n",
      "Epoch 8/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.1204 - accuracy: 0.6077\n",
      "Epoch 00008: val_loss improved from 1.15912 to 1.09877, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1208 - accuracy: 0.6064 - val_loss: 1.0988 - val_accuracy: 0.6146\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.6240\n",
      "Epoch 00009: val_loss improved from 1.09877 to 1.05985, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0788 - accuracy: 0.6240 - val_loss: 1.0599 - val_accuracy: 0.6201\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0555 - accuracy: 0.6356\n",
      "Epoch 00010: val_loss improved from 1.05985 to 1.04661, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0525 - accuracy: 0.6347 - val_loss: 1.0466 - val_accuracy: 0.6153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0155 - accuracy: 0.6467\n",
      "Epoch 00011: val_loss improved from 1.04661 to 1.01114, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0142 - accuracy: 0.6451 - val_loss: 1.0111 - val_accuracy: 0.6371\n",
      "Epoch 12/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9878 - accuracy: 0.6596\n",
      "Epoch 00012: val_loss improved from 1.01114 to 0.97752, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9880 - accuracy: 0.6591 - val_loss: 0.9775 - val_accuracy: 0.6521\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9531 - accuracy: 0.6704\n",
      "Epoch 00013: val_loss improved from 0.97752 to 0.96790, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9531 - accuracy: 0.6704 - val_loss: 0.9679 - val_accuracy: 0.6712\n",
      "Epoch 14/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9497 - accuracy: 0.6772\n",
      "Epoch 00014: val_loss improved from 0.96790 to 0.96042, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9476 - accuracy: 0.6787 - val_loss: 0.9604 - val_accuracy: 0.6712\n",
      "Epoch 15/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9118 - accuracy: 0.6839\n",
      "Epoch 00015: val_loss improved from 0.96042 to 0.95445, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9131 - accuracy: 0.6844 - val_loss: 0.9545 - val_accuracy: 0.6719\n",
      "Epoch 16/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9081 - accuracy: 0.6839\n",
      "Epoch 00016: val_loss improved from 0.95445 to 0.93250, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9054 - accuracy: 0.6855 - val_loss: 0.9325 - val_accuracy: 0.6828\n",
      "Epoch 17/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8724 - accuracy: 0.7051\n",
      "Epoch 00017: val_loss improved from 0.93250 to 0.89783, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8741 - accuracy: 0.7043 - val_loss: 0.8978 - val_accuracy: 0.6930\n",
      "Epoch 18/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8643 - accuracy: 0.7054\n",
      "Epoch 00018: val_loss improved from 0.89783 to 0.89212, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8632 - accuracy: 0.7055 - val_loss: 0.8921 - val_accuracy: 0.6917\n",
      "Epoch 19/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8509 - accuracy: 0.7095\n",
      "Epoch 00019: val_loss improved from 0.89212 to 0.87302, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.8491 - accuracy: 0.7101 - val_loss: 0.8730 - val_accuracy: 0.7094\n",
      "Epoch 20/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8374 - accuracy: 0.7111\n",
      "Epoch 00020: val_loss did not improve from 0.87302\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8381 - accuracy: 0.7118 - val_loss: 0.8906 - val_accuracy: 0.6985\n",
      "Epoch 21/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8396 - accuracy: 0.7124\n",
      "Epoch 00021: val_loss did not improve from 0.87302\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8365 - accuracy: 0.7144 - val_loss: 0.8793 - val_accuracy: 0.7060\n",
      "Epoch 22/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8298 - accuracy: 0.7189\n",
      "Epoch 00022: val_loss improved from 0.87302 to 0.86484, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8279 - accuracy: 0.7195 - val_loss: 0.8648 - val_accuracy: 0.7121\n",
      "Epoch 23/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8021 - accuracy: 0.7276\n",
      "Epoch 00023: val_loss improved from 0.86484 to 0.84872, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8005 - accuracy: 0.7280 - val_loss: 0.8487 - val_accuracy: 0.7169\n",
      "Epoch 24/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7907 - accuracy: 0.7371\n",
      "Epoch 00024: val_loss improved from 0.84872 to 0.83943, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7932 - accuracy: 0.7330 - val_loss: 0.8394 - val_accuracy: 0.7176\n",
      "Epoch 25/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7804 - accuracy: 0.7351\n",
      "Epoch 00025: val_loss improved from 0.83943 to 0.82719, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7830 - accuracy: 0.7347 - val_loss: 0.8272 - val_accuracy: 0.7258\n",
      "Epoch 26/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7783 - accuracy: 0.7351\n",
      "Epoch 00026: val_loss did not improve from 0.82719\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7756 - accuracy: 0.7359 - val_loss: 0.8374 - val_accuracy: 0.7251\n",
      "Epoch 27/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7734 - accuracy: 0.7385\n",
      "Epoch 00027: val_loss did not improve from 0.82719\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7714 - accuracy: 0.7393 - val_loss: 0.8305 - val_accuracy: 0.7360\n",
      "Epoch 28/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7638 - accuracy: 0.7394\n",
      "Epoch 00028: val_loss improved from 0.82719 to 0.80514, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.7652 - accuracy: 0.7388 - val_loss: 0.8051 - val_accuracy: 0.7353\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7510 - accuracy: 0.7485\n",
      "Epoch 00029: val_loss did not improve from 0.80514\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7510 - accuracy: 0.7485 - val_loss: 0.8108 - val_accuracy: 0.7422\n",
      "Epoch 30/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7461 - accuracy: 0.7496\n",
      "Epoch 00030: val_loss did not improve from 0.80514\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7413 - accuracy: 0.7519 - val_loss: 0.8123 - val_accuracy: 0.7340\n",
      "Epoch 31/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7307 - accuracy: 0.7524\n",
      "Epoch 00031: val_loss improved from 0.80514 to 0.79009, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7351 - accuracy: 0.7512 - val_loss: 0.7901 - val_accuracy: 0.7422\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7257 - accuracy: 0.7570\n",
      "Epoch 00032: val_loss did not improve from 0.79009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7257 - accuracy: 0.7570 - val_loss: 0.7909 - val_accuracy: 0.7381\n",
      "Epoch 33/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7279 - accuracy: 0.7578\n",
      "Epoch 00033: val_loss improved from 0.79009 to 0.77593, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7251 - accuracy: 0.7579 - val_loss: 0.7759 - val_accuracy: 0.7360\n",
      "Epoch 34/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7146 - accuracy: 0.7578\n",
      "Epoch 00034: val_loss improved from 0.77593 to 0.77580, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7158 - accuracy: 0.7569 - val_loss: 0.7758 - val_accuracy: 0.7401\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7042 - accuracy: 0.7668\n",
      "Epoch 00035: val_loss did not improve from 0.77580\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7042 - accuracy: 0.7668 - val_loss: 0.7777 - val_accuracy: 0.7347\n",
      "Epoch 36/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6972 - accuracy: 0.7708\n",
      "Epoch 00036: val_loss did not improve from 0.77580\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7013 - accuracy: 0.7703 - val_loss: 0.7789 - val_accuracy: 0.7381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6864 - accuracy: 0.7692\n",
      "Epoch 00037: val_loss improved from 0.77580 to 0.75207, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6864 - accuracy: 0.7695 - val_loss: 0.7521 - val_accuracy: 0.7524\n",
      "Epoch 38/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6808 - accuracy: 0.7740\n",
      "Epoch 00038: val_loss did not improve from 0.75207\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6815 - accuracy: 0.7744 - val_loss: 0.7787 - val_accuracy: 0.7408\n",
      "Epoch 39/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6877 - accuracy: 0.7701\n",
      "Epoch 00039: val_loss did not improve from 0.75207\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6844 - accuracy: 0.7709 - val_loss: 0.7563 - val_accuracy: 0.7531\n",
      "Epoch 40/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6675 - accuracy: 0.7756\n",
      "Epoch 00040: val_loss improved from 0.75207 to 0.74521, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6682 - accuracy: 0.7746 - val_loss: 0.7452 - val_accuracy: 0.7442\n",
      "Epoch 41/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6551 - accuracy: 0.7876\n",
      "Epoch 00041: val_loss improved from 0.74521 to 0.72759, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6519 - accuracy: 0.7876 - val_loss: 0.7276 - val_accuracy: 0.7510\n",
      "Epoch 42/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6446 - accuracy: 0.7903\n",
      "Epoch 00042: val_loss did not improve from 0.72759\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6473 - accuracy: 0.7888 - val_loss: 0.7533 - val_accuracy: 0.7456\n",
      "Epoch 43/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6547 - accuracy: 0.7846\n",
      "Epoch 00043: val_loss improved from 0.72759 to 0.72615, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6530 - accuracy: 0.7852 - val_loss: 0.7262 - val_accuracy: 0.7565\n",
      "Epoch 44/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6532 - accuracy: 0.7844\n",
      "Epoch 00044: val_loss did not improve from 0.72615\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6528 - accuracy: 0.7845 - val_loss: 0.7513 - val_accuracy: 0.7462\n",
      "Epoch 45/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6555 - accuracy: 0.7822\n",
      "Epoch 00045: val_loss improved from 0.72615 to 0.70731, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6518 - accuracy: 0.7854 - val_loss: 0.7073 - val_accuracy: 0.7674\n",
      "Epoch 46/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6206 - accuracy: 0.7945\n",
      "Epoch 00046: val_loss did not improve from 0.70731\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6231 - accuracy: 0.7944 - val_loss: 0.7102 - val_accuracy: 0.7544\n",
      "Epoch 47/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6237 - accuracy: 0.7920\n",
      "Epoch 00047: val_loss improved from 0.70731 to 0.69733, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6240 - accuracy: 0.7915 - val_loss: 0.6973 - val_accuracy: 0.7701\n",
      "Epoch 48/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6216 - accuracy: 0.7956\n",
      "Epoch 00048: val_loss improved from 0.69733 to 0.69432, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6238 - accuracy: 0.7944 - val_loss: 0.6943 - val_accuracy: 0.7701\n",
      "Epoch 49/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6096 - accuracy: 0.8031\n",
      "Epoch 00049: val_loss did not improve from 0.69432\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6134 - accuracy: 0.7997 - val_loss: 0.7016 - val_accuracy: 0.7606\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6121 - accuracy: 0.7987\n",
      "Epoch 00050: val_loss did not improve from 0.69432\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6083 - accuracy: 0.8002 - val_loss: 0.7240 - val_accuracy: 0.7694\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6133 - accuracy: 0.7985\n",
      "Epoch 00051: val_loss did not improve from 0.69432\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6133 - accuracy: 0.7985 - val_loss: 0.7163 - val_accuracy: 0.7708\n",
      "Epoch 52/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5971 - accuracy: 0.8012\n",
      "Epoch 00052: val_loss did not improve from 0.69432\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6013 - accuracy: 0.7995 - val_loss: 0.7028 - val_accuracy: 0.7756\n",
      "Epoch 53/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5893 - accuracy: 0.7999\n",
      "Epoch 00053: val_loss improved from 0.69432 to 0.68214, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5971 - accuracy: 0.7988 - val_loss: 0.6821 - val_accuracy: 0.7729\n",
      "Epoch 54/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5849 - accuracy: 0.8116\n",
      "Epoch 00054: val_loss did not improve from 0.68214\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5825 - accuracy: 0.8127 - val_loss: 0.6831 - val_accuracy: 0.7735\n",
      "Epoch 55/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5751 - accuracy: 0.8134\n",
      "Epoch 00055: val_loss did not improve from 0.68214\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5803 - accuracy: 0.8120 - val_loss: 0.6889 - val_accuracy: 0.7674\n",
      "Epoch 56/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5762 - accuracy: 0.8093\n",
      "Epoch 00056: val_loss improved from 0.68214 to 0.67463, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5833 - accuracy: 0.8057 - val_loss: 0.6746 - val_accuracy: 0.7804\n",
      "Epoch 57/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5687 - accuracy: 0.8152\n",
      "Epoch 00057: val_loss improved from 0.67463 to 0.67453, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5751 - accuracy: 0.8125 - val_loss: 0.6745 - val_accuracy: 0.7790\n",
      "Epoch 58/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5674 - accuracy: 0.8164\n",
      "Epoch 00058: val_loss improved from 0.67453 to 0.66291, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5651 - accuracy: 0.8174 - val_loss: 0.6629 - val_accuracy: 0.7810\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.8173\n",
      "Epoch 00059: val_loss did not improve from 0.66291\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5650 - accuracy: 0.8173 - val_loss: 0.6687 - val_accuracy: 0.7797\n",
      "Epoch 60/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5597 - accuracy: 0.8181\n",
      "Epoch 00060: val_loss improved from 0.66291 to 0.65536, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5605 - accuracy: 0.8186 - val_loss: 0.6554 - val_accuracy: 0.7865\n",
      "Epoch 61/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5570 - accuracy: 0.8207\n",
      "Epoch 00061: val_loss did not improve from 0.65536\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5531 - accuracy: 0.8224 - val_loss: 0.6612 - val_accuracy: 0.7858\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.8191\n",
      "Epoch 00062: val_loss improved from 0.65536 to 0.64182, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5533 - accuracy: 0.8191 - val_loss: 0.6418 - val_accuracy: 0.7947\n",
      "Epoch 63/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5403 - accuracy: 0.8258\n",
      "Epoch 00063: val_loss improved from 0.64182 to 0.64125, saving model to models/saved_models/best_models_DP_1.0.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5403 - accuracy: 0.8251 - val_loss: 0.6412 - val_accuracy: 0.7933\n",
      "Epoch 64/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5451 - accuracy: 0.8238\n",
      "Epoch 00064: val_loss did not improve from 0.64125\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.5421 - accuracy: 0.8234 - val_loss: 0.6432 - val_accuracy: 0.7981\n",
      "Epoch 65/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5444 - accuracy: 0.8261\n",
      "Epoch 00065: val_loss did not improve from 0.64125\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5498 - accuracy: 0.8243 - val_loss: 0.6502 - val_accuracy: 0.7885\n",
      "Epoch 66/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5431 - accuracy: 0.8201\n",
      "Epoch 00066: val_loss improved from 0.64125 to 0.63508, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5489 - accuracy: 0.8186 - val_loss: 0.6351 - val_accuracy: 0.7981\n",
      "Epoch 67/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5318 - accuracy: 0.8269\n",
      "Epoch 00067: val_loss improved from 0.63508 to 0.63336, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5292 - accuracy: 0.8260 - val_loss: 0.6334 - val_accuracy: 0.7940\n",
      "Epoch 68/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5327 - accuracy: 0.8271\n",
      "Epoch 00068: val_loss did not improve from 0.63336\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5334 - accuracy: 0.8278 - val_loss: 0.6670 - val_accuracy: 0.7899\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5269 - accuracy: 0.8320\n",
      "Epoch 00069: val_loss did not improve from 0.63336\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5235 - accuracy: 0.8325 - val_loss: 0.6446 - val_accuracy: 0.7885\n",
      "Epoch 70/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5305 - accuracy: 0.8302\n",
      "Epoch 00070: val_loss improved from 0.63336 to 0.63031, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5264 - accuracy: 0.8314 - val_loss: 0.6303 - val_accuracy: 0.7988\n",
      "Epoch 71/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5140 - accuracy: 0.8371\n",
      "Epoch 00071: val_loss did not improve from 0.63031\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5113 - accuracy: 0.8369 - val_loss: 0.6346 - val_accuracy: 0.8001\n",
      "Epoch 72/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5150 - accuracy: 0.8354\n",
      "Epoch 00072: val_loss did not improve from 0.63031\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5184 - accuracy: 0.8347 - val_loss: 0.6362 - val_accuracy: 0.7995\n",
      "Epoch 73/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5122 - accuracy: 0.8343\n",
      "Epoch 00073: val_loss improved from 0.63031 to 0.61708, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5111 - accuracy: 0.8336 - val_loss: 0.6171 - val_accuracy: 0.7988\n",
      "Epoch 74/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5037 - accuracy: 0.8391\n",
      "Epoch 00074: val_loss did not improve from 0.61708\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5028 - accuracy: 0.8391 - val_loss: 0.6210 - val_accuracy: 0.8015\n",
      "Epoch 75/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4989 - accuracy: 0.8376\n",
      "Epoch 00075: val_loss did not improve from 0.61708\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5001 - accuracy: 0.8396 - val_loss: 0.6248 - val_accuracy: 0.7988\n",
      "Epoch 76/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5067 - accuracy: 0.8343\n",
      "Epoch 00076: val_loss did not improve from 0.61708\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5054 - accuracy: 0.8343 - val_loss: 0.6582 - val_accuracy: 0.7831\n",
      "Epoch 77/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4979 - accuracy: 0.8358\n",
      "Epoch 00077: val_loss did not improve from 0.61708\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4954 - accuracy: 0.8383 - val_loss: 0.6204 - val_accuracy: 0.7974\n",
      "Epoch 78/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4893 - accuracy: 0.8415\n",
      "Epoch 00078: val_loss did not improve from 0.61708\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4920 - accuracy: 0.8398 - val_loss: 0.6236 - val_accuracy: 0.7906\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.8374\n",
      "Epoch 00079: val_loss improved from 0.61708 to 0.60561, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5022 - accuracy: 0.8374 - val_loss: 0.6056 - val_accuracy: 0.8029\n",
      "Epoch 80/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4826 - accuracy: 0.8455\n",
      "Epoch 00080: val_loss improved from 0.60561 to 0.59563, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4828 - accuracy: 0.8446 - val_loss: 0.5956 - val_accuracy: 0.8049\n",
      "Epoch 81/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4767 - accuracy: 0.8469\n",
      "Epoch 00081: val_loss did not improve from 0.59563\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4776 - accuracy: 0.8473 - val_loss: 0.5998 - val_accuracy: 0.8049\n",
      "Epoch 82/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4764 - accuracy: 0.8428\n",
      "Epoch 00082: val_loss did not improve from 0.59563\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4759 - accuracy: 0.8427 - val_loss: 0.6054 - val_accuracy: 0.8056\n",
      "Epoch 83/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4708 - accuracy: 0.8501\n",
      "Epoch 00083: val_loss improved from 0.59563 to 0.59222, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4721 - accuracy: 0.8497 - val_loss: 0.5922 - val_accuracy: 0.8111\n",
      "Epoch 84/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.4778 - accuracy: 0.8421\n",
      "Epoch 00084: val_loss did not improve from 0.59222\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4848 - accuracy: 0.8413 - val_loss: 0.5933 - val_accuracy: 0.8131\n",
      "Epoch 85/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4658 - accuracy: 0.8493\n",
      "Epoch 00085: val_loss did not improve from 0.59222\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4685 - accuracy: 0.8488 - val_loss: 0.6071 - val_accuracy: 0.7974\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4757 - accuracy: 0.8432\n",
      "Epoch 00086: val_loss improved from 0.59222 to 0.58212, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4761 - accuracy: 0.8444 - val_loss: 0.5821 - val_accuracy: 0.8124\n",
      "Epoch 87/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4733 - accuracy: 0.8480\n",
      "Epoch 00087: val_loss did not improve from 0.58212\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4699 - accuracy: 0.8478 - val_loss: 0.5893 - val_accuracy: 0.8111\n",
      "Epoch 88/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4754 - accuracy: 0.8445\n",
      "Epoch 00088: val_loss did not improve from 0.58212\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4756 - accuracy: 0.8451 - val_loss: 0.5942 - val_accuracy: 0.8042\n",
      "Epoch 89/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4683 - accuracy: 0.8464\n",
      "Epoch 00089: val_loss did not improve from 0.58212\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4687 - accuracy: 0.8459 - val_loss: 0.5845 - val_accuracy: 0.8138\n",
      "Epoch 90/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4649 - accuracy: 0.8473\n",
      "Epoch 00090: val_loss did not improve from 0.58212\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4631 - accuracy: 0.8476 - val_loss: 0.5853 - val_accuracy: 0.8056\n",
      "Epoch 91/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4671 - accuracy: 0.8475\n",
      "Epoch 00091: val_loss improved from 0.58212 to 0.57893, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4636 - accuracy: 0.8475 - val_loss: 0.5789 - val_accuracy: 0.8199\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.8531\n",
      "Epoch 00092: val_loss did not improve from 0.57893\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4519 - accuracy: 0.8531 - val_loss: 0.5986 - val_accuracy: 0.8056\n",
      "Epoch 93/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4537 - accuracy: 0.8546\n",
      "Epoch 00093: val_loss improved from 0.57893 to 0.57360, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4547 - accuracy: 0.8548 - val_loss: 0.5736 - val_accuracy: 0.8138\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8575\n",
      "Epoch 00094: val_loss did not improve from 0.57360\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4494 - accuracy: 0.8575 - val_loss: 0.5905 - val_accuracy: 0.8138\n",
      "Epoch 95/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4391 - accuracy: 0.8578\n",
      "Epoch 00095: val_loss improved from 0.57360 to 0.56532, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4451 - accuracy: 0.8555 - val_loss: 0.5653 - val_accuracy: 0.8179\n",
      "Epoch 96/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4376 - accuracy: 0.8612\n",
      "Epoch 00096: val_loss did not improve from 0.56532\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4395 - accuracy: 0.8606 - val_loss: 0.5673 - val_accuracy: 0.8179\n",
      "Epoch 97/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4358 - accuracy: 0.8590\n",
      "Epoch 00097: val_loss did not improve from 0.56532\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4342 - accuracy: 0.8598 - val_loss: 0.5767 - val_accuracy: 0.8192\n",
      "Epoch 98/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4376 - accuracy: 0.8584\n",
      "Epoch 00098: val_loss did not improve from 0.56532\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4364 - accuracy: 0.8591 - val_loss: 0.5835 - val_accuracy: 0.8001\n",
      "Epoch 99/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4421 - accuracy: 0.8564\n",
      "Epoch 00099: val_loss did not improve from 0.56532\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4400 - accuracy: 0.8570 - val_loss: 0.5697 - val_accuracy: 0.8186\n",
      "Epoch 100/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4255 - accuracy: 0.8592\n",
      "Epoch 00100: val_loss improved from 0.56532 to 0.54620, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4300 - accuracy: 0.8586 - val_loss: 0.5462 - val_accuracy: 0.8342\n",
      "Epoch 101/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4229 - accuracy: 0.8635\n",
      "Epoch 00101: val_loss did not improve from 0.54620\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4215 - accuracy: 0.8642 - val_loss: 0.5526 - val_accuracy: 0.8247\n",
      "Epoch 102/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4140 - accuracy: 0.8638\n",
      "Epoch 00102: val_loss improved from 0.54620 to 0.54357, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4156 - accuracy: 0.8635 - val_loss: 0.5436 - val_accuracy: 0.8281\n",
      "Epoch 103/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4226 - accuracy: 0.8665\n",
      "Epoch 00103: val_loss did not improve from 0.54357\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4209 - accuracy: 0.8667 - val_loss: 0.5482 - val_accuracy: 0.8288\n",
      "Epoch 104/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4220 - accuracy: 0.8659\n",
      "Epoch 00104: val_loss improved from 0.54357 to 0.54022, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4247 - accuracy: 0.8642 - val_loss: 0.5402 - val_accuracy: 0.8302\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.8664\n",
      "Epoch 00105: val_loss did not improve from 0.54022\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4171 - accuracy: 0.8664 - val_loss: 0.5718 - val_accuracy: 0.8138\n",
      "Epoch 106/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4142 - accuracy: 0.8635\n",
      "Epoch 00106: val_loss improved from 0.54022 to 0.53356, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4161 - accuracy: 0.8630 - val_loss: 0.5336 - val_accuracy: 0.8308\n",
      "Epoch 107/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4055 - accuracy: 0.8691\n",
      "Epoch 00107: val_loss did not improve from 0.53356\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4038 - accuracy: 0.8693 - val_loss: 0.5478 - val_accuracy: 0.8254\n",
      "Epoch 108/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4062 - accuracy: 0.8702\n",
      "Epoch 00108: val_loss did not improve from 0.53356\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4046 - accuracy: 0.8707 - val_loss: 0.5395 - val_accuracy: 0.8267\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8703\n",
      "Epoch 00109: val_loss did not improve from 0.53356\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4001 - accuracy: 0.8703 - val_loss: 0.5480 - val_accuracy: 0.8233\n",
      "Epoch 110/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4006 - accuracy: 0.8741\n",
      "Epoch 00110: val_loss did not improve from 0.53356\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3996 - accuracy: 0.8732 - val_loss: 0.5366 - val_accuracy: 0.8302\n",
      "Epoch 111/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4061 - accuracy: 0.8676\n",
      "Epoch 00111: val_loss did not improve from 0.53356\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4035 - accuracy: 0.8698 - val_loss: 0.5420 - val_accuracy: 0.8172\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3877 - accuracy: 0.8759\n",
      "Epoch 00112: val_loss improved from 0.53356 to 0.52759, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3890 - accuracy: 0.8753 - val_loss: 0.5276 - val_accuracy: 0.8295\n",
      "Epoch 113/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3823 - accuracy: 0.8764\n",
      "Epoch 00113: val_loss improved from 0.52759 to 0.52557, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3866 - accuracy: 0.8753 - val_loss: 0.5256 - val_accuracy: 0.8356\n",
      "Epoch 114/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3839 - accuracy: 0.8757\n",
      "Epoch 00114: val_loss did not improve from 0.52557\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3846 - accuracy: 0.8753 - val_loss: 0.5271 - val_accuracy: 0.8370\n",
      "Epoch 115/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3805 - accuracy: 0.8769\n",
      "Epoch 00115: val_loss did not improve from 0.52557\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3824 - accuracy: 0.8741 - val_loss: 0.5264 - val_accuracy: 0.8370\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3782 - accuracy: 0.8795\n",
      "Epoch 00116: val_loss did not improve from 0.52557\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3799 - accuracy: 0.8780 - val_loss: 0.5283 - val_accuracy: 0.8247\n",
      "Epoch 117/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3895 - accuracy: 0.8752\n",
      "Epoch 00117: val_loss did not improve from 0.52557\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3871 - accuracy: 0.8756 - val_loss: 0.5281 - val_accuracy: 0.8383\n",
      "Epoch 118/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3792 - accuracy: 0.8770\n",
      "Epoch 00118: val_loss did not improve from 0.52557\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3804 - accuracy: 0.8773 - val_loss: 0.5368 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3755 - accuracy: 0.8770\n",
      "Epoch 00119: val_loss improved from 0.52557 to 0.51870, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3733 - accuracy: 0.8782 - val_loss: 0.5187 - val_accuracy: 0.8226\n",
      "Epoch 120/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3736 - accuracy: 0.8773\n",
      "Epoch 00120: val_loss did not improve from 0.51870\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3728 - accuracy: 0.8772 - val_loss: 0.5266 - val_accuracy: 0.8342\n",
      "Epoch 121/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3731 - accuracy: 0.8795\n",
      "Epoch 00121: val_loss improved from 0.51870 to 0.50895, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3777 - accuracy: 0.8777 - val_loss: 0.5090 - val_accuracy: 0.8383\n",
      "Epoch 122/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3647 - accuracy: 0.8844\n",
      "Epoch 00122: val_loss improved from 0.50895 to 0.50448, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3627 - accuracy: 0.8843 - val_loss: 0.5045 - val_accuracy: 0.8452\n",
      "Epoch 123/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3673 - accuracy: 0.8820\n",
      "Epoch 00123: val_loss did not improve from 0.50448\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3636 - accuracy: 0.8850 - val_loss: 0.5251 - val_accuracy: 0.8302\n",
      "Epoch 124/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3702 - accuracy: 0.8811\n",
      "Epoch 00124: val_loss did not improve from 0.50448\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3705 - accuracy: 0.8807 - val_loss: 0.5304 - val_accuracy: 0.8295\n",
      "Epoch 125/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3661 - accuracy: 0.8802\n",
      "Epoch 00125: val_loss did not improve from 0.50448\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3653 - accuracy: 0.8802 - val_loss: 0.5063 - val_accuracy: 0.8404\n",
      "Epoch 126/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3631 - accuracy: 0.8834\n",
      "Epoch 00126: val_loss did not improve from 0.50448\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3653 - accuracy: 0.8818 - val_loss: 0.5211 - val_accuracy: 0.8356\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3604 - accuracy: 0.8828\n",
      "Epoch 00127: val_loss did not improve from 0.50448\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3628 - accuracy: 0.8826 - val_loss: 0.5089 - val_accuracy: 0.8390\n",
      "Epoch 128/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3604 - accuracy: 0.8793\n",
      "Epoch 00128: val_loss improved from 0.50448 to 0.50054, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3593 - accuracy: 0.8783 - val_loss: 0.5005 - val_accuracy: 0.8424\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8893\n",
      "Epoch 00129: val_loss improved from 0.50054 to 0.49835, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3509 - accuracy: 0.8893 - val_loss: 0.4984 - val_accuracy: 0.8458\n",
      "Epoch 130/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3507 - accuracy: 0.8824\n",
      "Epoch 00130: val_loss did not improve from 0.49835\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3538 - accuracy: 0.8814 - val_loss: 0.5225 - val_accuracy: 0.8261\n",
      "Epoch 131/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3548 - accuracy: 0.8830\n",
      "Epoch 00131: val_loss improved from 0.49835 to 0.49754, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3539 - accuracy: 0.8830 - val_loss: 0.4975 - val_accuracy: 0.8431\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8925\n",
      "Epoch 00132: val_loss improved from 0.49754 to 0.48723, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3379 - accuracy: 0.8925 - val_loss: 0.4872 - val_accuracy: 0.8431\n",
      "Epoch 133/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3411 - accuracy: 0.8851\n",
      "Epoch 00133: val_loss did not improve from 0.48723\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3441 - accuracy: 0.8848 - val_loss: 0.5126 - val_accuracy: 0.8322\n",
      "Epoch 134/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3677 - accuracy: 0.8785\n",
      "Epoch 00134: val_loss did not improve from 0.48723\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3645 - accuracy: 0.8789 - val_loss: 0.4985 - val_accuracy: 0.8452\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3437 - accuracy: 0.8845\n",
      "Epoch 00135: val_loss did not improve from 0.48723\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3455 - accuracy: 0.8847 - val_loss: 0.4909 - val_accuracy: 0.8417\n",
      "Epoch 136/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3406 - accuracy: 0.8893\n",
      "Epoch 00136: val_loss did not improve from 0.48723\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3401 - accuracy: 0.8896 - val_loss: 0.4909 - val_accuracy: 0.8452\n",
      "Epoch 137/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3472 - accuracy: 0.8921\n",
      "Epoch 00137: val_loss did not improve from 0.48723\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3485 - accuracy: 0.8913 - val_loss: 0.4888 - val_accuracy: 0.8363\n",
      "Epoch 138/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3306 - accuracy: 0.8926\n",
      "Epoch 00138: val_loss did not improve from 0.48723\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3313 - accuracy: 0.8932 - val_loss: 0.4880 - val_accuracy: 0.8520\n",
      "Epoch 139/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3235 - accuracy: 0.9007\n",
      "Epoch 00139: val_loss improved from 0.48723 to 0.48440, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3269 - accuracy: 0.8990 - val_loss: 0.4844 - val_accuracy: 0.8472\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8915\n",
      "Epoch 00140: val_loss improved from 0.48440 to 0.47603, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3301 - accuracy: 0.8915 - val_loss: 0.4760 - val_accuracy: 0.8513\n",
      "Epoch 141/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3246 - accuracy: 0.8904\n",
      "Epoch 00141: val_loss did not improve from 0.47603\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3269 - accuracy: 0.8900 - val_loss: 0.4874 - val_accuracy: 0.8479\n",
      "Epoch 142/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3221 - accuracy: 0.8988\n",
      "Epoch 00142: val_loss did not improve from 0.47603\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3210 - accuracy: 0.8985 - val_loss: 0.4992 - val_accuracy: 0.8390\n",
      "Epoch 143/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3241 - accuracy: 0.8921\n",
      "Epoch 00143: val_loss did not improve from 0.47603\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3304 - accuracy: 0.8905 - val_loss: 0.4876 - val_accuracy: 0.8458\n",
      "Epoch 144/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3134 - accuracy: 0.8992\n",
      "Epoch 00144: val_loss did not improve from 0.47603\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3152 - accuracy: 0.8983 - val_loss: 0.4801 - val_accuracy: 0.8438\n",
      "Epoch 145/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3287 - accuracy: 0.8928\n",
      "Epoch 00145: val_loss did not improve from 0.47603\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3297 - accuracy: 0.8922 - val_loss: 0.4884 - val_accuracy: 0.8486\n",
      "Epoch 146/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3326 - accuracy: 0.8903\n",
      "Epoch 00146: val_loss did not improve from 0.47603\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3318 - accuracy: 0.8923 - val_loss: 0.4952 - val_accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3154 - accuracy: 0.8949\n",
      "Epoch 00147: val_loss improved from 0.47603 to 0.46091, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3200 - accuracy: 0.8932 - val_loss: 0.4609 - val_accuracy: 0.8568\n",
      "Epoch 148/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3116 - accuracy: 0.8999\n",
      "Epoch 00148: val_loss did not improve from 0.46091\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3086 - accuracy: 0.9010 - val_loss: 0.4729 - val_accuracy: 0.8479\n",
      "Epoch 149/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3085 - accuracy: 0.8996\n",
      "Epoch 00149: val_loss did not improve from 0.46091\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3103 - accuracy: 0.8992 - val_loss: 0.4785 - val_accuracy: 0.8445\n",
      "Epoch 150/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3022 - accuracy: 0.9023\n",
      "Epoch 00150: val_loss did not improve from 0.46091\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3037 - accuracy: 0.9021 - val_loss: 0.4834 - val_accuracy: 0.8424\n",
      "Epoch 151/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3096 - accuracy: 0.8975\n",
      "Epoch 00151: val_loss improved from 0.46091 to 0.46053, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3090 - accuracy: 0.8976 - val_loss: 0.4605 - val_accuracy: 0.8479\n",
      "Epoch 152/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3079 - accuracy: 0.9068\n",
      "Epoch 00152: val_loss did not improve from 0.46053\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3080 - accuracy: 0.9062 - val_loss: 0.4803 - val_accuracy: 0.8492\n",
      "Epoch 153/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3064 - accuracy: 0.9029\n",
      "Epoch 00153: val_loss improved from 0.46053 to 0.45987, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3051 - accuracy: 0.9022 - val_loss: 0.4599 - val_accuracy: 0.8506\n",
      "Epoch 154/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2928 - accuracy: 0.9031\n",
      "Epoch 00154: val_loss improved from 0.45987 to 0.45157, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2947 - accuracy: 0.9031 - val_loss: 0.4516 - val_accuracy: 0.8527\n",
      "Epoch 155/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2849 - accuracy: 0.9087\n",
      "Epoch 00155: val_loss improved from 0.45157 to 0.44925, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2856 - accuracy: 0.9072 - val_loss: 0.4493 - val_accuracy: 0.8506\n",
      "Epoch 156/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2922 - accuracy: 0.9055\n",
      "Epoch 00156: val_loss did not improve from 0.44925\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2977 - accuracy: 0.9031 - val_loss: 0.4740 - val_accuracy: 0.8520\n",
      "Epoch 157/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2987 - accuracy: 0.9016\n",
      "Epoch 00157: val_loss did not improve from 0.44925\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2970 - accuracy: 0.9021 - val_loss: 0.4820 - val_accuracy: 0.8452\n",
      "Epoch 158/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2926 - accuracy: 0.9036\n",
      "Epoch 00158: val_loss did not improve from 0.44925\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2951 - accuracy: 0.9024 - val_loss: 0.4606 - val_accuracy: 0.8595\n",
      "Epoch 159/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2904 - accuracy: 0.9090\n",
      "Epoch 00159: val_loss did not improve from 0.44925\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2893 - accuracy: 0.9092 - val_loss: 0.4651 - val_accuracy: 0.8568\n",
      "Epoch 160/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2835 - accuracy: 0.9085\n",
      "Epoch 00160: val_loss did not improve from 0.44925\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2835 - accuracy: 0.9091 - val_loss: 0.4608 - val_accuracy: 0.8520\n",
      "Epoch 161/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2851 - accuracy: 0.9096\n",
      "Epoch 00161: val_loss did not improve from 0.44925\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2884 - accuracy: 0.9087 - val_loss: 0.4714 - val_accuracy: 0.8506\n",
      "Epoch 162/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2714 - accuracy: 0.9195\n",
      "Epoch 00162: val_loss improved from 0.44925 to 0.43852, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2777 - accuracy: 0.9162 - val_loss: 0.4385 - val_accuracy: 0.8608\n",
      "Epoch 163/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2860 - accuracy: 0.9054\n",
      "Epoch 00163: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2887 - accuracy: 0.9039 - val_loss: 0.4635 - val_accuracy: 0.8472\n",
      "Epoch 164/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2861 - accuracy: 0.9049\n",
      "Epoch 00164: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2841 - accuracy: 0.9046 - val_loss: 0.4626 - val_accuracy: 0.8513\n",
      "Epoch 165/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2829 - accuracy: 0.9064\n",
      "Epoch 00165: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2807 - accuracy: 0.9074 - val_loss: 0.4718 - val_accuracy: 0.8561\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3004 - accuracy: 0.8983\n",
      "Epoch 00166: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2962 - accuracy: 0.9002 - val_loss: 0.5101 - val_accuracy: 0.8492\n",
      "Epoch 167/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3034 - accuracy: 0.8991\n",
      "Epoch 00167: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.3006 - accuracy: 0.9005 - val_loss: 0.4489 - val_accuracy: 0.8595\n",
      "Epoch 168/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2747 - accuracy: 0.9087\n",
      "Epoch 00168: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2742 - accuracy: 0.9091 - val_loss: 0.4680 - val_accuracy: 0.8445\n",
      "Epoch 169/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2882 - accuracy: 0.9070\n",
      "Epoch 00169: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2843 - accuracy: 0.9080 - val_loss: 0.4425 - val_accuracy: 0.8581\n",
      "Epoch 170/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2740 - accuracy: 0.9079\n",
      "Epoch 00170: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2725 - accuracy: 0.9087 - val_loss: 0.4533 - val_accuracy: 0.8568\n",
      "Epoch 171/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2735 - accuracy: 0.9103\n",
      "Epoch 00171: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2772 - accuracy: 0.9096 - val_loss: 0.4421 - val_accuracy: 0.8595\n",
      "Epoch 172/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2663 - accuracy: 0.9139\n",
      "Epoch 00172: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2703 - accuracy: 0.9125 - val_loss: 0.4776 - val_accuracy: 0.8404\n",
      "Epoch 173/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2774 - accuracy: 0.9092\n",
      "Epoch 00173: val_loss did not improve from 0.43852\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2772 - accuracy: 0.9087 - val_loss: 0.4524 - val_accuracy: 0.8568\n",
      "Epoch 174/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2590 - accuracy: 0.9142\n",
      "Epoch 00174: val_loss improved from 0.43852 to 0.43429, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2612 - accuracy: 0.9138 - val_loss: 0.4343 - val_accuracy: 0.8554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2668 - accuracy: 0.9137\n",
      "Epoch 00175: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2685 - accuracy: 0.9132 - val_loss: 0.4443 - val_accuracy: 0.8636\n",
      "Epoch 176/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2686 - accuracy: 0.9133\n",
      "Epoch 00176: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2698 - accuracy: 0.9128 - val_loss: 0.4726 - val_accuracy: 0.8554\n",
      "Epoch 177/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2742 - accuracy: 0.9102\n",
      "Epoch 00177: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2769 - accuracy: 0.9079 - val_loss: 0.4384 - val_accuracy: 0.8540\n",
      "Epoch 178/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2756 - accuracy: 0.9055\n",
      "Epoch 00178: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2749 - accuracy: 0.9060 - val_loss: 0.4655 - val_accuracy: 0.8547\n",
      "Epoch 179/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2735 - accuracy: 0.9057\n",
      "Epoch 00179: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2730 - accuracy: 0.9062 - val_loss: 0.4826 - val_accuracy: 0.8479\n",
      "Epoch 180/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2803 - accuracy: 0.9068\n",
      "Epoch 00180: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2852 - accuracy: 0.9060 - val_loss: 0.4473 - val_accuracy: 0.8588\n",
      "Epoch 181/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2942 - accuracy: 0.9022\n",
      "Epoch 00181: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2947 - accuracy: 0.9004 - val_loss: 0.4786 - val_accuracy: 0.8458\n",
      "Epoch 182/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2656 - accuracy: 0.9137\n",
      "Epoch 00182: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2663 - accuracy: 0.9120 - val_loss: 0.4571 - val_accuracy: 0.8445\n",
      "Epoch 183/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2501 - accuracy: 0.9193\n",
      "Epoch 00183: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2527 - accuracy: 0.9184 - val_loss: 0.4464 - val_accuracy: 0.8622\n",
      "Epoch 184/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2550 - accuracy: 0.9176\n",
      "Epoch 00184: val_loss did not improve from 0.43429\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2584 - accuracy: 0.9145 - val_loss: 0.4522 - val_accuracy: 0.8581\n",
      "Epoch 185/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2501 - accuracy: 0.9191\n",
      "Epoch 00185: val_loss improved from 0.43429 to 0.42789, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2488 - accuracy: 0.9195 - val_loss: 0.4279 - val_accuracy: 0.8629\n",
      "Epoch 186/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2565 - accuracy: 0.9148\n",
      "Epoch 00186: val_loss did not improve from 0.42789\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2577 - accuracy: 0.9135 - val_loss: 0.4335 - val_accuracy: 0.8581\n",
      "Epoch 187/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2472 - accuracy: 0.9189\n",
      "Epoch 00187: val_loss did not improve from 0.42789\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2462 - accuracy: 0.9198 - val_loss: 0.4371 - val_accuracy: 0.8595\n",
      "Epoch 188/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2429 - accuracy: 0.9209\n",
      "Epoch 00188: val_loss improved from 0.42789 to 0.42566, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2446 - accuracy: 0.9202 - val_loss: 0.4257 - val_accuracy: 0.8677\n",
      "Epoch 189/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2430 - accuracy: 0.9205\n",
      "Epoch 00189: val_loss did not improve from 0.42566\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2446 - accuracy: 0.9193 - val_loss: 0.4459 - val_accuracy: 0.8622\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.9181\n",
      "Epoch 00190: val_loss did not improve from 0.42566\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2428 - accuracy: 0.9181 - val_loss: 0.4467 - val_accuracy: 0.8636\n",
      "Epoch 191/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2432 - accuracy: 0.9182\n",
      "Epoch 00191: val_loss did not improve from 0.42566\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2451 - accuracy: 0.9179 - val_loss: 0.4456 - val_accuracy: 0.8636\n",
      "Epoch 192/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2392 - accuracy: 0.9219\n",
      "Epoch 00192: val_loss improved from 0.42566 to 0.42541, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2404 - accuracy: 0.9217 - val_loss: 0.4254 - val_accuracy: 0.8643\n",
      "Epoch 193/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2463 - accuracy: 0.9183\n",
      "Epoch 00193: val_loss did not improve from 0.42541\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2471 - accuracy: 0.9183 - val_loss: 0.4353 - val_accuracy: 0.8602\n",
      "Epoch 194/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2608 - accuracy: 0.9113\n",
      "Epoch 00194: val_loss did not improve from 0.42541\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2619 - accuracy: 0.9096 - val_loss: 0.4562 - val_accuracy: 0.8561\n",
      "Epoch 195/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2643 - accuracy: 0.9094\n",
      "Epoch 00195: val_loss did not improve from 0.42541\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2572 - accuracy: 0.9116 - val_loss: 0.4555 - val_accuracy: 0.8581\n",
      "Epoch 196/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2535 - accuracy: 0.9180\n",
      "Epoch 00196: val_loss did not improve from 0.42541\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2532 - accuracy: 0.9172 - val_loss: 0.4377 - val_accuracy: 0.8574\n",
      "Epoch 197/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2375 - accuracy: 0.9241\n",
      "Epoch 00197: val_loss improved from 0.42541 to 0.41576, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2400 - accuracy: 0.9234 - val_loss: 0.4158 - val_accuracy: 0.8683\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.9217\n",
      "Epoch 00198: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2367 - accuracy: 0.9217 - val_loss: 0.4331 - val_accuracy: 0.8697\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9225\n",
      "Epoch 00199: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2338 - accuracy: 0.9225 - val_loss: 0.4313 - val_accuracy: 0.8677\n",
      "Epoch 200/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2375 - accuracy: 0.9221\n",
      "Epoch 00200: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2376 - accuracy: 0.9222 - val_loss: 0.4266 - val_accuracy: 0.8656\n",
      "Epoch 201/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2413 - accuracy: 0.9199\n",
      "Epoch 00201: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2412 - accuracy: 0.9196 - val_loss: 0.4302 - val_accuracy: 0.8724\n",
      "Epoch 202/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2431 - accuracy: 0.9174\n",
      "Epoch 00202: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2441 - accuracy: 0.9162 - val_loss: 0.4490 - val_accuracy: 0.8656\n",
      "Epoch 203/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2365 - accuracy: 0.9204\n",
      "Epoch 00203: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2365 - accuracy: 0.9200 - val_loss: 0.4387 - val_accuracy: 0.8656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2343 - accuracy: 0.9202\n",
      "Epoch 00204: val_loss did not improve from 0.41576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2333 - accuracy: 0.9205 - val_loss: 0.4242 - val_accuracy: 0.8643\n",
      "Epoch 205/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2218 - accuracy: 0.9275\n",
      "Epoch 00205: val_loss improved from 0.41576 to 0.41505, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2243 - accuracy: 0.9275 - val_loss: 0.4150 - val_accuracy: 0.8718\n",
      "Epoch 206/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2244 - accuracy: 0.9260\n",
      "Epoch 00206: val_loss improved from 0.41505 to 0.40907, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2267 - accuracy: 0.9242 - val_loss: 0.4091 - val_accuracy: 0.8731\n",
      "Epoch 207/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2251 - accuracy: 0.9238\n",
      "Epoch 00207: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2226 - accuracy: 0.9246 - val_loss: 0.4336 - val_accuracy: 0.8636\n",
      "Epoch 208/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2428 - accuracy: 0.9172\n",
      "Epoch 00208: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2399 - accuracy: 0.9172 - val_loss: 0.4376 - val_accuracy: 0.8622\n",
      "Epoch 209/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2261 - accuracy: 0.9271\n",
      "Epoch 00209: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2222 - accuracy: 0.9287 - val_loss: 0.4306 - val_accuracy: 0.8711\n",
      "Epoch 210/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2228 - accuracy: 0.9271\n",
      "Epoch 00210: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2209 - accuracy: 0.9280 - val_loss: 0.4138 - val_accuracy: 0.8649\n",
      "Epoch 211/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2263 - accuracy: 0.9270\n",
      "Epoch 00211: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2278 - accuracy: 0.9254 - val_loss: 0.4321 - val_accuracy: 0.8731\n",
      "Epoch 212/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2311 - accuracy: 0.9199\n",
      "Epoch 00212: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2294 - accuracy: 0.9220 - val_loss: 0.4333 - val_accuracy: 0.8636\n",
      "Epoch 213/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2199 - accuracy: 0.9289\n",
      "Epoch 00213: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2190 - accuracy: 0.9282 - val_loss: 0.4203 - val_accuracy: 0.8677\n",
      "Epoch 214/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2116 - accuracy: 0.9302\n",
      "Epoch 00214: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2171 - accuracy: 0.9283 - val_loss: 0.4369 - val_accuracy: 0.8649\n",
      "Epoch 215/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2267 - accuracy: 0.9215\n",
      "Epoch 00215: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2227 - accuracy: 0.9229 - val_loss: 0.4325 - val_accuracy: 0.8738\n",
      "Epoch 216/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2138 - accuracy: 0.9291\n",
      "Epoch 00216: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2111 - accuracy: 0.9312 - val_loss: 0.4290 - val_accuracy: 0.8772\n",
      "Epoch 217/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2171 - accuracy: 0.9280\n",
      "Epoch 00217: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2180 - accuracy: 0.9280 - val_loss: 0.4242 - val_accuracy: 0.8690\n",
      "Epoch 218/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2175 - accuracy: 0.9266\n",
      "Epoch 00218: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2174 - accuracy: 0.9278 - val_loss: 0.4226 - val_accuracy: 0.8711\n",
      "Epoch 219/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2075 - accuracy: 0.9334\n",
      "Epoch 00219: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2111 - accuracy: 0.9319 - val_loss: 0.4246 - val_accuracy: 0.8711\n",
      "Epoch 220/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2126 - accuracy: 0.9304\n",
      "Epoch 00220: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2114 - accuracy: 0.9316 - val_loss: 0.4103 - val_accuracy: 0.8731\n",
      "Epoch 221/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2082 - accuracy: 0.9304\n",
      "Epoch 00221: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2084 - accuracy: 0.9306 - val_loss: 0.4258 - val_accuracy: 0.8683\n",
      "Epoch 222/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2177 - accuracy: 0.9279\n",
      "Epoch 00222: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2158 - accuracy: 0.9283 - val_loss: 0.4174 - val_accuracy: 0.8677\n",
      "Epoch 223/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2115 - accuracy: 0.9288\n",
      "Epoch 00223: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2116 - accuracy: 0.9294 - val_loss: 0.4350 - val_accuracy: 0.8670\n",
      "Epoch 224/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2070 - accuracy: 0.9345\n",
      "Epoch 00224: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2089 - accuracy: 0.9335 - val_loss: 0.4110 - val_accuracy: 0.8690\n",
      "Epoch 225/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2062 - accuracy: 0.9336\n",
      "Epoch 00225: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2059 - accuracy: 0.9335 - val_loss: 0.4217 - val_accuracy: 0.8670\n",
      "Epoch 226/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2131 - accuracy: 0.9317\n",
      "Epoch 00226: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2165 - accuracy: 0.9307 - val_loss: 0.4367 - val_accuracy: 0.8724\n",
      "Epoch 227/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2165 - accuracy: 0.9273\n",
      "Epoch 00227: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2161 - accuracy: 0.9265 - val_loss: 0.4232 - val_accuracy: 0.8765\n",
      "Epoch 228/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2091 - accuracy: 0.9319\n",
      "Epoch 00228: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2100 - accuracy: 0.9321 - val_loss: 0.4351 - val_accuracy: 0.8629\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9265\n",
      "Epoch 00229: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2181 - accuracy: 0.9265 - val_loss: 0.4521 - val_accuracy: 0.8629\n",
      "Epoch 230/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2093 - accuracy: 0.9315\n",
      "Epoch 00230: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2068 - accuracy: 0.9318 - val_loss: 0.4093 - val_accuracy: 0.8786\n",
      "Epoch 231/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2066 - accuracy: 0.9301\n",
      "Epoch 00231: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2086 - accuracy: 0.9302 - val_loss: 0.4197 - val_accuracy: 0.8813\n",
      "Epoch 232/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1993 - accuracy: 0.9360\n",
      "Epoch 00232: val_loss did not improve from 0.40907\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2036 - accuracy: 0.9353 - val_loss: 0.4347 - val_accuracy: 0.8683\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2061 - accuracy: 0.9301\n",
      "Epoch 00233: val_loss improved from 0.40907 to 0.40374, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2049 - accuracy: 0.9307 - val_loss: 0.4037 - val_accuracy: 0.8799\n",
      "Epoch 234/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1970 - accuracy: 0.9364\n",
      "Epoch 00234: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1968 - accuracy: 0.9369 - val_loss: 0.4114 - val_accuracy: 0.8799\n",
      "Epoch 235/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.9369\n",
      "Epoch 00235: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2000 - accuracy: 0.9341 - val_loss: 0.4265 - val_accuracy: 0.8656\n",
      "Epoch 236/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2060 - accuracy: 0.9325\n",
      "Epoch 00236: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2037 - accuracy: 0.9331 - val_loss: 0.4546 - val_accuracy: 0.8581\n",
      "Epoch 237/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2079 - accuracy: 0.9315\n",
      "Epoch 00237: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2099 - accuracy: 0.9306 - val_loss: 0.4067 - val_accuracy: 0.8718\n",
      "Epoch 238/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1943 - accuracy: 0.9348\n",
      "Epoch 00238: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1958 - accuracy: 0.9345 - val_loss: 0.4247 - val_accuracy: 0.8718\n",
      "Epoch 239/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1998 - accuracy: 0.9368\n",
      "Epoch 00239: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1958 - accuracy: 0.9367 - val_loss: 0.4062 - val_accuracy: 0.8786\n",
      "Epoch 240/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1961 - accuracy: 0.9336\n",
      "Epoch 00240: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1938 - accuracy: 0.9336 - val_loss: 0.4101 - val_accuracy: 0.8670\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.9277\n",
      "Epoch 00241: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2068 - accuracy: 0.9277 - val_loss: 0.4208 - val_accuracy: 0.8690\n",
      "Epoch 242/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1903 - accuracy: 0.9379\n",
      "Epoch 00242: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1934 - accuracy: 0.9370 - val_loss: 0.4110 - val_accuracy: 0.8765\n",
      "Epoch 243/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2004 - accuracy: 0.9332\n",
      "Epoch 00243: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1974 - accuracy: 0.9345 - val_loss: 0.4112 - val_accuracy: 0.8731\n",
      "Epoch 244/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2061 - accuracy: 0.9327\n",
      "Epoch 00244: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2082 - accuracy: 0.9324 - val_loss: 0.4364 - val_accuracy: 0.8704\n",
      "Epoch 245/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1951 - accuracy: 0.9362\n",
      "Epoch 00245: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1944 - accuracy: 0.9357 - val_loss: 0.4177 - val_accuracy: 0.8759\n",
      "Epoch 246/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2124 - accuracy: 0.9302\n",
      "Epoch 00246: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2145 - accuracy: 0.9295 - val_loss: 0.4350 - val_accuracy: 0.8629\n",
      "Epoch 247/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2004 - accuracy: 0.9330\n",
      "Epoch 00247: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2020 - accuracy: 0.9319 - val_loss: 0.4476 - val_accuracy: 0.8731\n",
      "Epoch 248/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1883 - accuracy: 0.9410\n",
      "Epoch 00248: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1898 - accuracy: 0.9405 - val_loss: 0.4088 - val_accuracy: 0.8806\n",
      "Epoch 249/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1826 - accuracy: 0.9418\n",
      "Epoch 00249: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1805 - accuracy: 0.9425 - val_loss: 0.4049 - val_accuracy: 0.8915\n",
      "Epoch 250/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1884 - accuracy: 0.9362\n",
      "Epoch 00250: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1928 - accuracy: 0.9345 - val_loss: 0.4129 - val_accuracy: 0.8806\n",
      "Epoch 251/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1774 - accuracy: 0.9442\n",
      "Epoch 00251: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1823 - accuracy: 0.9415 - val_loss: 0.4148 - val_accuracy: 0.8759\n",
      "Epoch 252/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1961 - accuracy: 0.9352\n",
      "Epoch 00252: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1968 - accuracy: 0.9345 - val_loss: 0.4198 - val_accuracy: 0.8779\n",
      "Epoch 253/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1884 - accuracy: 0.9395\n",
      "Epoch 00253: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1873 - accuracy: 0.9396 - val_loss: 0.4094 - val_accuracy: 0.8834\n",
      "Epoch 254/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1844 - accuracy: 0.9425\n",
      "Epoch 00254: val_loss did not improve from 0.40374\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1816 - accuracy: 0.9428 - val_loss: 0.4077 - val_accuracy: 0.8827\n",
      "Epoch 255/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1745 - accuracy: 0.9420\n",
      "Epoch 00255: val_loss improved from 0.40374 to 0.39885, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1745 - accuracy: 0.9428 - val_loss: 0.3988 - val_accuracy: 0.8834\n",
      "Epoch 256/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1876 - accuracy: 0.9358\n",
      "Epoch 00256: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1901 - accuracy: 0.9353 - val_loss: 0.4128 - val_accuracy: 0.8820\n",
      "Epoch 257/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1822 - accuracy: 0.9391\n",
      "Epoch 00257: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1817 - accuracy: 0.9396 - val_loss: 0.4240 - val_accuracy: 0.8799\n",
      "Epoch 258/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1938 - accuracy: 0.9343\n",
      "Epoch 00258: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1943 - accuracy: 0.9341 - val_loss: 0.4094 - val_accuracy: 0.8779\n",
      "Epoch 259/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1907 - accuracy: 0.9355\n",
      "Epoch 00259: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1880 - accuracy: 0.9362 - val_loss: 0.4221 - val_accuracy: 0.8752\n",
      "Epoch 260/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1865 - accuracy: 0.9366\n",
      "Epoch 00260: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1820 - accuracy: 0.9379 - val_loss: 0.4271 - val_accuracy: 0.8765\n",
      "Epoch 261/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1774 - accuracy: 0.9412\n",
      "Epoch 00261: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1796 - accuracy: 0.9408 - val_loss: 0.4071 - val_accuracy: 0.8779\n",
      "Epoch 262/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1866 - accuracy: 0.9375\n",
      "Epoch 00262: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1833 - accuracy: 0.9387 - val_loss: 0.4165 - val_accuracy: 0.8765\n",
      "Epoch 263/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.9450\n",
      "Epoch 00263: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1754 - accuracy: 0.9449 - val_loss: 0.4230 - val_accuracy: 0.8752\n",
      "Epoch 264/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1819 - accuracy: 0.9403\n",
      "Epoch 00264: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1818 - accuracy: 0.9403 - val_loss: 0.4094 - val_accuracy: 0.8799\n",
      "Epoch 265/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1836 - accuracy: 0.9395\n",
      "Epoch 00265: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1786 - accuracy: 0.9420 - val_loss: 0.4290 - val_accuracy: 0.8759\n",
      "Epoch 266/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1703 - accuracy: 0.9457\n",
      "Epoch 00266: val_loss did not improve from 0.39885\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1706 - accuracy: 0.9454 - val_loss: 0.4364 - val_accuracy: 0.8697\n",
      "Epoch 267/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1876 - accuracy: 0.9381\n",
      "Epoch 00267: val_loss improved from 0.39885 to 0.39693, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1856 - accuracy: 0.9384 - val_loss: 0.3969 - val_accuracy: 0.8854\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9358\n",
      "Epoch 00268: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1847 - accuracy: 0.9358 - val_loss: 0.4075 - val_accuracy: 0.8779\n",
      "Epoch 269/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9410\n",
      "Epoch 00269: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1817 - accuracy: 0.9408 - val_loss: 0.4333 - val_accuracy: 0.8724\n",
      "Epoch 270/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1762 - accuracy: 0.9399\n",
      "Epoch 00270: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1786 - accuracy: 0.9403 - val_loss: 0.4020 - val_accuracy: 0.8909\n",
      "Epoch 271/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1720 - accuracy: 0.9427\n",
      "Epoch 00271: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1729 - accuracy: 0.9425 - val_loss: 0.4084 - val_accuracy: 0.8813\n",
      "Epoch 272/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1786 - accuracy: 0.9381\n",
      "Epoch 00272: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1771 - accuracy: 0.9386 - val_loss: 0.3985 - val_accuracy: 0.8854\n",
      "Epoch 273/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1649 - accuracy: 0.9455\n",
      "Epoch 00273: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1670 - accuracy: 0.9439 - val_loss: 0.4327 - val_accuracy: 0.8765\n",
      "Epoch 274/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1770 - accuracy: 0.9390\n",
      "Epoch 00274: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1757 - accuracy: 0.9394 - val_loss: 0.4086 - val_accuracy: 0.8847\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9457\n",
      "Epoch 00275: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1650 - accuracy: 0.9457 - val_loss: 0.4005 - val_accuracy: 0.8840\n",
      "Epoch 276/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1675 - accuracy: 0.9449\n",
      "Epoch 00276: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1683 - accuracy: 0.9444 - val_loss: 0.4010 - val_accuracy: 0.8854\n",
      "Epoch 277/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1711 - accuracy: 0.9449\n",
      "Epoch 00277: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1712 - accuracy: 0.9444 - val_loss: 0.4497 - val_accuracy: 0.8738\n",
      "Epoch 278/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1658 - accuracy: 0.9436\n",
      "Epoch 00278: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1640 - accuracy: 0.9449 - val_loss: 0.4044 - val_accuracy: 0.8874\n",
      "Epoch 279/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1558 - accuracy: 0.9513\n",
      "Epoch 00279: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1575 - accuracy: 0.9509 - val_loss: 0.3991 - val_accuracy: 0.8834\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9483\n",
      "Epoch 00280: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1579 - accuracy: 0.9483 - val_loss: 0.4217 - val_accuracy: 0.8861\n",
      "Epoch 281/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1712 - accuracy: 0.9438\n",
      "Epoch 00281: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1687 - accuracy: 0.9439 - val_loss: 0.4034 - val_accuracy: 0.8847\n",
      "Epoch 282/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1637 - accuracy: 0.9455\n",
      "Epoch 00282: val_loss did not improve from 0.39693\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1647 - accuracy: 0.9451 - val_loss: 0.4116 - val_accuracy: 0.8834\n",
      "Epoch 283/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1587 - accuracy: 0.9468\n",
      "Epoch 00283: val_loss improved from 0.39693 to 0.39027, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1603 - accuracy: 0.9461 - val_loss: 0.3903 - val_accuracy: 0.8888\n",
      "Epoch 284/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1575 - accuracy: 0.9464\n",
      "Epoch 00284: val_loss did not improve from 0.39027\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1590 - accuracy: 0.9459 - val_loss: 0.3935 - val_accuracy: 0.8881\n",
      "Epoch 285/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1618 - accuracy: 0.9469\n",
      "Epoch 00285: val_loss did not improve from 0.39027\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1622 - accuracy: 0.9464 - val_loss: 0.3925 - val_accuracy: 0.8909\n",
      "Epoch 286/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1683 - accuracy: 0.9431\n",
      "Epoch 00286: val_loss did not improve from 0.39027\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1681 - accuracy: 0.9434 - val_loss: 0.4259 - val_accuracy: 0.8806\n",
      "Epoch 287/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1750 - accuracy: 0.9395\n",
      "Epoch 00287: val_loss did not improve from 0.39027\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1761 - accuracy: 0.9384 - val_loss: 0.4171 - val_accuracy: 0.8786\n",
      "Epoch 288/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1607 - accuracy: 0.9461\n",
      "Epoch 00288: val_loss did not improve from 0.39027\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1616 - accuracy: 0.9457 - val_loss: 0.4354 - val_accuracy: 0.8765\n",
      "Epoch 289/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1559 - accuracy: 0.9477\n",
      "Epoch 00289: val_loss improved from 0.39027 to 0.38660, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1537 - accuracy: 0.9483 - val_loss: 0.3866 - val_accuracy: 0.8888\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9510\n",
      "Epoch 00290: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1503 - accuracy: 0.9510 - val_loss: 0.3948 - val_accuracy: 0.8847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1532 - accuracy: 0.9498\n",
      "Epoch 00291: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1567 - accuracy: 0.9480 - val_loss: 0.4623 - val_accuracy: 0.8670\n",
      "Epoch 292/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1602 - accuracy: 0.9467\n",
      "Epoch 00292: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1616 - accuracy: 0.9459 - val_loss: 0.3867 - val_accuracy: 0.8929\n",
      "Epoch 293/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1624 - accuracy: 0.9444\n",
      "Epoch 00293: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1611 - accuracy: 0.9452 - val_loss: 0.3955 - val_accuracy: 0.8861\n",
      "Epoch 294/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1634 - accuracy: 0.9474\n",
      "Epoch 00294: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1630 - accuracy: 0.9463 - val_loss: 0.4324 - val_accuracy: 0.8772\n",
      "Epoch 295/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1678 - accuracy: 0.9414\n",
      "Epoch 00295: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1659 - accuracy: 0.9427 - val_loss: 0.4027 - val_accuracy: 0.8820\n",
      "Epoch 296/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1552 - accuracy: 0.9485\n",
      "Epoch 00296: val_loss did not improve from 0.38660\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1547 - accuracy: 0.9488 - val_loss: 0.4154 - val_accuracy: 0.8799\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9527\n",
      "Epoch 00297: val_loss improved from 0.38660 to 0.38393, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1481 - accuracy: 0.9527 - val_loss: 0.3839 - val_accuracy: 0.8868\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9498\n",
      "Epoch 00298: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1451 - accuracy: 0.9498 - val_loss: 0.3953 - val_accuracy: 0.8936\n",
      "Epoch 299/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1586 - accuracy: 0.9472\n",
      "Epoch 00299: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1578 - accuracy: 0.9478 - val_loss: 0.4406 - val_accuracy: 0.8752\n",
      "Epoch 300/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1607 - accuracy: 0.9461\n",
      "Epoch 00300: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1602 - accuracy: 0.9457 - val_loss: 0.4073 - val_accuracy: 0.8868\n",
      "Epoch 301/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1504 - accuracy: 0.9501\n",
      "Epoch 00301: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1525 - accuracy: 0.9500 - val_loss: 0.4057 - val_accuracy: 0.8840\n",
      "Epoch 302/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1522 - accuracy: 0.9496\n",
      "Epoch 00302: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1503 - accuracy: 0.9512 - val_loss: 0.4063 - val_accuracy: 0.8936\n",
      "Epoch 303/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.9540\n",
      "Epoch 00303: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1450 - accuracy: 0.9527 - val_loss: 0.4119 - val_accuracy: 0.8731\n",
      "Epoch 304/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1562 - accuracy: 0.9464\n",
      "Epoch 00304: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1542 - accuracy: 0.9473 - val_loss: 0.3925 - val_accuracy: 0.8868\n",
      "Epoch 305/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1535 - accuracy: 0.9481\n",
      "Epoch 00305: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1548 - accuracy: 0.9480 - val_loss: 0.4019 - val_accuracy: 0.8881\n",
      "Epoch 306/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1451 - accuracy: 0.9516\n",
      "Epoch 00306: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1456 - accuracy: 0.9515 - val_loss: 0.3994 - val_accuracy: 0.8847\n",
      "Epoch 307/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1446 - accuracy: 0.9531\n",
      "Epoch 00307: val_loss did not improve from 0.38393\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1458 - accuracy: 0.9531 - val_loss: 0.4112 - val_accuracy: 0.8820\n",
      "Epoch 308/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1379 - accuracy: 0.9563\n",
      "Epoch 00308: val_loss improved from 0.38393 to 0.37998, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1416 - accuracy: 0.9555 - val_loss: 0.3800 - val_accuracy: 0.8895\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9544\n",
      "Epoch 00309: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1409 - accuracy: 0.9544 - val_loss: 0.3914 - val_accuracy: 0.8868\n",
      "Epoch 310/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1400 - accuracy: 0.9544\n",
      "Epoch 00310: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1397 - accuracy: 0.9541 - val_loss: 0.3928 - val_accuracy: 0.8950\n",
      "Epoch 311/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1401 - accuracy: 0.9522\n",
      "Epoch 00311: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1397 - accuracy: 0.9527 - val_loss: 0.4056 - val_accuracy: 0.8868\n",
      "Epoch 312/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1517 - accuracy: 0.9481\n",
      "Epoch 00312: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1503 - accuracy: 0.9476 - val_loss: 0.4329 - val_accuracy: 0.8827\n",
      "Epoch 313/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1428 - accuracy: 0.9516\n",
      "Epoch 00313: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1431 - accuracy: 0.9514 - val_loss: 0.3969 - val_accuracy: 0.8874\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9553\n",
      "Epoch 00314: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1375 - accuracy: 0.9553 - val_loss: 0.4091 - val_accuracy: 0.8915\n",
      "Epoch 315/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1283 - accuracy: 0.9580\n",
      "Epoch 00315: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1340 - accuracy: 0.9560 - val_loss: 0.4233 - val_accuracy: 0.8854\n",
      "Epoch 316/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1390 - accuracy: 0.9515\n",
      "Epoch 00316: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1416 - accuracy: 0.9500 - val_loss: 0.3936 - val_accuracy: 0.8895\n",
      "Epoch 317/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1395 - accuracy: 0.9544\n",
      "Epoch 00317: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1390 - accuracy: 0.9531 - val_loss: 0.4092 - val_accuracy: 0.8902\n",
      "Epoch 318/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1368 - accuracy: 0.9561\n",
      "Epoch 00318: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1358 - accuracy: 0.9563 - val_loss: 0.4130 - val_accuracy: 0.8854\n",
      "Epoch 319/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1424 - accuracy: 0.9524\n",
      "Epoch 00319: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1465 - accuracy: 0.9519 - val_loss: 0.3906 - val_accuracy: 0.8984\n",
      "Epoch 320/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1343 - accuracy: 0.9518\n",
      "Epoch 00320: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1328 - accuracy: 0.9529 - val_loss: 0.3945 - val_accuracy: 0.8936\n",
      "Epoch 321/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1287 - accuracy: 0.9580\n",
      "Epoch 00321: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1309 - accuracy: 0.9570 - val_loss: 0.3873 - val_accuracy: 0.8956\n",
      "Epoch 322/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1353 - accuracy: 0.9574\n",
      "Epoch 00322: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1361 - accuracy: 0.9573 - val_loss: 0.3986 - val_accuracy: 0.8874\n",
      "Epoch 323/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1361 - accuracy: 0.9537\n",
      "Epoch 00323: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1379 - accuracy: 0.9536 - val_loss: 0.4073 - val_accuracy: 0.8827\n",
      "Epoch 324/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9606\n",
      "Epoch 00324: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1310 - accuracy: 0.9589 - val_loss: 0.3839 - val_accuracy: 0.8881\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9539\n",
      "Epoch 00325: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1356 - accuracy: 0.9539 - val_loss: 0.4206 - val_accuracy: 0.8813\n",
      "Epoch 326/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1303 - accuracy: 0.9567\n",
      "Epoch 00326: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1326 - accuracy: 0.9555 - val_loss: 0.4178 - val_accuracy: 0.8881\n",
      "Epoch 327/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1335 - accuracy: 0.9552\n",
      "Epoch 00327: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1347 - accuracy: 0.9544 - val_loss: 0.4169 - val_accuracy: 0.8861\n",
      "Epoch 328/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9619\n",
      "Epoch 00328: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1251 - accuracy: 0.9616 - val_loss: 0.3949 - val_accuracy: 0.8902\n",
      "Epoch 329/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1232 - accuracy: 0.9615\n",
      "Epoch 00329: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1218 - accuracy: 0.9618 - val_loss: 0.3907 - val_accuracy: 0.8915\n",
      "Epoch 330/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1253 - accuracy: 0.9587\n",
      "Epoch 00330: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1239 - accuracy: 0.9592 - val_loss: 0.4291 - val_accuracy: 0.8820\n",
      "Epoch 331/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1349 - accuracy: 0.9545\n",
      "Epoch 00331: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1378 - accuracy: 0.9526 - val_loss: 0.4033 - val_accuracy: 0.8881\n",
      "Epoch 332/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1401 - accuracy: 0.9520\n",
      "Epoch 00332: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1400 - accuracy: 0.9521 - val_loss: 0.4063 - val_accuracy: 0.8936\n",
      "Epoch 333/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1229 - accuracy: 0.9608\n",
      "Epoch 00333: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1249 - accuracy: 0.9591 - val_loss: 0.4136 - val_accuracy: 0.8881\n",
      "Epoch 334/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1319 - accuracy: 0.9529\n",
      "Epoch 00334: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1319 - accuracy: 0.9529 - val_loss: 0.4100 - val_accuracy: 0.8834\n",
      "Epoch 335/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1266 - accuracy: 0.9570\n",
      "Epoch 00335: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1248 - accuracy: 0.9577 - val_loss: 0.4008 - val_accuracy: 0.8868\n",
      "Epoch 336/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1241 - accuracy: 0.9619\n",
      "Epoch 00336: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1231 - accuracy: 0.9620 - val_loss: 0.4050 - val_accuracy: 0.8943\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9594\n",
      "Epoch 00337: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1212 - accuracy: 0.9594 - val_loss: 0.4069 - val_accuracy: 0.8874\n",
      "Epoch 338/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1320 - accuracy: 0.9526\n",
      "Epoch 00338: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1300 - accuracy: 0.9534 - val_loss: 0.4127 - val_accuracy: 0.8895\n",
      "Epoch 339/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1238 - accuracy: 0.9581\n",
      "Epoch 00339: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1257 - accuracy: 0.9580 - val_loss: 0.3953 - val_accuracy: 0.8936\n",
      "Epoch 340/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1230 - accuracy: 0.9606\n",
      "Epoch 00340: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1265 - accuracy: 0.9592 - val_loss: 0.4216 - val_accuracy: 0.8820\n",
      "Epoch 341/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1322 - accuracy: 0.9565\n",
      "Epoch 00341: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1313 - accuracy: 0.9570 - val_loss: 0.4040 - val_accuracy: 0.8888\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9573\n",
      "Epoch 00342: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1257 - accuracy: 0.9573 - val_loss: 0.3877 - val_accuracy: 0.8922\n",
      "Epoch 343/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1287 - accuracy: 0.9568\n",
      "Epoch 00343: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1278 - accuracy: 0.9567 - val_loss: 0.4110 - val_accuracy: 0.8874\n",
      "Epoch 344/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1189 - accuracy: 0.9624\n",
      "Epoch 00344: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1173 - accuracy: 0.9635 - val_loss: 0.4067 - val_accuracy: 0.8868\n",
      "Epoch 345/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1154 - accuracy: 0.9635\n",
      "Epoch 00345: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1150 - accuracy: 0.9635 - val_loss: 0.4421 - val_accuracy: 0.8840\n",
      "Epoch 346/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1163 - accuracy: 0.9618\n",
      "Epoch 00346: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1166 - accuracy: 0.9620 - val_loss: 0.4051 - val_accuracy: 0.8915\n",
      "Epoch 347/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1219 - accuracy: 0.9581\n",
      "Epoch 00347: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1198 - accuracy: 0.9589 - val_loss: 0.4053 - val_accuracy: 0.8922\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9594\n",
      "Epoch 00348: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1181 - accuracy: 0.9594 - val_loss: 0.3867 - val_accuracy: 0.8997\n",
      "Epoch 349/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1169 - accuracy: 0.9628\n",
      "Epoch 00349: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1166 - accuracy: 0.9633 - val_loss: 0.4054 - val_accuracy: 0.8888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1195 - accuracy: 0.9587\n",
      "Epoch 00350: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1203 - accuracy: 0.9587 - val_loss: 0.4017 - val_accuracy: 0.8874\n",
      "Epoch 351/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1158 - accuracy: 0.9613\n",
      "Epoch 00351: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1144 - accuracy: 0.9620 - val_loss: 0.3933 - val_accuracy: 0.8943\n",
      "Epoch 352/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1107 - accuracy: 0.9641\n",
      "Epoch 00352: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1129 - accuracy: 0.9638 - val_loss: 0.4036 - val_accuracy: 0.8922\n",
      "Epoch 353/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1294 - accuracy: 0.9563\n",
      "Epoch 00353: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1303 - accuracy: 0.9560 - val_loss: 0.4327 - val_accuracy: 0.8909\n",
      "Epoch 354/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1214 - accuracy: 0.9603\n",
      "Epoch 00354: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1192 - accuracy: 0.9611 - val_loss: 0.3989 - val_accuracy: 0.8997\n",
      "Epoch 355/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1108 - accuracy: 0.9637\n",
      "Epoch 00355: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1094 - accuracy: 0.9645 - val_loss: 0.4172 - val_accuracy: 0.8943\n",
      "Epoch 356/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1270 - accuracy: 0.9563\n",
      "Epoch 00356: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1283 - accuracy: 0.9562 - val_loss: 0.3935 - val_accuracy: 0.8956\n",
      "Epoch 357/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9643\n",
      "Epoch 00357: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1114 - accuracy: 0.9643 - val_loss: 0.4067 - val_accuracy: 0.8881\n",
      "Epoch 358/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1133 - accuracy: 0.9626\n",
      "Epoch 00358: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1124 - accuracy: 0.9625 - val_loss: 0.3807 - val_accuracy: 0.8936\n",
      "Epoch 359/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9638\n",
      "Epoch 00359: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1122 - accuracy: 0.9635 - val_loss: 0.4001 - val_accuracy: 0.9004\n",
      "Epoch 360/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9622\n",
      "Epoch 00360: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1103 - accuracy: 0.9631 - val_loss: 0.4076 - val_accuracy: 0.8943\n",
      "Epoch 361/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1091 - accuracy: 0.9665\n",
      "Epoch 00361: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1095 - accuracy: 0.9667 - val_loss: 0.4062 - val_accuracy: 0.8956\n",
      "Epoch 362/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9650\n",
      "Epoch 00362: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1088 - accuracy: 0.9647 - val_loss: 0.4056 - val_accuracy: 0.8895\n",
      "Epoch 363/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1117 - accuracy: 0.9630\n",
      "Epoch 00363: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1136 - accuracy: 0.9625 - val_loss: 0.4020 - val_accuracy: 0.8915\n",
      "Epoch 364/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9654\n",
      "Epoch 00364: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1071 - accuracy: 0.9655 - val_loss: 0.4137 - val_accuracy: 0.8929\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9671\n",
      "Epoch 00365: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1050 - accuracy: 0.9671 - val_loss: 0.4152 - val_accuracy: 0.8881\n",
      "Epoch 366/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1165 - accuracy: 0.9624\n",
      "Epoch 00366: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1172 - accuracy: 0.9623 - val_loss: 0.4237 - val_accuracy: 0.8840\n",
      "Epoch 367/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1169 - accuracy: 0.9591\n",
      "Epoch 00367: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1170 - accuracy: 0.9587 - val_loss: 0.3928 - val_accuracy: 0.8936\n",
      "Epoch 368/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1102 - accuracy: 0.9624\n",
      "Epoch 00368: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1125 - accuracy: 0.9614 - val_loss: 0.3999 - val_accuracy: 0.8963\n",
      "Epoch 369/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1104 - accuracy: 0.9619\n",
      "Epoch 00369: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1095 - accuracy: 0.9625 - val_loss: 0.4083 - val_accuracy: 0.8922\n",
      "Epoch 370/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1014 - accuracy: 0.9678\n",
      "Epoch 00370: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1017 - accuracy: 0.9681 - val_loss: 0.3852 - val_accuracy: 0.8902\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9696\n",
      "Epoch 00371: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0960 - accuracy: 0.9696 - val_loss: 0.3809 - val_accuracy: 0.9031\n",
      "Epoch 372/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0974 - accuracy: 0.9712\n",
      "Epoch 00372: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1006 - accuracy: 0.9684 - val_loss: 0.4247 - val_accuracy: 0.8943\n",
      "Epoch 373/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1041 - accuracy: 0.9654\n",
      "Epoch 00373: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1026 - accuracy: 0.9655 - val_loss: 0.4093 - val_accuracy: 0.8997\n",
      "Epoch 374/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1057 - accuracy: 0.9667\n",
      "Epoch 00374: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1058 - accuracy: 0.9669 - val_loss: 0.3970 - val_accuracy: 0.8936\n",
      "Epoch 375/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0972 - accuracy: 0.9676\n",
      "Epoch 00375: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1004 - accuracy: 0.9657 - val_loss: 0.4093 - val_accuracy: 0.8943\n",
      "Epoch 376/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1123 - accuracy: 0.9622\n",
      "Epoch 00376: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1117 - accuracy: 0.9626 - val_loss: 0.4172 - val_accuracy: 0.8868\n",
      "Epoch 377/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9567\n",
      "Epoch 00377: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1242 - accuracy: 0.9572 - val_loss: 0.4949 - val_accuracy: 0.8779\n",
      "Epoch 378/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9552\n",
      "Epoch 00378: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1227 - accuracy: 0.9562 - val_loss: 0.4098 - val_accuracy: 0.8943\n",
      "Epoch 379/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1127 - accuracy: 0.9600\n",
      "Epoch 00379: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1126 - accuracy: 0.9604 - val_loss: 0.4078 - val_accuracy: 0.8922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1102 - accuracy: 0.9628\n",
      "Epoch 00380: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1098 - accuracy: 0.9633 - val_loss: 0.4137 - val_accuracy: 0.8888\n",
      "Epoch 381/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1033 - accuracy: 0.9654\n",
      "Epoch 00381: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1024 - accuracy: 0.9659 - val_loss: 0.4100 - val_accuracy: 0.8909\n",
      "Epoch 382/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0983 - accuracy: 0.9680\n",
      "Epoch 00382: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0981 - accuracy: 0.9684 - val_loss: 0.4079 - val_accuracy: 0.8929\n",
      "Epoch 383/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0976 - accuracy: 0.9674\n",
      "Epoch 00383: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0958 - accuracy: 0.9683 - val_loss: 0.4126 - val_accuracy: 0.8827\n",
      "Epoch 384/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1011 - accuracy: 0.9669\n",
      "Epoch 00384: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1001 - accuracy: 0.9679 - val_loss: 0.4219 - val_accuracy: 0.8984\n",
      "Epoch 385/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0950 - accuracy: 0.9706\n",
      "Epoch 00385: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0939 - accuracy: 0.9713 - val_loss: 0.4009 - val_accuracy: 0.8970\n",
      "Epoch 386/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0912 - accuracy: 0.9727\n",
      "Epoch 00386: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0926 - accuracy: 0.9718 - val_loss: 0.4011 - val_accuracy: 0.8984\n",
      "Epoch 387/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0931 - accuracy: 0.9699\n",
      "Epoch 00387: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0950 - accuracy: 0.9696 - val_loss: 0.4028 - val_accuracy: 0.8956\n",
      "Epoch 388/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0968 - accuracy: 0.9707\n",
      "Epoch 00388: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0983 - accuracy: 0.9691 - val_loss: 0.3987 - val_accuracy: 0.8915\n",
      "Epoch 389/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1145 - accuracy: 0.9578\n",
      "Epoch 00389: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1135 - accuracy: 0.9580 - val_loss: 0.3910 - val_accuracy: 0.8950\n",
      "Epoch 390/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0911 - accuracy: 0.9693\n",
      "Epoch 00390: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0927 - accuracy: 0.9693 - val_loss: 0.4221 - val_accuracy: 0.8915\n",
      "Epoch 391/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0956 - accuracy: 0.9693\n",
      "Epoch 00391: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0965 - accuracy: 0.9689 - val_loss: 0.4270 - val_accuracy: 0.8874\n",
      "Epoch 392/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0914 - accuracy: 0.9710\n",
      "Epoch 00392: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0931 - accuracy: 0.9708 - val_loss: 0.3981 - val_accuracy: 0.8956\n",
      "Epoch 393/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0898 - accuracy: 0.9721\n",
      "Epoch 00393: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0877 - accuracy: 0.9734 - val_loss: 0.3844 - val_accuracy: 0.8984\n",
      "Epoch 394/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0866 - accuracy: 0.9749\n",
      "Epoch 00394: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0873 - accuracy: 0.9741 - val_loss: 0.4020 - val_accuracy: 0.8915\n",
      "Epoch 395/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0929 - accuracy: 0.9693\n",
      "Epoch 00395: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0938 - accuracy: 0.9689 - val_loss: 0.3933 - val_accuracy: 0.8990\n",
      "Epoch 396/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1049 - accuracy: 0.9628\n",
      "Epoch 00396: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1063 - accuracy: 0.9625 - val_loss: 0.3896 - val_accuracy: 0.8997\n",
      "Epoch 397/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0896 - accuracy: 0.9719\n",
      "Epoch 00397: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0908 - accuracy: 0.9713 - val_loss: 0.4494 - val_accuracy: 0.8929\n",
      "Epoch 398/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1008 - accuracy: 0.9663\n",
      "Epoch 00398: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1010 - accuracy: 0.9659 - val_loss: 0.4194 - val_accuracy: 0.8902\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9662\n",
      "Epoch 00399: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0999 - accuracy: 0.9662 - val_loss: 0.3940 - val_accuracy: 0.8984\n",
      "Epoch 400/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0972 - accuracy: 0.9665\n",
      "Epoch 00400: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1001 - accuracy: 0.9650 - val_loss: 0.4486 - val_accuracy: 0.8827\n",
      "Epoch 401/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0943 - accuracy: 0.9680\n",
      "Epoch 00401: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0936 - accuracy: 0.9681 - val_loss: 0.4151 - val_accuracy: 0.8895\n",
      "Epoch 402/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0910 - accuracy: 0.9688\n",
      "Epoch 00402: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0927 - accuracy: 0.9676 - val_loss: 0.4549 - val_accuracy: 0.8820\n",
      "Epoch 403/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0957 - accuracy: 0.9660\n",
      "Epoch 00403: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0965 - accuracy: 0.9657 - val_loss: 0.4199 - val_accuracy: 0.8956\n",
      "Epoch 404/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0949 - accuracy: 0.9680\n",
      "Epoch 00404: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0945 - accuracy: 0.9674 - val_loss: 0.4040 - val_accuracy: 0.8943\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9732\n",
      "Epoch 00405: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0843 - accuracy: 0.9732 - val_loss: 0.3924 - val_accuracy: 0.8943\n",
      "Epoch 406/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0781 - accuracy: 0.9766\n",
      "Epoch 00406: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0811 - accuracy: 0.9749 - val_loss: 0.4283 - val_accuracy: 0.8909\n",
      "Epoch 407/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0902 - accuracy: 0.9701\n",
      "Epoch 00407: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0895 - accuracy: 0.9703 - val_loss: 0.3877 - val_accuracy: 0.8950\n",
      "Epoch 408/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0801 - accuracy: 0.9749\n",
      "Epoch 00408: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0839 - accuracy: 0.9729 - val_loss: 0.4028 - val_accuracy: 0.8943\n",
      "Epoch 409/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0845 - accuracy: 0.9749\n",
      "Epoch 00409: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0860 - accuracy: 0.9747 - val_loss: 0.4090 - val_accuracy: 0.9038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 410/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0874 - accuracy: 0.9714\n",
      "Epoch 00410: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0888 - accuracy: 0.9710 - val_loss: 0.4127 - val_accuracy: 0.8929\n",
      "Epoch 411/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0865 - accuracy: 0.9714\n",
      "Epoch 00411: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0866 - accuracy: 0.9710 - val_loss: 0.4201 - val_accuracy: 0.8943\n",
      "Epoch 412/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0906 - accuracy: 0.9710\n",
      "Epoch 00412: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0911 - accuracy: 0.9707 - val_loss: 0.4197 - val_accuracy: 0.8956\n",
      "Epoch 413/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0848 - accuracy: 0.9712\n",
      "Epoch 00413: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0847 - accuracy: 0.9713 - val_loss: 0.3895 - val_accuracy: 0.8997\n",
      "Epoch 414/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0797 - accuracy: 0.9730\n",
      "Epoch 00414: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0798 - accuracy: 0.9734 - val_loss: 0.4230 - val_accuracy: 0.8909\n",
      "Epoch 415/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0875 - accuracy: 0.9730\n",
      "Epoch 00415: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0906 - accuracy: 0.9718 - val_loss: 0.4123 - val_accuracy: 0.8929\n",
      "Epoch 416/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9754\n",
      "Epoch 00416: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.4382 - val_accuracy: 0.8936\n",
      "Epoch 417/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0833 - accuracy: 0.9736\n",
      "Epoch 00417: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0818 - accuracy: 0.9744 - val_loss: 0.4372 - val_accuracy: 0.8909\n",
      "Epoch 418/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0876 - accuracy: 0.9708\n",
      "Epoch 00418: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0874 - accuracy: 0.9708 - val_loss: 0.4543 - val_accuracy: 0.8915\n",
      "Epoch 419/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0835 - accuracy: 0.9723\n",
      "Epoch 00419: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0841 - accuracy: 0.9715 - val_loss: 0.4433 - val_accuracy: 0.8922\n",
      "Epoch 420/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0866 - accuracy: 0.9715\n",
      "Epoch 00420: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0856 - accuracy: 0.9722 - val_loss: 0.4358 - val_accuracy: 0.8895\n",
      "Epoch 421/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0760 - accuracy: 0.9766\n",
      "Epoch 00421: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0776 - accuracy: 0.9763 - val_loss: 0.4144 - val_accuracy: 0.8970\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9787\n",
      "Epoch 00422: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0724 - accuracy: 0.9787 - val_loss: 0.4071 - val_accuracy: 0.8922\n",
      "Epoch 423/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0798 - accuracy: 0.9738\n",
      "Epoch 00423: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0843 - accuracy: 0.9712 - val_loss: 0.4183 - val_accuracy: 0.8943\n",
      "Epoch 424/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0834 - accuracy: 0.9719\n",
      "Epoch 00424: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0852 - accuracy: 0.9715 - val_loss: 0.4287 - val_accuracy: 0.8984\n",
      "Epoch 425/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0833 - accuracy: 0.9725\n",
      "Epoch 00425: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0850 - accuracy: 0.9718 - val_loss: 0.4256 - val_accuracy: 0.8936\n",
      "Epoch 426/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0853 - accuracy: 0.9701\n",
      "Epoch 00426: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0838 - accuracy: 0.9707 - val_loss: 0.4111 - val_accuracy: 0.8963\n",
      "Epoch 427/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0768 - accuracy: 0.9771\n",
      "Epoch 00427: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0760 - accuracy: 0.9771 - val_loss: 0.4129 - val_accuracy: 0.8922\n",
      "Epoch 428/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0776 - accuracy: 0.9754\n",
      "Epoch 00428: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0772 - accuracy: 0.9756 - val_loss: 0.4255 - val_accuracy: 0.8943\n",
      "Epoch 429/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0835 - accuracy: 0.9741\n",
      "Epoch 00429: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0834 - accuracy: 0.9737 - val_loss: 0.4475 - val_accuracy: 0.8881\n",
      "Epoch 430/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0909 - accuracy: 0.9680\n",
      "Epoch 00430: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0898 - accuracy: 0.9688 - val_loss: 0.3976 - val_accuracy: 0.8984\n",
      "Epoch 431/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0739 - accuracy: 0.9771\n",
      "Epoch 00431: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0730 - accuracy: 0.9773 - val_loss: 0.4122 - val_accuracy: 0.9025\n",
      "Epoch 432/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0722 - accuracy: 0.9784\n",
      "Epoch 00432: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0736 - accuracy: 0.9776 - val_loss: 0.4088 - val_accuracy: 0.9038\n",
      "Epoch 433/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0725 - accuracy: 0.9767\n",
      "Epoch 00433: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0754 - accuracy: 0.9761 - val_loss: 0.4451 - val_accuracy: 0.8840\n",
      "Epoch 434/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0778 - accuracy: 0.9754\n",
      "Epoch 00434: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0773 - accuracy: 0.9758 - val_loss: 0.4155 - val_accuracy: 0.8929\n",
      "Epoch 435/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0726 - accuracy: 0.9767\n",
      "Epoch 00435: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0725 - accuracy: 0.9771 - val_loss: 0.4183 - val_accuracy: 0.9004\n",
      "Epoch 436/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0674 - accuracy: 0.9790\n",
      "Epoch 00436: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0684 - accuracy: 0.9787 - val_loss: 0.4160 - val_accuracy: 0.8990\n",
      "Epoch 437/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0784 - accuracy: 0.9753\n",
      "Epoch 00437: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0782 - accuracy: 0.9749 - val_loss: 0.4428 - val_accuracy: 0.8895\n",
      "Epoch 438/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0782 - accuracy: 0.9725\n",
      "Epoch 00438: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0778 - accuracy: 0.9727 - val_loss: 0.4053 - val_accuracy: 0.8956\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9833\n",
      "Epoch 00439: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.4260 - val_accuracy: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0646 - accuracy: 0.9807\n",
      "Epoch 00440: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0641 - accuracy: 0.9811 - val_loss: 0.4257 - val_accuracy: 0.8915\n",
      "Epoch 441/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0732 - accuracy: 0.9754\n",
      "Epoch 00441: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0726 - accuracy: 0.9759 - val_loss: 0.4493 - val_accuracy: 0.8943\n",
      "Epoch 442/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0779 - accuracy: 0.9747\n",
      "Epoch 00442: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0774 - accuracy: 0.9753 - val_loss: 0.4131 - val_accuracy: 0.8990\n",
      "Epoch 443/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0824 - accuracy: 0.9701\n",
      "Epoch 00443: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0817 - accuracy: 0.9710 - val_loss: 0.4083 - val_accuracy: 0.9011\n",
      "Epoch 444/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.0683 - accuracy: 0.9788\n",
      "Epoch 00444: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0689 - accuracy: 0.9776 - val_loss: 0.4240 - val_accuracy: 0.8943\n",
      "Epoch 445/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0805 - accuracy: 0.9737\n",
      "Epoch 00445: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0809 - accuracy: 0.9736 - val_loss: 0.4158 - val_accuracy: 0.8895\n",
      "Epoch 446/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0704 - accuracy: 0.9790\n",
      "Epoch 00446: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0713 - accuracy: 0.9780 - val_loss: 0.4432 - val_accuracy: 0.8956\n",
      "Epoch 447/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0731 - accuracy: 0.9764\n",
      "Epoch 00447: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0721 - accuracy: 0.9766 - val_loss: 0.4278 - val_accuracy: 0.8977\n",
      "Epoch 448/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0621 - accuracy: 0.9833\n",
      "Epoch 00448: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0637 - accuracy: 0.9821 - val_loss: 0.4266 - val_accuracy: 0.9004\n",
      "Epoch 449/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0664 - accuracy: 0.9797\n",
      "Epoch 00449: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0676 - accuracy: 0.9790 - val_loss: 0.4529 - val_accuracy: 0.8922\n",
      "Epoch 450/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.0711 - accuracy: 0.9778\n",
      "Epoch 00450: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0710 - accuracy: 0.9778 - val_loss: 0.4369 - val_accuracy: 0.9025\n",
      "Epoch 451/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0680 - accuracy: 0.9785\n",
      "Epoch 00451: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0684 - accuracy: 0.9785 - val_loss: 0.4512 - val_accuracy: 0.8909\n",
      "Epoch 452/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0682 - accuracy: 0.9788\n",
      "Epoch 00452: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0678 - accuracy: 0.9787 - val_loss: 0.4186 - val_accuracy: 0.8956\n",
      "Epoch 453/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0616 - accuracy: 0.9821\n",
      "Epoch 00453: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0612 - accuracy: 0.9821 - val_loss: 0.4049 - val_accuracy: 0.9025\n",
      "Epoch 454/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0606 - accuracy: 0.9810\n",
      "Epoch 00454: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0648 - accuracy: 0.9790 - val_loss: 0.4234 - val_accuracy: 0.9025\n",
      "Epoch 455/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0747 - accuracy: 0.9756\n",
      "Epoch 00455: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0750 - accuracy: 0.9754 - val_loss: 0.4496 - val_accuracy: 0.8888\n",
      "Epoch 456/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.0717 - accuracy: 0.9774\n",
      "Epoch 00456: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0705 - accuracy: 0.9776 - val_loss: 0.4457 - val_accuracy: 0.8977\n",
      "Epoch 457/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0656 - accuracy: 0.9795\n",
      "Epoch 00457: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0666 - accuracy: 0.9794 - val_loss: 0.4365 - val_accuracy: 0.8943\n",
      "Epoch 458/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0626 - accuracy: 0.9807\n",
      "Epoch 00458: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0659 - accuracy: 0.9795 - val_loss: 0.4138 - val_accuracy: 0.8956\n",
      "Epoch 459/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0717 - accuracy: 0.9779\n",
      "Epoch 00459: val_loss did not improve from 0.37998\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0715 - accuracy: 0.9780 - val_loss: 0.4320 - val_accuracy: 0.8909\n",
      "Epoch 00459: early stopping\n",
      "Training for model  3  completed in time:  0:02:57.480146 seconds\n",
      "Training for model  4  has started.\n",
      "Epoch 1/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 2.7292 - accuracy: 0.2718\n",
      "Epoch 00001: val_loss improved from inf to 1.56504, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 2.6818 - accuracy: 0.2769 - val_loss: 1.5650 - val_accuracy: 0.4188\n",
      "Epoch 2/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3556 - accuracy: 0.5249\n",
      "Epoch 00002: val_loss improved from 1.56504 to 1.19964, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3514 - accuracy: 0.5247 - val_loss: 1.1996 - val_accuracy: 0.5887\n",
      "Epoch 3/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0967 - accuracy: 0.6136\n",
      "Epoch 00003: val_loss improved from 1.19964 to 1.03008, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.0902 - accuracy: 0.6153 - val_loss: 1.0301 - val_accuracy: 0.6623\n",
      "Epoch 4/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9406 - accuracy: 0.6818\n",
      "Epoch 00004: val_loss improved from 1.03008 to 0.89324, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9417 - accuracy: 0.6808 - val_loss: 0.8932 - val_accuracy: 0.6883\n",
      "Epoch 5/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8129 - accuracy: 0.7217\n",
      "Epoch 00005: val_loss improved from 0.89324 to 0.77891, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.8144 - accuracy: 0.7202 - val_loss: 0.7789 - val_accuracy: 0.7299\n",
      "Epoch 6/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7036 - accuracy: 0.7615\n",
      "Epoch 00006: val_loss improved from 0.77891 to 0.68863, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7074 - accuracy: 0.7610 - val_loss: 0.6886 - val_accuracy: 0.7640\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.7925\n",
      "Epoch 00007: val_loss improved from 0.68863 to 0.63920, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6139 - accuracy: 0.7925 - val_loss: 0.6392 - val_accuracy: 0.7715\n",
      "Epoch 8/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5627 - accuracy: 0.8155\n",
      "Epoch 00008: val_loss improved from 0.63920 to 0.58579, saving model to models/saved_models/best_models_DP_1.0.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5640 - accuracy: 0.8157 - val_loss: 0.5858 - val_accuracy: 0.8015\n",
      "Epoch 9/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5155 - accuracy: 0.8299\n",
      "Epoch 00009: val_loss improved from 0.58579 to 0.52907, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5158 - accuracy: 0.8292 - val_loss: 0.5291 - val_accuracy: 0.8240\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4546 - accuracy: 0.8491\n",
      "Epoch 00010: val_loss improved from 0.52907 to 0.51227, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4515 - accuracy: 0.8505 - val_loss: 0.5123 - val_accuracy: 0.8363\n",
      "Epoch 11/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4264 - accuracy: 0.8576\n",
      "Epoch 00011: val_loss did not improve from 0.51227\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4294 - accuracy: 0.8572 - val_loss: 0.5510 - val_accuracy: 0.8158\n",
      "Epoch 12/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4092 - accuracy: 0.8631\n",
      "Epoch 00012: val_loss improved from 0.51227 to 0.46109, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4088 - accuracy: 0.8632 - val_loss: 0.4611 - val_accuracy: 0.8486\n",
      "Epoch 13/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3670 - accuracy: 0.8765\n",
      "Epoch 00013: val_loss improved from 0.46109 to 0.42970, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3692 - accuracy: 0.8758 - val_loss: 0.4297 - val_accuracy: 0.8602\n",
      "Epoch 14/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3401 - accuracy: 0.8871\n",
      "Epoch 00014: val_loss improved from 0.42970 to 0.42108, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3438 - accuracy: 0.8859 - val_loss: 0.4211 - val_accuracy: 0.8568\n",
      "Epoch 15/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3114 - accuracy: 0.8953\n",
      "Epoch 00015: val_loss improved from 0.42108 to 0.38556, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3139 - accuracy: 0.8944 - val_loss: 0.3856 - val_accuracy: 0.8759\n",
      "Epoch 16/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2946 - accuracy: 0.9023\n",
      "Epoch 00016: val_loss did not improve from 0.38556\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2953 - accuracy: 0.9021 - val_loss: 0.4358 - val_accuracy: 0.8602\n",
      "Epoch 17/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3146 - accuracy: 0.8910\n",
      "Epoch 00017: val_loss improved from 0.38556 to 0.38154, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3093 - accuracy: 0.8929 - val_loss: 0.3815 - val_accuracy: 0.8827\n",
      "Epoch 18/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2613 - accuracy: 0.9122\n",
      "Epoch 00018: val_loss did not improve from 0.38154\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2616 - accuracy: 0.9116 - val_loss: 0.3857 - val_accuracy: 0.8827\n",
      "Epoch 19/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2427 - accuracy: 0.9222\n",
      "Epoch 00019: val_loss improved from 0.38154 to 0.36707, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2449 - accuracy: 0.9215 - val_loss: 0.3671 - val_accuracy: 0.8881\n",
      "Epoch 20/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2242 - accuracy: 0.9276\n",
      "Epoch 00020: val_loss improved from 0.36707 to 0.34989, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2253 - accuracy: 0.9270 - val_loss: 0.3499 - val_accuracy: 0.8868\n",
      "Epoch 21/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2228 - accuracy: 0.9265\n",
      "Epoch 00021: val_loss did not improve from 0.34989\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2222 - accuracy: 0.9271 - val_loss: 0.3708 - val_accuracy: 0.8902\n",
      "Epoch 22/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2204 - accuracy: 0.9288\n",
      "Epoch 00022: val_loss did not improve from 0.34989\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2180 - accuracy: 0.9292 - val_loss: 0.3676 - val_accuracy: 0.8793\n",
      "Epoch 23/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2061 - accuracy: 0.9289\n",
      "Epoch 00023: val_loss improved from 0.34989 to 0.33978, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2012 - accuracy: 0.9307 - val_loss: 0.3398 - val_accuracy: 0.8895\n",
      "Epoch 24/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1763 - accuracy: 0.9423\n",
      "Epoch 00024: val_loss improved from 0.33978 to 0.32809, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1742 - accuracy: 0.9442 - val_loss: 0.3281 - val_accuracy: 0.9025\n",
      "Epoch 25/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1681 - accuracy: 0.9457\n",
      "Epoch 00025: val_loss did not improve from 0.32809\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1689 - accuracy: 0.9451 - val_loss: 0.3427 - val_accuracy: 0.8874\n",
      "Epoch 26/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1677 - accuracy: 0.9455\n",
      "Epoch 00026: val_loss improved from 0.32809 to 0.32489, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1702 - accuracy: 0.9451 - val_loss: 0.3249 - val_accuracy: 0.8970\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9398\n",
      "Epoch 00027: val_loss did not improve from 0.32489\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1708 - accuracy: 0.9398 - val_loss: 0.3301 - val_accuracy: 0.8915\n",
      "Epoch 28/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1653 - accuracy: 0.9464\n",
      "Epoch 00028: val_loss did not improve from 0.32489\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1653 - accuracy: 0.9452 - val_loss: 0.3284 - val_accuracy: 0.8936\n",
      "Epoch 29/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1558 - accuracy: 0.9472\n",
      "Epoch 00029: val_loss improved from 0.32489 to 0.32398, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1577 - accuracy: 0.9468 - val_loss: 0.3240 - val_accuracy: 0.8997\n",
      "Epoch 30/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.9547\n",
      "Epoch 00030: val_loss improved from 0.32398 to 0.30406, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1374 - accuracy: 0.9544 - val_loss: 0.3041 - val_accuracy: 0.9059\n",
      "Epoch 31/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1188 - accuracy: 0.9637\n",
      "Epoch 00031: val_loss did not improve from 0.30406\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1187 - accuracy: 0.9638 - val_loss: 0.3176 - val_accuracy: 0.8990\n",
      "Epoch 32/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9595\n",
      "Epoch 00032: val_loss did not improve from 0.30406\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1250 - accuracy: 0.9594 - val_loss: 0.3522 - val_accuracy: 0.8950\n",
      "Epoch 33/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9581\n",
      "Epoch 00033: val_loss improved from 0.30406 to 0.29595, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1278 - accuracy: 0.9562 - val_loss: 0.2959 - val_accuracy: 0.9065\n",
      "Epoch 34/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1116 - accuracy: 0.9652\n",
      "Epoch 00034: val_loss did not improve from 0.29595\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1135 - accuracy: 0.9635 - val_loss: 0.3042 - val_accuracy: 0.9072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0966 - accuracy: 0.9738\n",
      "Epoch 00035: val_loss did not improve from 0.29595\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0975 - accuracy: 0.9734 - val_loss: 0.3021 - val_accuracy: 0.9168\n",
      "Epoch 36/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1020 - accuracy: 0.9661\n",
      "Epoch 00036: val_loss did not improve from 0.29595\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1028 - accuracy: 0.9662 - val_loss: 0.3077 - val_accuracy: 0.9154\n",
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1001 - accuracy: 0.9691\n",
      "Epoch 00037: val_loss did not improve from 0.29595\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1006 - accuracy: 0.9684 - val_loss: 0.3208 - val_accuracy: 0.9059\n",
      "Epoch 38/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0954 - accuracy: 0.9699\n",
      "Epoch 00038: val_loss improved from 0.29595 to 0.28815, saving model to models/saved_models/best_models_DP_1.0.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 0.2881 - val_accuracy: 0.9100\n",
      "Epoch 39/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0855 - accuracy: 0.9727\n",
      "Epoch 00039: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0846 - accuracy: 0.9729 - val_loss: 0.3119 - val_accuracy: 0.9175\n",
      "Epoch 40/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0817 - accuracy: 0.9743\n",
      "Epoch 00040: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0822 - accuracy: 0.9736 - val_loss: 0.2957 - val_accuracy: 0.9113\n",
      "Epoch 41/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0882 - accuracy: 0.9721\n",
      "Epoch 00041: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0893 - accuracy: 0.9718 - val_loss: 0.3143 - val_accuracy: 0.9093\n",
      "Epoch 42/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0859 - accuracy: 0.9730\n",
      "Epoch 00042: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0853 - accuracy: 0.9729 - val_loss: 0.2922 - val_accuracy: 0.9127\n",
      "Epoch 43/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0754 - accuracy: 0.9783\n",
      "Epoch 00043: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0760 - accuracy: 0.9776 - val_loss: 0.2895 - val_accuracy: 0.9154\n",
      "Epoch 44/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0733 - accuracy: 0.9771\n",
      "Epoch 00044: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.3171 - val_accuracy: 0.9127\n",
      "Epoch 45/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0644 - accuracy: 0.9799\n",
      "Epoch 00045: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0649 - accuracy: 0.9797 - val_loss: 0.3143 - val_accuracy: 0.9106\n",
      "Epoch 46/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0588 - accuracy: 0.9833\n",
      "Epoch 00046: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0596 - accuracy: 0.9828 - val_loss: 0.2941 - val_accuracy: 0.9216\n",
      "Epoch 47/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0562 - accuracy: 0.9833\n",
      "Epoch 00047: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.2959 - val_accuracy: 0.9202\n",
      "Epoch 48/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0582 - accuracy: 0.9840\n",
      "Epoch 00048: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0574 - accuracy: 0.9841 - val_loss: 0.3259 - val_accuracy: 0.9093\n",
      "Epoch 49/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0529 - accuracy: 0.9864\n",
      "Epoch 00049: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0528 - accuracy: 0.9858 - val_loss: 0.2974 - val_accuracy: 0.9229\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0477 - accuracy: 0.9866\n",
      "Epoch 00050: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0467 - accuracy: 0.9874 - val_loss: 0.3083 - val_accuracy: 0.9209\n",
      "Epoch 51/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0467 - accuracy: 0.9860\n",
      "Epoch 00051: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0474 - accuracy: 0.9855 - val_loss: 0.2939 - val_accuracy: 0.9222\n",
      "Epoch 52/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0405 - accuracy: 0.9890\n",
      "Epoch 00052: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0410 - accuracy: 0.9884 - val_loss: 0.3180 - val_accuracy: 0.9202\n",
      "Epoch 53/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0452 - accuracy: 0.9864\n",
      "Epoch 00053: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0464 - accuracy: 0.9860 - val_loss: 0.3206 - val_accuracy: 0.9216\n",
      "Epoch 54/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0451 - accuracy: 0.9856\n",
      "Epoch 00054: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0450 - accuracy: 0.9855 - val_loss: 0.3238 - val_accuracy: 0.9222\n",
      "Epoch 55/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0401 - accuracy: 0.9900\n",
      "Epoch 00055: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0402 - accuracy: 0.9901 - val_loss: 0.3268 - val_accuracy: 0.9222\n",
      "Epoch 56/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0392 - accuracy: 0.9901\n",
      "Epoch 00056: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0390 - accuracy: 0.9901 - val_loss: 0.3167 - val_accuracy: 0.9222\n",
      "Epoch 57/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0409 - accuracy: 0.9896\n",
      "Epoch 00057: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0404 - accuracy: 0.9898 - val_loss: 0.2975 - val_accuracy: 0.9229\n",
      "Epoch 58/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0409 - accuracy: 0.9892\n",
      "Epoch 00058: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0420 - accuracy: 0.9887 - val_loss: 0.3284 - val_accuracy: 0.9202\n",
      "Epoch 59/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9816\n",
      "Epoch 00059: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0527 - accuracy: 0.9824 - val_loss: 0.3025 - val_accuracy: 0.9209\n",
      "Epoch 60/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0356 - accuracy: 0.9909\n",
      "Epoch 00060: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0358 - accuracy: 0.9906 - val_loss: 0.2996 - val_accuracy: 0.9277\n",
      "Epoch 61/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0318 - accuracy: 0.9913\n",
      "Epoch 00061: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0323 - accuracy: 0.9910 - val_loss: 0.3106 - val_accuracy: 0.9243\n",
      "Epoch 62/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0298 - accuracy: 0.9927\n",
      "Epoch 00062: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0296 - accuracy: 0.9932 - val_loss: 0.3156 - val_accuracy: 0.9202\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0258 - accuracy: 0.9939\n",
      "Epoch 00063: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 0.3298 - val_accuracy: 0.9284\n",
      "Epoch 64/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0373 - accuracy: 0.9892\n",
      "Epoch 00064: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0383 - accuracy: 0.9882 - val_loss: 0.4364 - val_accuracy: 0.8990\n",
      "Epoch 65/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0437 - accuracy: 0.9853\n",
      "Epoch 00065: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0437 - accuracy: 0.9852 - val_loss: 0.3300 - val_accuracy: 0.9216\n",
      "Epoch 66/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0505 - accuracy: 0.9846\n",
      "Epoch 00066: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0497 - accuracy: 0.9850 - val_loss: 0.3541 - val_accuracy: 0.9086\n",
      "Epoch 67/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0428 - accuracy: 0.9868\n",
      "Epoch 00067: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0414 - accuracy: 0.9875 - val_loss: 0.3394 - val_accuracy: 0.9127\n",
      "Epoch 68/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0279 - accuracy: 0.9929\n",
      "Epoch 00068: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0281 - accuracy: 0.9925 - val_loss: 0.3321 - val_accuracy: 0.9222\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0248 - accuracy: 0.9950\n",
      "Epoch 00069: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0247 - accuracy: 0.9951 - val_loss: 0.3403 - val_accuracy: 0.9188\n",
      "Epoch 70/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0268 - accuracy: 0.9924\n",
      "Epoch 00070: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0274 - accuracy: 0.9922 - val_loss: 0.3465 - val_accuracy: 0.9270\n",
      "Epoch 71/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0261 - accuracy: 0.9939\n",
      "Epoch 00071: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0252 - accuracy: 0.9944 - val_loss: 0.3370 - val_accuracy: 0.9229\n",
      "Epoch 72/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0211 - accuracy: 0.9957\n",
      "Epoch 00072: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0203 - accuracy: 0.9961 - val_loss: 0.3196 - val_accuracy: 0.9277\n",
      "Epoch 73/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0179 - accuracy: 0.9968\n",
      "Epoch 00073: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0184 - accuracy: 0.9962 - val_loss: 0.3290 - val_accuracy: 0.9263\n",
      "Epoch 74/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0159 - accuracy: 0.9972\n",
      "Epoch 00074: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.3166 - val_accuracy: 0.9270\n",
      "Epoch 75/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0126 - accuracy: 0.9989\n",
      "Epoch 00075: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0127 - accuracy: 0.9988 - val_loss: 0.3187 - val_accuracy: 0.9263\n",
      "Epoch 76/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0136 - accuracy: 0.9983\n",
      "Epoch 00076: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 0.3362 - val_accuracy: 0.9250\n",
      "Epoch 77/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0157 - accuracy: 0.9972\n",
      "Epoch 00077: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0174 - accuracy: 0.9961 - val_loss: 0.3764 - val_accuracy: 0.9181\n",
      "Epoch 78/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0155 - accuracy: 0.9974\n",
      "Epoch 00078: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.3342 - val_accuracy: 0.9270\n",
      "Epoch 79/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0114 - accuracy: 0.9981\n",
      "Epoch 00079: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0111 - accuracy: 0.9983 - val_loss: 0.3320 - val_accuracy: 0.9236\n",
      "Epoch 80/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0097 - accuracy: 0.9985\n",
      "Epoch 00080: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0096 - accuracy: 0.9986 - val_loss: 0.3350 - val_accuracy: 0.9256\n",
      "Epoch 81/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0091 - accuracy: 0.9991\n",
      "Epoch 00081: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0091 - accuracy: 0.9991 - val_loss: 0.3295 - val_accuracy: 0.9284\n",
      "Epoch 82/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0091 - accuracy: 0.9991\n",
      "Epoch 00082: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0092 - accuracy: 0.9990 - val_loss: 0.3316 - val_accuracy: 0.9284\n",
      "Epoch 83/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0087 - accuracy: 0.9993\n",
      "Epoch 00083: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0089 - accuracy: 0.9991 - val_loss: 0.3369 - val_accuracy: 0.9270\n",
      "Epoch 84/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0114 - accuracy: 0.9978\n",
      "Epoch 00084: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.3464 - val_accuracy: 0.9291\n",
      "Epoch 85/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0156 - accuracy: 0.9963\n",
      "Epoch 00085: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.3527 - val_accuracy: 0.9263\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0155 - accuracy: 0.9972\n",
      "Epoch 00086: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0153 - accuracy: 0.9973 - val_loss: 0.3843 - val_accuracy: 0.9250\n",
      "Epoch 87/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0127 - accuracy: 0.9979\n",
      "Epoch 00087: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0129 - accuracy: 0.9980 - val_loss: 0.3522 - val_accuracy: 0.9236\n",
      "Epoch 88/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0170 - accuracy: 0.9961\n",
      "Epoch 00088: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0169 - accuracy: 0.9962 - val_loss: 0.3610 - val_accuracy: 0.9188\n",
      "Epoch 89/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0158 - accuracy: 0.9967\n",
      "Epoch 00089: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0159 - accuracy: 0.9968 - val_loss: 0.3592 - val_accuracy: 0.9291\n",
      "Epoch 90/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0140 - accuracy: 0.9980\n",
      "Epoch 00090: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0134 - accuracy: 0.9981 - val_loss: 0.3552 - val_accuracy: 0.9270\n",
      "Epoch 91/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0146 - accuracy: 0.9972\n",
      "Epoch 00091: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0149 - accuracy: 0.9968 - val_loss: 0.3603 - val_accuracy: 0.9284\n",
      "Epoch 92/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0262 - accuracy: 0.9931\n",
      "Epoch 00092: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0272 - accuracy: 0.9928 - val_loss: 0.3940 - val_accuracy: 0.9202\n",
      "Epoch 93/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0224 - accuracy: 0.9931\n",
      "Epoch 00093: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.4072 - val_accuracy: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0218 - accuracy: 0.9942\n",
      "Epoch 00094: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0209 - accuracy: 0.9947 - val_loss: 0.3855 - val_accuracy: 0.9188\n",
      "Epoch 95/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0139 - accuracy: 0.9965\n",
      "Epoch 00095: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0131 - accuracy: 0.9968 - val_loss: 0.3622 - val_accuracy: 0.9325\n",
      "Epoch 96/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0101 - accuracy: 0.9981\n",
      "Epoch 00096: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0097 - accuracy: 0.9983 - val_loss: 0.3499 - val_accuracy: 0.9318\n",
      "Epoch 97/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0088 - accuracy: 0.9981\n",
      "Epoch 00097: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0088 - accuracy: 0.9983 - val_loss: 0.3614 - val_accuracy: 0.9256\n",
      "Epoch 98/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0088 - accuracy: 0.9982\n",
      "Epoch 00098: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.3898 - val_accuracy: 0.9202\n",
      "Epoch 99/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0092 - accuracy: 0.9983\n",
      "Epoch 00099: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.3703 - val_accuracy: 0.9297\n",
      "Epoch 100/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0057 - accuracy: 0.9989\n",
      "Epoch 00100: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.3578 - val_accuracy: 0.9291\n",
      "Epoch 101/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 00101: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.3642 - val_accuracy: 0.9243\n",
      "Epoch 102/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 00102: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0049 - accuracy: 0.9995 - val_loss: 0.3707 - val_accuracy: 0.9256\n",
      "Epoch 103/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0047 - accuracy: 0.9993\n",
      "Epoch 00103: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.3737 - val_accuracy: 0.9284\n",
      "Epoch 104/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0061 - accuracy: 0.9984\n",
      "Epoch 00104: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.3762 - val_accuracy: 0.9277\n",
      "Epoch 105/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0050 - accuracy: 0.9991\n",
      "Epoch 00105: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.3711 - val_accuracy: 0.9270\n",
      "Epoch 106/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0048 - accuracy: 0.9985\n",
      "Epoch 00106: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.3709 - val_accuracy: 0.9291\n",
      "Epoch 107/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0046 - accuracy: 0.9989\n",
      "Epoch 00107: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.3773 - val_accuracy: 0.9263\n",
      "Epoch 108/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0033 - accuracy: 0.9996\n",
      "Epoch 00108: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.3779 - val_accuracy: 0.9284\n",
      "Epoch 109/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0069 - accuracy: 0.9985\n",
      "Epoch 00109: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.3779 - val_accuracy: 0.9318\n",
      "Epoch 110/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0089 - accuracy: 0.9980\n",
      "Epoch 00110: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.4046 - val_accuracy: 0.9229\n",
      "Epoch 111/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0073 - accuracy: 0.9989\n",
      "Epoch 00111: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.3990 - val_accuracy: 0.9297\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0061 - accuracy: 0.9989\n",
      "Epoch 00112: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 0.4786 - val_accuracy: 0.9209\n",
      "Epoch 113/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0113 - accuracy: 0.9974\n",
      "Epoch 00113: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0112 - accuracy: 0.9974 - val_loss: 0.4010 - val_accuracy: 0.9297\n",
      "Epoch 114/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0086 - accuracy: 0.9981\n",
      "Epoch 00114: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.4034 - val_accuracy: 0.9229\n",
      "Epoch 115/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0068 - accuracy: 0.9989\n",
      "Epoch 00115: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.4065 - val_accuracy: 0.9318\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0055 - accuracy: 0.9991\n",
      "Epoch 00116: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 0.3979 - val_accuracy: 0.9202\n",
      "Epoch 117/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 00117: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.3721 - val_accuracy: 0.9338\n",
      "Epoch 118/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 00118: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.3914 - val_accuracy: 0.9318\n",
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0055 - accuracy: 0.9991\n",
      "Epoch 00119: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.4063 - val_accuracy: 0.9277\n",
      "Epoch 120/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0061 - accuracy: 0.9985\n",
      "Epoch 00120: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.4080 - val_accuracy: 0.9277\n",
      "Epoch 121/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0088 - accuracy: 0.9974\n",
      "Epoch 00121: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.4289 - val_accuracy: 0.9209\n",
      "Epoch 122/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0108 - accuracy: 0.9968\n",
      "Epoch 00122: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0109 - accuracy: 0.9968 - val_loss: 0.4527 - val_accuracy: 0.9188\n",
      "Epoch 123/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1747 - accuracy: 0.9559\n",
      "Epoch 00123: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1771 - accuracy: 0.9533 - val_loss: 0.8218 - val_accuracy: 0.8513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1947 - accuracy: 0.9397\n",
      "Epoch 00124: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1862 - accuracy: 0.9416 - val_loss: 0.4171 - val_accuracy: 0.9100\n",
      "Epoch 125/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0774 - accuracy: 0.9743\n",
      "Epoch 00125: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0733 - accuracy: 0.9758 - val_loss: 0.4099 - val_accuracy: 0.9141\n",
      "Epoch 126/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0343 - accuracy: 0.9872\n",
      "Epoch 00126: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0344 - accuracy: 0.9872 - val_loss: 0.3862 - val_accuracy: 0.9181\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0205 - accuracy: 0.9953\n",
      "Epoch 00127: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0195 - accuracy: 0.9957 - val_loss: 0.3467 - val_accuracy: 0.9270\n",
      "Epoch 128/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0160 - accuracy: 0.9963\n",
      "Epoch 00128: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0154 - accuracy: 0.9964 - val_loss: 0.3532 - val_accuracy: 0.9297\n",
      "Epoch 129/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0088 - accuracy: 0.9987\n",
      "Epoch 00129: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.3164 - val_accuracy: 0.9332\n",
      "Epoch 130/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0060 - accuracy: 0.9991\n",
      "Epoch 00130: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 0.3342 - val_accuracy: 0.9297\n",
      "Epoch 131/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9995\n",
      "Epoch 00131: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0049 - accuracy: 0.9995 - val_loss: 0.3217 - val_accuracy: 0.9318\n",
      "Epoch 132/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0045 - accuracy: 0.9993\n",
      "Epoch 00132: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.3216 - val_accuracy: 0.9332\n",
      "Epoch 133/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 00133: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.3194 - val_accuracy: 0.9318\n",
      "Epoch 134/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 00134: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.3374 - val_accuracy: 0.9291\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00135: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.3227 - val_accuracy: 0.9318\n",
      "Epoch 136/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0036 - accuracy: 0.9993\n",
      "Epoch 00136: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.3347 - val_accuracy: 0.9291\n",
      "Epoch 137/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 00137: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.3374 - val_accuracy: 0.9291\n",
      "Epoch 138/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0056 - accuracy: 0.9983\n",
      "Epoch 00138: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.3370 - val_accuracy: 0.9277\n",
      "Epoch 139/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0034 - accuracy: 0.9996\n",
      "Epoch 00139: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.3359 - val_accuracy: 0.9338\n",
      "Epoch 140/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00140: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.3394 - val_accuracy: 0.9304\n",
      "Epoch 141/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00141: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.3669 - val_accuracy: 0.9243\n",
      "Epoch 142/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0042 - accuracy: 0.9994\n",
      "Epoch 00142: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.3650 - val_accuracy: 0.9250\n",
      "Epoch 143/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0059 - accuracy: 0.9982\n",
      "Epoch 00143: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.3416 - val_accuracy: 0.9325\n",
      "Epoch 144/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 00144: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.3461 - val_accuracy: 0.9256\n",
      "Epoch 145/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 00145: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.3428 - val_accuracy: 0.9332\n",
      "Epoch 146/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 00146: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.3553 - val_accuracy: 0.9284\n",
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 00147: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.3638 - val_accuracy: 0.9270\n",
      "Epoch 148/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0098 - accuracy: 0.9968\n",
      "Epoch 00148: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.3693 - val_accuracy: 0.9256\n",
      "Epoch 149/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9980\n",
      "Epoch 00149: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.3599 - val_accuracy: 0.9325\n",
      "Epoch 150/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0049 - accuracy: 0.9984\n",
      "Epoch 00150: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.3625 - val_accuracy: 0.9297\n",
      "Epoch 151/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0066 - accuracy: 0.9976\n",
      "Epoch 00151: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.3651 - val_accuracy: 0.9318\n",
      "Epoch 152/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 00152: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.3649 - val_accuracy: 0.9277\n",
      "Epoch 153/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 00153: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.3599 - val_accuracy: 0.9332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 00154: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.3560 - val_accuracy: 0.9284\n",
      "Epoch 155/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 00155: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.3641 - val_accuracy: 0.9291\n",
      "Epoch 156/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0063 - accuracy: 0.9985\n",
      "Epoch 00156: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.3615 - val_accuracy: 0.9297\n",
      "Epoch 157/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 00157: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.3643 - val_accuracy: 0.9277\n",
      "Epoch 158/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 00158: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.3578 - val_accuracy: 0.9318\n",
      "Epoch 159/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9998\n",
      "Epoch 00159: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.3669 - val_accuracy: 0.9277\n",
      "Epoch 160/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 00160: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.3605 - val_accuracy: 0.9297\n",
      "Epoch 161/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 00161: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.3898 - val_accuracy: 0.9297\n",
      "Epoch 162/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0052 - accuracy: 0.9987\n",
      "Epoch 00162: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.3728 - val_accuracy: 0.9284\n",
      "Epoch 163/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 00163: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.3851 - val_accuracy: 0.9277\n",
      "Epoch 164/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0043 - accuracy: 0.9987\n",
      "Epoch 00164: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.3821 - val_accuracy: 0.9297\n",
      "Epoch 165/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 00165: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.3789 - val_accuracy: 0.9297\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0026 - accuracy: 0.9991\n",
      "Epoch 00166: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.3801 - val_accuracy: 0.9325\n",
      "Epoch 167/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0027 - accuracy: 0.9991\n",
      "Epoch 00167: val_loss did not improve from 0.28815\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.3722 - val_accuracy: 0.9297\n",
      "Epoch 00167: early stopping\n",
      "Training for model  4  completed in time:  0:01:06.642679 seconds\n"
     ]
    }
   ],
   "source": [
    "histories, durations = model.train(X_train, X_test, y_train, y_test, num_epochs=500, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES FOR MODEL WITH  1  LAYERS : \n",
      "Training Accuracy:  0.5159528851509094\n",
      "Testing Accuracy:  0.46793997287750244\n",
      "Duration of training:  0:01:48.897237 \n",
      "\n",
      "SCORES FOR MODEL WITH  2  LAYERS : \n",
      "Training Accuracy:  0.8097594380378723\n",
      "Testing Accuracy:  0.7830832004547119\n",
      "Duration of training:  0:02:59.421081 \n",
      "\n",
      "SCORES FOR MODEL WITH  3  LAYERS : \n",
      "Training Accuracy:  0.9754307866096497\n",
      "Testing Accuracy:  0.8908594846725464\n",
      "Duration of training:  0:02:57.480146 \n",
      "\n",
      "SCORES FOR MODEL WITH  4  LAYERS : \n",
      "Training Accuracy:  0.999488115310669\n",
      "Testing Accuracy:  0.9297407865524292\n",
      "Duration of training:  0:01:06.642679 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'training_score_1': [1.4135992527008057, 0.5159528851509094],\n",
       "  'training_score_2': [0.575949490070343, 0.8097594380378723],\n",
       "  'training_score_3': [0.07090330123901367, 0.9754307866096497],\n",
       "  'training_score_4': [0.0018402889836579561, 0.999488115310669]},\n",
       " {'testing_score_1': [1.4886473417282104, 0.46793997287750244],\n",
       "  'testing_score_2': [0.6514788269996643, 0.7830832004547119],\n",
       "  'testing_score_3': [0.43203628063201904, 0.8908594846725464],\n",
       "  'testing_score_4': [0.3722497522830963, 0.9297407865524292]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Plots for Model  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Plots for Model  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Plots for Model  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Plots for Model  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.plot_all_histories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 850x510 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.accuracies_vs_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 850x510 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.duration_vs_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}