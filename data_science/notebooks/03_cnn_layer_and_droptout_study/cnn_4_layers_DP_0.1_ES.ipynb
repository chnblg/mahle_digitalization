{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import warnings\r\n",
    "import cnn_loop\r\n",
    "import os\r\n",
    "\r\n",
    "from processing import preprocessing\r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 of data loaded\n"
     ]
    }
   ],
   "source": [
    "data_df = preprocessing.load_dataset(num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('/datasets/UrbanSound8K/processed/mean_mfcc_data.json')\n",
    "data_df = preprocessing.filter_mfccs(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocessing.create_training_data(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5861, 40, 173, 1)\n",
      "X_test shape: (1466, 40, 173, 1)\n",
      "y_train shape: (5861, 10)\n",
      "y_test shape: (1466, 10)\n",
      "num_outputs:  10\n"
     ]
    }
   ],
   "source": [
    "num_outputs = data_df['label'].unique().shape[0]  # labels = 10\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"num_outputs: \", num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_loop.CNN(num_outputs, num_models=4, DP_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f1cfab571d0>,\n",
       " 'model_2': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f1cfabc8908>,\n",
       " 'model_3': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f1cfabbb4e0>,\n",
       " 'model_4': <tensorflow.python.keras.engine.sequential.Sequential at 0x7f1cfabaf5c0>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUMMARY FOR MODEL  1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 5.6375 - accuracy: 0.1508\n",
      "Pre-training accuracy: 15.0750%\n",
      "\n",
      " SUMMARY FOR MODEL  2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 2,490\n",
      "Trainable params: 2,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 6.7914 - accuracy: 0.1112\n",
      "Pre-training accuracy: 11.1187%\n",
      "\n",
      " SUMMARY FOR MODEL  3\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 11,066\n",
      "Trainable params: 11,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 7.7406 - accuracy: 0.0866\n",
      "Pre-training accuracy: 8.6630%\n",
      "\n",
      " SUMMARY FOR MODEL  4\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 39, 172, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 19, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 9.2733 - accuracy: 0.1207\n",
      "Pre-training accuracy: 12.0737%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.initialize(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for model  1  has started.\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 4.0629 - accuracy: 0.1580\n",
      "Epoch 00001: val_loss improved from inf to 3.09715, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 4.0629 - accuracy: 0.1580 - val_loss: 3.0971 - val_accuracy: 0.1733\n",
      "Epoch 2/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 2.5924 - accuracy: 0.1616\n",
      "Epoch 00002: val_loss improved from 3.09715 to 2.27010, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.5666 - accuracy: 0.1638 - val_loss: 2.2701 - val_accuracy: 0.1937\n",
      "Epoch 3/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 2.1765 - accuracy: 0.1950\n",
      "Epoch 00003: val_loss improved from 2.27010 to 2.13745, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.1782 - accuracy: 0.1948 - val_loss: 2.1374 - val_accuracy: 0.2108\n",
      "Epoch 4/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 2.0850 - accuracy: 0.2067\n",
      "Epoch 00004: val_loss improved from 2.13745 to 2.08198, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.0841 - accuracy: 0.2071 - val_loss: 2.0820 - val_accuracy: 0.1999\n",
      "Epoch 5/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 2.0316 - accuracy: 0.2173\n",
      "Epoch 00005: val_loss improved from 2.08198 to 2.03860, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.0335 - accuracy: 0.2140 - val_loss: 2.0386 - val_accuracy: 0.2101\n",
      "Epoch 6/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.9923 - accuracy: 0.2250\n",
      "Epoch 00006: val_loss improved from 2.03860 to 2.00699, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9929 - accuracy: 0.2240 - val_loss: 2.0070 - val_accuracy: 0.2094\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9616 - accuracy: 0.2249\n",
      "Epoch 00007: val_loss improved from 2.00699 to 1.98459, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.9616 - accuracy: 0.2249 - val_loss: 1.9846 - val_accuracy: 0.2312\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9375 - accuracy: 0.2361\n",
      "Epoch 00008: val_loss improved from 1.98459 to 1.96549, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9375 - accuracy: 0.2361 - val_loss: 1.9655 - val_accuracy: 0.2367\n",
      "Epoch 9/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.9159 - accuracy: 0.2610\n",
      "Epoch 00009: val_loss improved from 1.96549 to 1.95062, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.9181 - accuracy: 0.2602 - val_loss: 1.9506 - val_accuracy: 0.2265\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.9031 - accuracy: 0.2643\n",
      "Epoch 00010: val_loss improved from 1.95062 to 1.93793, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.9005 - accuracy: 0.2643 - val_loss: 1.9379 - val_accuracy: 0.2592\n",
      "Epoch 11/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.8888 - accuracy: 0.2755\n",
      "Epoch 00011: val_loss improved from 1.93793 to 1.93015, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8887 - accuracy: 0.2750 - val_loss: 1.9301 - val_accuracy: 0.2483\n",
      "Epoch 12/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.8813 - accuracy: 0.2910\n",
      "Epoch 00012: val_loss improved from 1.93015 to 1.92041, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8772 - accuracy: 0.2894 - val_loss: 1.9204 - val_accuracy: 0.2490\n",
      "Epoch 13/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.8630 - accuracy: 0.3000\n",
      "Epoch 00013: val_loss improved from 1.92041 to 1.90595, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8619 - accuracy: 0.3023 - val_loss: 1.9059 - val_accuracy: 0.2906\n",
      "Epoch 14/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.8514 - accuracy: 0.3065\n",
      "Epoch 00014: val_loss improved from 1.90595 to 1.89779, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8518 - accuracy: 0.3095 - val_loss: 1.8978 - val_accuracy: 0.2824\n",
      "Epoch 15/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.8403 - accuracy: 0.3164\n",
      "Epoch 00015: val_loss improved from 1.89779 to 1.88831, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8402 - accuracy: 0.3168 - val_loss: 1.8883 - val_accuracy: 0.2879\n",
      "Epoch 16/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.8346 - accuracy: 0.3125\n",
      "Epoch 00016: val_loss improved from 1.88831 to 1.88093, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8355 - accuracy: 0.3127 - val_loss: 1.8809 - val_accuracy: 0.2892\n",
      "Epoch 17/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.8254 - accuracy: 0.3276\n",
      "Epoch 00017: val_loss improved from 1.88093 to 1.87381, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8249 - accuracy: 0.3223 - val_loss: 1.8738 - val_accuracy: 0.2708\n",
      "Epoch 18/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.8148 - accuracy: 0.3270\n",
      "Epoch 00018: val_loss improved from 1.87381 to 1.86883, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.8160 - accuracy: 0.3276 - val_loss: 1.8688 - val_accuracy: 0.2790\n",
      "Epoch 19/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.8174 - accuracy: 0.3227\n",
      "Epoch 00019: val_loss improved from 1.86883 to 1.85857, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8122 - accuracy: 0.3276 - val_loss: 1.8586 - val_accuracy: 0.3179\n",
      "Epoch 20/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.8043 - accuracy: 0.3342\n",
      "Epoch 00020: val_loss improved from 1.85857 to 1.85802, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8038 - accuracy: 0.3317 - val_loss: 1.8580 - val_accuracy: 0.2749\n",
      "Epoch 21/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.8020 - accuracy: 0.3347\n",
      "Epoch 00021: val_loss improved from 1.85802 to 1.85592, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8030 - accuracy: 0.3336 - val_loss: 1.8559 - val_accuracy: 0.2708\n",
      "Epoch 22/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7991 - accuracy: 0.3248\n",
      "Epoch 00022: val_loss improved from 1.85592 to 1.84851, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7980 - accuracy: 0.3238 - val_loss: 1.8485 - val_accuracy: 0.2885\n",
      "Epoch 23/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7957 - accuracy: 0.3305\n",
      "Epoch 00023: val_loss improved from 1.84851 to 1.84252, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7937 - accuracy: 0.3279 - val_loss: 1.8425 - val_accuracy: 0.2933\n",
      "Epoch 24/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7902 - accuracy: 0.3301\n",
      "Epoch 00024: val_loss did not improve from 1.84252\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7877 - accuracy: 0.3300 - val_loss: 1.8426 - val_accuracy: 0.2892\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7851 - accuracy: 0.3317\n",
      "Epoch 00025: val_loss improved from 1.84252 to 1.83284, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7861 - accuracy: 0.3336 - val_loss: 1.8328 - val_accuracy: 0.3070\n",
      "Epoch 26/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7875 - accuracy: 0.3268\n",
      "Epoch 00026: val_loss did not improve from 1.83284\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7818 - accuracy: 0.3276 - val_loss: 1.8349 - val_accuracy: 0.2913\n",
      "Epoch 27/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.7736 - accuracy: 0.3364\n",
      "Epoch 00027: val_loss improved from 1.83284 to 1.82845, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7746 - accuracy: 0.3404 - val_loss: 1.8284 - val_accuracy: 0.2940\n",
      "Epoch 28/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7751 - accuracy: 0.3385\n",
      "Epoch 00028: val_loss improved from 1.82845 to 1.82804, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7749 - accuracy: 0.3380 - val_loss: 1.8280 - val_accuracy: 0.2947\n",
      "Epoch 29/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7714 - accuracy: 0.3382\n",
      "Epoch 00029: val_loss did not improve from 1.82804\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7743 - accuracy: 0.3353 - val_loss: 1.8301 - val_accuracy: 0.2940\n",
      "Epoch 30/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7688 - accuracy: 0.3387\n",
      "Epoch 00030: val_loss improved from 1.82804 to 1.81866, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7706 - accuracy: 0.3377 - val_loss: 1.8187 - val_accuracy: 0.3220\n",
      "Epoch 31/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7710 - accuracy: 0.3398\n",
      "Epoch 00031: val_loss improved from 1.81866 to 1.81514, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7681 - accuracy: 0.3411 - val_loss: 1.8151 - val_accuracy: 0.3117\n",
      "Epoch 32/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7667 - accuracy: 0.3380\n",
      "Epoch 00032: val_loss improved from 1.81514 to 1.81167, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7658 - accuracy: 0.3389 - val_loss: 1.8117 - val_accuracy: 0.3274\n",
      "Epoch 33/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7613 - accuracy: 0.3452\n",
      "Epoch 00033: val_loss improved from 1.81167 to 1.81129, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7625 - accuracy: 0.3438 - val_loss: 1.8113 - val_accuracy: 0.3179\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7621 - accuracy: 0.3394\n",
      "Epoch 00034: val_loss improved from 1.81129 to 1.80981, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7621 - accuracy: 0.3394 - val_loss: 1.8098 - val_accuracy: 0.3111\n",
      "Epoch 35/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7628 - accuracy: 0.3529\n",
      "Epoch 00035: val_loss improved from 1.80981 to 1.80476, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7550 - accuracy: 0.3525 - val_loss: 1.8048 - val_accuracy: 0.3192\n",
      "Epoch 36/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7550 - accuracy: 0.3445\n",
      "Epoch 00036: val_loss improved from 1.80476 to 1.80267, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7559 - accuracy: 0.3448 - val_loss: 1.8027 - val_accuracy: 0.3179\n",
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7562 - accuracy: 0.3480\n",
      "Epoch 00037: val_loss improved from 1.80267 to 1.80263, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7561 - accuracy: 0.3493 - val_loss: 1.8026 - val_accuracy: 0.3104\n",
      "Epoch 38/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7506 - accuracy: 0.3571\n",
      "Epoch 00038: val_loss did not improve from 1.80263\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7513 - accuracy: 0.3559 - val_loss: 1.8038 - val_accuracy: 0.3186\n",
      "Epoch 39/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7508 - accuracy: 0.3451\n",
      "Epoch 00039: val_loss improved from 1.80263 to 1.79459, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7489 - accuracy: 0.3467 - val_loss: 1.7946 - val_accuracy: 0.3220\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7491 - accuracy: 0.3494\n",
      "Epoch 00040: val_loss did not improve from 1.79459\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7491 - accuracy: 0.3494 - val_loss: 1.7965 - val_accuracy: 0.3111\n",
      "Epoch 41/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7481 - accuracy: 0.3535\n",
      "Epoch 00041: val_loss did not improve from 1.79459\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7490 - accuracy: 0.3511 - val_loss: 1.7983 - val_accuracy: 0.3247\n",
      "Epoch 42/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7486 - accuracy: 0.3497\n",
      "Epoch 00042: val_loss improved from 1.79459 to 1.79401, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7455 - accuracy: 0.3520 - val_loss: 1.7940 - val_accuracy: 0.3186\n",
      "Epoch 43/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7513 - accuracy: 0.3491\n",
      "Epoch 00043: val_loss improved from 1.79401 to 1.79177, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7460 - accuracy: 0.3513 - val_loss: 1.7918 - val_accuracy: 0.3295\n",
      "Epoch 44/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7462 - accuracy: 0.3477\n",
      "Epoch 00044: val_loss improved from 1.79177 to 1.78373, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7437 - accuracy: 0.3510 - val_loss: 1.7837 - val_accuracy: 0.3322\n",
      "Epoch 45/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7410 - accuracy: 0.3521\n",
      "Epoch 00045: val_loss did not improve from 1.78373\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7379 - accuracy: 0.3525 - val_loss: 1.7839 - val_accuracy: 0.3220\n",
      "Epoch 46/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7316 - accuracy: 0.3499\n",
      "Epoch 00046: val_loss did not improve from 1.78373\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7373 - accuracy: 0.3503 - val_loss: 1.7885 - val_accuracy: 0.3417\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7408 - accuracy: 0.3557\n",
      "Epoch 00047: val_loss did not improve from 1.78373\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7408 - accuracy: 0.3557 - val_loss: 1.7877 - val_accuracy: 0.3302\n",
      "Epoch 48/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7375 - accuracy: 0.3562\n",
      "Epoch 00048: val_loss improved from 1.78373 to 1.77950, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7361 - accuracy: 0.3551 - val_loss: 1.7795 - val_accuracy: 0.3220\n",
      "Epoch 49/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7338 - accuracy: 0.3571\n",
      "Epoch 00049: val_loss improved from 1.77950 to 1.77842, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7349 - accuracy: 0.3578 - val_loss: 1.7784 - val_accuracy: 0.3288\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7323 - accuracy: 0.3594\n",
      "Epoch 00050: val_loss improved from 1.77842 to 1.77697, saving model to models/saved_models/best_models_DP_0.1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7317 - accuracy: 0.3602 - val_loss: 1.7770 - val_accuracy: 0.3315\n",
      "Epoch 51/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7320 - accuracy: 0.3638\n",
      "Epoch 00051: val_loss did not improve from 1.77697\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7306 - accuracy: 0.3634 - val_loss: 1.7776 - val_accuracy: 0.3199\n",
      "Epoch 52/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7317 - accuracy: 0.3575\n",
      "Epoch 00052: val_loss improved from 1.77697 to 1.77389, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7301 - accuracy: 0.3583 - val_loss: 1.7739 - val_accuracy: 0.3349\n",
      "Epoch 53/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7219 - accuracy: 0.3659\n",
      "Epoch 00053: val_loss did not improve from 1.77389\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7278 - accuracy: 0.3631 - val_loss: 1.7797 - val_accuracy: 0.3151\n",
      "Epoch 54/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.7324 - accuracy: 0.3553\n",
      "Epoch 00054: val_loss improved from 1.77389 to 1.76945, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7313 - accuracy: 0.3609 - val_loss: 1.7695 - val_accuracy: 0.3377\n",
      "Epoch 55/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7299 - accuracy: 0.3603\n",
      "Epoch 00055: val_loss did not improve from 1.76945\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7256 - accuracy: 0.3627 - val_loss: 1.7727 - val_accuracy: 0.3267\n",
      "Epoch 56/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7247 - accuracy: 0.3603\n",
      "Epoch 00056: val_loss improved from 1.76945 to 1.76892, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7241 - accuracy: 0.3626 - val_loss: 1.7689 - val_accuracy: 0.3254\n",
      "Epoch 57/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7243 - accuracy: 0.3601\n",
      "Epoch 00057: val_loss improved from 1.76892 to 1.76185, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7242 - accuracy: 0.3595 - val_loss: 1.7619 - val_accuracy: 0.3492\n",
      "Epoch 58/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7213 - accuracy: 0.3606\n",
      "Epoch 00058: val_loss improved from 1.76185 to 1.76149, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7232 - accuracy: 0.3602 - val_loss: 1.7615 - val_accuracy: 0.3424\n",
      "Epoch 59/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7170 - accuracy: 0.3633\n",
      "Epoch 00059: val_loss did not improve from 1.76149\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7178 - accuracy: 0.3634 - val_loss: 1.7689 - val_accuracy: 0.3322\n",
      "Epoch 60/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7177 - accuracy: 0.3697\n",
      "Epoch 00060: val_loss did not improve from 1.76149\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7219 - accuracy: 0.3684 - val_loss: 1.7698 - val_accuracy: 0.3329\n",
      "Epoch 61/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7210 - accuracy: 0.3718\n",
      "Epoch 00061: val_loss did not improve from 1.76149\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7186 - accuracy: 0.3720 - val_loss: 1.7627 - val_accuracy: 0.3390\n",
      "Epoch 62/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7186 - accuracy: 0.3620\n",
      "Epoch 00062: val_loss improved from 1.76149 to 1.76056, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7173 - accuracy: 0.3641 - val_loss: 1.7606 - val_accuracy: 0.3404\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7123 - accuracy: 0.3720\n",
      "Epoch 00063: val_loss did not improve from 1.76056\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7169 - accuracy: 0.3679 - val_loss: 1.7621 - val_accuracy: 0.3356\n",
      "Epoch 64/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7166 - accuracy: 0.3698\n",
      "Epoch 00064: val_loss improved from 1.76056 to 1.75691, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7143 - accuracy: 0.3679 - val_loss: 1.7569 - val_accuracy: 0.3411\n",
      "Epoch 65/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7139 - accuracy: 0.3688\n",
      "Epoch 00065: val_loss did not improve from 1.75691\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7145 - accuracy: 0.3701 - val_loss: 1.7585 - val_accuracy: 0.3479\n",
      "Epoch 66/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7120 - accuracy: 0.3733\n",
      "Epoch 00066: val_loss improved from 1.75691 to 1.75239, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7137 - accuracy: 0.3675 - val_loss: 1.7524 - val_accuracy: 0.3329\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7107 - accuracy: 0.3726\n",
      "Epoch 00067: val_loss did not improve from 1.75239\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7107 - accuracy: 0.3726 - val_loss: 1.7557 - val_accuracy: 0.3438\n",
      "Epoch 68/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7106 - accuracy: 0.3702\n",
      "Epoch 00068: val_loss did not improve from 1.75239\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7091 - accuracy: 0.3702 - val_loss: 1.7597 - val_accuracy: 0.3220\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7092 - accuracy: 0.3705\n",
      "Epoch 00069: val_loss did not improve from 1.75239\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7107 - accuracy: 0.3684 - val_loss: 1.7616 - val_accuracy: 0.3329\n",
      "Epoch 70/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7064 - accuracy: 0.3646\n",
      "Epoch 00070: val_loss improved from 1.75239 to 1.74756, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7082 - accuracy: 0.3638 - val_loss: 1.7476 - val_accuracy: 0.3486\n",
      "Epoch 71/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7077 - accuracy: 0.3711\n",
      "Epoch 00071: val_loss improved from 1.74756 to 1.74604, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7078 - accuracy: 0.3706 - val_loss: 1.7460 - val_accuracy: 0.3465\n",
      "Epoch 72/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7049 - accuracy: 0.3716\n",
      "Epoch 00072: val_loss improved from 1.74604 to 1.74395, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7057 - accuracy: 0.3709 - val_loss: 1.7440 - val_accuracy: 0.3540\n",
      "Epoch 73/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7107 - accuracy: 0.3715\n",
      "Epoch 00073: val_loss did not improve from 1.74395\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7049 - accuracy: 0.3714 - val_loss: 1.7446 - val_accuracy: 0.3370\n",
      "Epoch 74/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6947 - accuracy: 0.3711\n",
      "Epoch 00074: val_loss did not improve from 1.74395\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7031 - accuracy: 0.3714 - val_loss: 1.7449 - val_accuracy: 0.3377\n",
      "Epoch 75/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7041 - accuracy: 0.3707\n",
      "Epoch 00075: val_loss improved from 1.74395 to 1.74094, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7017 - accuracy: 0.3721 - val_loss: 1.7409 - val_accuracy: 0.3547\n",
      "Epoch 76/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7009 - accuracy: 0.3817\n",
      "Epoch 00076: val_loss did not improve from 1.74094\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7004 - accuracy: 0.3803 - val_loss: 1.7414 - val_accuracy: 0.3370\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7019 - accuracy: 0.3770\n",
      "Epoch 00077: val_loss did not improve from 1.74094\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7007 - accuracy: 0.3762 - val_loss: 1.7459 - val_accuracy: 0.3383\n",
      "Epoch 78/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6953 - accuracy: 0.3795\n",
      "Epoch 00078: val_loss improved from 1.74094 to 1.73543, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6968 - accuracy: 0.3769 - val_loss: 1.7354 - val_accuracy: 0.3561\n",
      "Epoch 79/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6966 - accuracy: 0.3754\n",
      "Epoch 00079: val_loss did not improve from 1.73543\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6950 - accuracy: 0.3774 - val_loss: 1.7380 - val_accuracy: 0.3486\n",
      "Epoch 80/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6944 - accuracy: 0.3750\n",
      "Epoch 00080: val_loss did not improve from 1.73543\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6952 - accuracy: 0.3752 - val_loss: 1.7427 - val_accuracy: 0.3595\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6981 - accuracy: 0.3716\n",
      "Epoch 00081: val_loss did not improve from 1.73543\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6981 - accuracy: 0.3716 - val_loss: 1.7369 - val_accuracy: 0.3554\n",
      "Epoch 82/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6968 - accuracy: 0.3761\n",
      "Epoch 00082: val_loss improved from 1.73543 to 1.73229, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6965 - accuracy: 0.3767 - val_loss: 1.7323 - val_accuracy: 0.3527\n",
      "Epoch 83/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6960 - accuracy: 0.3794\n",
      "Epoch 00083: val_loss improved from 1.73229 to 1.73038, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6946 - accuracy: 0.3803 - val_loss: 1.7304 - val_accuracy: 0.3643\n",
      "Epoch 84/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6913 - accuracy: 0.3754\n",
      "Epoch 00084: val_loss improved from 1.73038 to 1.72820, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6903 - accuracy: 0.3764 - val_loss: 1.7282 - val_accuracy: 0.3540\n",
      "Epoch 85/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6917 - accuracy: 0.3810\n",
      "Epoch 00085: val_loss did not improve from 1.72820\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6894 - accuracy: 0.3829 - val_loss: 1.7285 - val_accuracy: 0.3540\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6928 - accuracy: 0.3778\n",
      "Epoch 00086: val_loss did not improve from 1.72820\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6902 - accuracy: 0.3800 - val_loss: 1.7297 - val_accuracy: 0.3520\n",
      "Epoch 87/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6878 - accuracy: 0.3765\n",
      "Epoch 00087: val_loss improved from 1.72820 to 1.72689, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6838 - accuracy: 0.3781 - val_loss: 1.7269 - val_accuracy: 0.3499\n",
      "Epoch 88/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6922 - accuracy: 0.3731\n",
      "Epoch 00088: val_loss improved from 1.72689 to 1.72688, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6874 - accuracy: 0.3769 - val_loss: 1.7269 - val_accuracy: 0.3506\n",
      "Epoch 89/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6836 - accuracy: 0.3837\n",
      "Epoch 00089: val_loss did not improve from 1.72688\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6849 - accuracy: 0.3829 - val_loss: 1.7304 - val_accuracy: 0.3574\n",
      "Epoch 90/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6870 - accuracy: 0.3808\n",
      "Epoch 00090: val_loss improved from 1.72688 to 1.72160, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6871 - accuracy: 0.3805 - val_loss: 1.7216 - val_accuracy: 0.3656\n",
      "Epoch 91/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6851 - accuracy: 0.3841\n",
      "Epoch 00091: val_loss did not improve from 1.72160\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6849 - accuracy: 0.3837 - val_loss: 1.7249 - val_accuracy: 0.3561\n",
      "Epoch 92/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6777 - accuracy: 0.3860\n",
      "Epoch 00092: val_loss did not improve from 1.72160\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6836 - accuracy: 0.3795 - val_loss: 1.7249 - val_accuracy: 0.3574\n",
      "Epoch 93/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6811 - accuracy: 0.3834\n",
      "Epoch 00093: val_loss improved from 1.72160 to 1.71975, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6847 - accuracy: 0.3803 - val_loss: 1.7197 - val_accuracy: 0.3561\n",
      "Epoch 94/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6827 - accuracy: 0.3834\n",
      "Epoch 00094: val_loss did not improve from 1.71975\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6834 - accuracy: 0.3820 - val_loss: 1.7213 - val_accuracy: 0.3506\n",
      "Epoch 95/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6739 - accuracy: 0.3927\n",
      "Epoch 00095: val_loss did not improve from 1.71975\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6767 - accuracy: 0.3885 - val_loss: 1.7242 - val_accuracy: 0.3404\n",
      "Epoch 96/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6741 - accuracy: 0.3832\n",
      "Epoch 00096: val_loss improved from 1.71975 to 1.71916, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6792 - accuracy: 0.3815 - val_loss: 1.7192 - val_accuracy: 0.3431\n",
      "Epoch 97/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6762 - accuracy: 0.3848\n",
      "Epoch 00097: val_loss improved from 1.71916 to 1.71256, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6773 - accuracy: 0.3858 - val_loss: 1.7126 - val_accuracy: 0.3499\n",
      "Epoch 98/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6751 - accuracy: 0.3853\n",
      "Epoch 00098: val_loss did not improve from 1.71256\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6738 - accuracy: 0.3873 - val_loss: 1.7175 - val_accuracy: 0.3643\n",
      "Epoch 99/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6753 - accuracy: 0.3830\n",
      "Epoch 00099: val_loss improved from 1.71256 to 1.71189, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6756 - accuracy: 0.3844 - val_loss: 1.7119 - val_accuracy: 0.3690\n",
      "Epoch 100/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6765 - accuracy: 0.3814\n",
      "Epoch 00100: val_loss did not improve from 1.71189\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6787 - accuracy: 0.3815 - val_loss: 1.7203 - val_accuracy: 0.3581\n",
      "Epoch 101/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6705 - accuracy: 0.3848\n",
      "Epoch 00101: val_loss improved from 1.71189 to 1.70654, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6705 - accuracy: 0.3849 - val_loss: 1.7065 - val_accuracy: 0.3643\n",
      "Epoch 102/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6658 - accuracy: 0.3921\n",
      "Epoch 00102: val_loss improved from 1.70654 to 1.70512, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6718 - accuracy: 0.3894 - val_loss: 1.7051 - val_accuracy: 0.3656\n",
      "Epoch 103/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6719 - accuracy: 0.3812\n",
      "Epoch 00103: val_loss did not improve from 1.70512\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6699 - accuracy: 0.3844 - val_loss: 1.7085 - val_accuracy: 0.3697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6729 - accuracy: 0.3873\n",
      "Epoch 00104: val_loss did not improve from 1.70512\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6704 - accuracy: 0.3866 - val_loss: 1.7108 - val_accuracy: 0.3683\n",
      "Epoch 105/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6655 - accuracy: 0.3908\n",
      "Epoch 00105: val_loss did not improve from 1.70512\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6687 - accuracy: 0.3911 - val_loss: 1.7054 - val_accuracy: 0.3588\n",
      "Epoch 106/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6660 - accuracy: 0.3955\n",
      "Epoch 00106: val_loss did not improve from 1.70512\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6657 - accuracy: 0.3919 - val_loss: 1.7100 - val_accuracy: 0.3568\n",
      "Epoch 107/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6640 - accuracy: 0.3932\n",
      "Epoch 00107: val_loss improved from 1.70512 to 1.70495, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6639 - accuracy: 0.3914 - val_loss: 1.7050 - val_accuracy: 0.3636\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6649 - accuracy: 0.3957\n",
      "Epoch 00108: val_loss did not improve from 1.70495\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6649 - accuracy: 0.3957 - val_loss: 1.7084 - val_accuracy: 0.3595\n",
      "Epoch 109/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6597 - accuracy: 0.3900\n",
      "Epoch 00109: val_loss improved from 1.70495 to 1.69818, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6630 - accuracy: 0.3892 - val_loss: 1.6982 - val_accuracy: 0.3677\n",
      "Epoch 110/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6589 - accuracy: 0.3968\n",
      "Epoch 00110: val_loss did not improve from 1.69818\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6610 - accuracy: 0.3946 - val_loss: 1.7033 - val_accuracy: 0.3540\n",
      "Epoch 111/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6633 - accuracy: 0.3923\n",
      "Epoch 00111: val_loss improved from 1.69818 to 1.69796, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6608 - accuracy: 0.3929 - val_loss: 1.6980 - val_accuracy: 0.3752\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6610 - accuracy: 0.3921\n",
      "Epoch 00112: val_loss improved from 1.69796 to 1.69576, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6601 - accuracy: 0.3929 - val_loss: 1.6958 - val_accuracy: 0.3683\n",
      "Epoch 113/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6565 - accuracy: 0.3975\n",
      "Epoch 00113: val_loss improved from 1.69576 to 1.69278, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6590 - accuracy: 0.3957 - val_loss: 1.6928 - val_accuracy: 0.3547\n",
      "Epoch 114/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6576 - accuracy: 0.3932\n",
      "Epoch 00114: val_loss did not improve from 1.69278\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6536 - accuracy: 0.3958 - val_loss: 1.6960 - val_accuracy: 0.3656\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6569 - accuracy: 0.3876\n",
      "Epoch 00115: val_loss did not improve from 1.69278\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6569 - accuracy: 0.3876 - val_loss: 1.6996 - val_accuracy: 0.3731\n",
      "Epoch 116/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6520 - accuracy: 0.4019\n",
      "Epoch 00116: val_loss did not improve from 1.69278\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6556 - accuracy: 0.3970 - val_loss: 1.6941 - val_accuracy: 0.3574\n",
      "Epoch 117/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6523 - accuracy: 0.3953\n",
      "Epoch 00117: val_loss improved from 1.69278 to 1.69251, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6549 - accuracy: 0.3917 - val_loss: 1.6925 - val_accuracy: 0.3602\n",
      "Epoch 118/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6561 - accuracy: 0.3952\n",
      "Epoch 00118: val_loss improved from 1.69251 to 1.68823, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6534 - accuracy: 0.3962 - val_loss: 1.6882 - val_accuracy: 0.3718\n",
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6467 - accuracy: 0.4023\n",
      "Epoch 00119: val_loss improved from 1.68823 to 1.68704, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6487 - accuracy: 0.3999 - val_loss: 1.6870 - val_accuracy: 0.3683\n",
      "Epoch 120/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6442 - accuracy: 0.4022\n",
      "Epoch 00120: val_loss improved from 1.68704 to 1.68441, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6447 - accuracy: 0.4021 - val_loss: 1.6844 - val_accuracy: 0.3765\n",
      "Epoch 121/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6477 - accuracy: 0.3997\n",
      "Epoch 00121: val_loss did not improve from 1.68441\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6500 - accuracy: 0.3969 - val_loss: 1.6956 - val_accuracy: 0.3513\n",
      "Epoch 122/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6584 - accuracy: 0.3952\n",
      "Epoch 00122: val_loss did not improve from 1.68441\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6490 - accuracy: 0.3946 - val_loss: 1.6896 - val_accuracy: 0.3697\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6482 - accuracy: 0.3963\n",
      "Epoch 00123: val_loss improved from 1.68441 to 1.67885, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6482 - accuracy: 0.3963 - val_loss: 1.6788 - val_accuracy: 0.3813\n",
      "Epoch 124/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6399 - accuracy: 0.4012\n",
      "Epoch 00124: val_loss did not improve from 1.67885\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6434 - accuracy: 0.3989 - val_loss: 1.6820 - val_accuracy: 0.3765\n",
      "Epoch 125/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6484 - accuracy: 0.3960\n",
      "Epoch 00125: val_loss did not improve from 1.67885\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6452 - accuracy: 0.3982 - val_loss: 1.6804 - val_accuracy: 0.3704\n",
      "Epoch 126/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6445 - accuracy: 0.4022\n",
      "Epoch 00126: val_loss did not improve from 1.67885\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6432 - accuracy: 0.4015 - val_loss: 1.6800 - val_accuracy: 0.3683\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6384 - accuracy: 0.4076\n",
      "Epoch 00127: val_loss did not improve from 1.67885\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6395 - accuracy: 0.4069 - val_loss: 1.6818 - val_accuracy: 0.3690\n",
      "Epoch 128/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6466 - accuracy: 0.3968\n",
      "Epoch 00128: val_loss improved from 1.67885 to 1.67393, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6417 - accuracy: 0.3982 - val_loss: 1.6739 - val_accuracy: 0.3724\n",
      "Epoch 129/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6326 - accuracy: 0.4036\n",
      "Epoch 00129: val_loss did not improve from 1.67393\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6413 - accuracy: 0.3982 - val_loss: 1.6743 - val_accuracy: 0.3786\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6390 - accuracy: 0.3989\n",
      "Epoch 00130: val_loss improved from 1.67393 to 1.67152, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6390 - accuracy: 0.3989 - val_loss: 1.6715 - val_accuracy: 0.3861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6373 - accuracy: 0.3963\n",
      "Epoch 00131: val_loss did not improve from 1.67152\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6370 - accuracy: 0.3972 - val_loss: 1.6745 - val_accuracy: 0.3738\n",
      "Epoch 132/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6360 - accuracy: 0.4055\n",
      "Epoch 00132: val_loss did not improve from 1.67152\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6355 - accuracy: 0.4069 - val_loss: 1.6774 - val_accuracy: 0.3738\n",
      "Epoch 133/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6380 - accuracy: 0.4057\n",
      "Epoch 00133: val_loss did not improve from 1.67152\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6396 - accuracy: 0.4047 - val_loss: 1.6731 - val_accuracy: 0.3868\n",
      "Epoch 134/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6467 - accuracy: 0.3977\n",
      "Epoch 00134: val_loss improved from 1.67152 to 1.66817, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6381 - accuracy: 0.4021 - val_loss: 1.6682 - val_accuracy: 0.3779\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6383 - accuracy: 0.4033\n",
      "Epoch 00135: val_loss did not improve from 1.66817\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6331 - accuracy: 0.4056 - val_loss: 1.6708 - val_accuracy: 0.3786\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6350 - accuracy: 0.4069\n",
      "Epoch 00136: val_loss did not improve from 1.66817\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6350 - accuracy: 0.4069 - val_loss: 1.6695 - val_accuracy: 0.3854\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6312 - accuracy: 0.4083\n",
      "Epoch 00137: val_loss did not improve from 1.66817\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6312 - accuracy: 0.4083 - val_loss: 1.6700 - val_accuracy: 0.3656\n",
      "Epoch 138/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6288 - accuracy: 0.4085\n",
      "Epoch 00138: val_loss improved from 1.66817 to 1.66224, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6307 - accuracy: 0.4080 - val_loss: 1.6622 - val_accuracy: 0.3799\n",
      "Epoch 139/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6222 - accuracy: 0.4150\n",
      "Epoch 00139: val_loss did not improve from 1.66224\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6271 - accuracy: 0.4119 - val_loss: 1.6623 - val_accuracy: 0.3786\n",
      "Epoch 140/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6335 - accuracy: 0.3966\n",
      "Epoch 00140: val_loss did not improve from 1.66224\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6302 - accuracy: 0.3994 - val_loss: 1.6653 - val_accuracy: 0.3786\n",
      "Epoch 141/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6322 - accuracy: 0.4109\n",
      "Epoch 00141: val_loss improved from 1.66224 to 1.66173, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6279 - accuracy: 0.4090 - val_loss: 1.6617 - val_accuracy: 0.3861\n",
      "Epoch 142/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6231 - accuracy: 0.4054\n",
      "Epoch 00142: val_loss did not improve from 1.66173\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6255 - accuracy: 0.4045 - val_loss: 1.6652 - val_accuracy: 0.3888\n",
      "Epoch 143/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6145 - accuracy: 0.4243\n",
      "Epoch 00143: val_loss did not improve from 1.66173\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6185 - accuracy: 0.4209 - val_loss: 1.6628 - val_accuracy: 0.3765\n",
      "Epoch 144/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6202 - accuracy: 0.4139\n",
      "Epoch 00144: val_loss improved from 1.66173 to 1.65992, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6212 - accuracy: 0.4139 - val_loss: 1.6599 - val_accuracy: 0.3738\n",
      "Epoch 145/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6180 - accuracy: 0.4059\n",
      "Epoch 00145: val_loss did not improve from 1.65992\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6207 - accuracy: 0.4054 - val_loss: 1.6600 - val_accuracy: 0.3724\n",
      "Epoch 146/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6233 - accuracy: 0.4105\n",
      "Epoch 00146: val_loss improved from 1.65992 to 1.65529, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6177 - accuracy: 0.4115 - val_loss: 1.6553 - val_accuracy: 0.3861\n",
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6192 - accuracy: 0.4176\n",
      "Epoch 00147: val_loss improved from 1.65529 to 1.65298, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6168 - accuracy: 0.4187 - val_loss: 1.6530 - val_accuracy: 0.3820\n",
      "Epoch 148/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6100 - accuracy: 0.4183\n",
      "Epoch 00148: val_loss did not improve from 1.65298\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6159 - accuracy: 0.4161 - val_loss: 1.6559 - val_accuracy: 0.3840\n",
      "Epoch 149/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.6204 - accuracy: 0.4076\n",
      "Epoch 00149: val_loss improved from 1.65298 to 1.64887, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6182 - accuracy: 0.4100 - val_loss: 1.6489 - val_accuracy: 0.3922\n",
      "Epoch 150/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6190 - accuracy: 0.4143\n",
      "Epoch 00150: val_loss did not improve from 1.64887\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6139 - accuracy: 0.4136 - val_loss: 1.6503 - val_accuracy: 0.3984\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6125 - accuracy: 0.4126\n",
      "Epoch 00151: val_loss did not improve from 1.64887\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6125 - accuracy: 0.4126 - val_loss: 1.6522 - val_accuracy: 0.3806\n",
      "Epoch 152/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6158 - accuracy: 0.4089\n",
      "Epoch 00152: val_loss did not improve from 1.64887\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6139 - accuracy: 0.4107 - val_loss: 1.6500 - val_accuracy: 0.3868\n",
      "Epoch 153/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6142 - accuracy: 0.4185\n",
      "Epoch 00153: val_loss did not improve from 1.64887\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6078 - accuracy: 0.4196 - val_loss: 1.6499 - val_accuracy: 0.3943\n",
      "Epoch 154/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6111 - accuracy: 0.4116\n",
      "Epoch 00154: val_loss improved from 1.64887 to 1.64842, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6111 - accuracy: 0.4139 - val_loss: 1.6484 - val_accuracy: 0.3922\n",
      "Epoch 155/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6076 - accuracy: 0.4133\n",
      "Epoch 00155: val_loss improved from 1.64842 to 1.64372, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6117 - accuracy: 0.4158 - val_loss: 1.6437 - val_accuracy: 0.3990\n",
      "Epoch 156/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6023 - accuracy: 0.4217\n",
      "Epoch 00156: val_loss improved from 1.64372 to 1.64249, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6042 - accuracy: 0.4192 - val_loss: 1.6425 - val_accuracy: 0.3950\n",
      "Epoch 157/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.6119 - accuracy: 0.4190\n",
      "Epoch 00157: val_loss did not improve from 1.64249\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6067 - accuracy: 0.4231 - val_loss: 1.6445 - val_accuracy: 0.3881\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6057 - accuracy: 0.4202\n",
      "Epoch 00158: val_loss improved from 1.64249 to 1.63914, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6037 - accuracy: 0.4194 - val_loss: 1.6391 - val_accuracy: 0.3984\n",
      "Epoch 159/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6006 - accuracy: 0.4302\n",
      "Epoch 00159: val_loss did not improve from 1.63914\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6011 - accuracy: 0.4284 - val_loss: 1.6401 - val_accuracy: 0.3956\n",
      "Epoch 160/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5996 - accuracy: 0.4171\n",
      "Epoch 00160: val_loss improved from 1.63914 to 1.63662, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6011 - accuracy: 0.4160 - val_loss: 1.6366 - val_accuracy: 0.4011\n",
      "Epoch 161/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6026 - accuracy: 0.4201\n",
      "Epoch 00161: val_loss did not improve from 1.63662\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5994 - accuracy: 0.4211 - val_loss: 1.6413 - val_accuracy: 0.3943\n",
      "Epoch 162/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5988 - accuracy: 0.4219\n",
      "Epoch 00162: val_loss improved from 1.63662 to 1.63418, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6000 - accuracy: 0.4206 - val_loss: 1.6342 - val_accuracy: 0.3990\n",
      "Epoch 163/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5949 - accuracy: 0.4295\n",
      "Epoch 00163: val_loss improved from 1.63418 to 1.63204, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5938 - accuracy: 0.4306 - val_loss: 1.6320 - val_accuracy: 0.4052\n",
      "Epoch 164/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5984 - accuracy: 0.4171\n",
      "Epoch 00164: val_loss did not improve from 1.63204\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5968 - accuracy: 0.4184 - val_loss: 1.6327 - val_accuracy: 0.4079\n",
      "Epoch 165/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5999 - accuracy: 0.4234\n",
      "Epoch 00165: val_loss improved from 1.63204 to 1.63012, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5978 - accuracy: 0.4247 - val_loss: 1.6301 - val_accuracy: 0.4052\n",
      "Epoch 166/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6004 - accuracy: 0.4201\n",
      "Epoch 00166: val_loss improved from 1.63012 to 1.62807, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5958 - accuracy: 0.4226 - val_loss: 1.6281 - val_accuracy: 0.4004\n",
      "Epoch 167/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5883 - accuracy: 0.4300\n",
      "Epoch 00167: val_loss improved from 1.62807 to 1.62621, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5887 - accuracy: 0.4296 - val_loss: 1.6262 - val_accuracy: 0.4031\n",
      "Epoch 168/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5917 - accuracy: 0.4213\n",
      "Epoch 00168: val_loss improved from 1.62621 to 1.62488, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5920 - accuracy: 0.4204 - val_loss: 1.6249 - val_accuracy: 0.4059\n",
      "Epoch 169/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5949 - accuracy: 0.4286\n",
      "Epoch 00169: val_loss did not improve from 1.62488\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5893 - accuracy: 0.4293 - val_loss: 1.6265 - val_accuracy: 0.4011\n",
      "Epoch 170/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5822 - accuracy: 0.4310\n",
      "Epoch 00170: val_loss did not improve from 1.62488\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5857 - accuracy: 0.4300 - val_loss: 1.6284 - val_accuracy: 0.3950\n",
      "Epoch 171/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5884 - accuracy: 0.4280\n",
      "Epoch 00171: val_loss improved from 1.62488 to 1.62323, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5911 - accuracy: 0.4281 - val_loss: 1.6232 - val_accuracy: 0.4106\n",
      "Epoch 172/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5904 - accuracy: 0.4293\n",
      "Epoch 00172: val_loss improved from 1.62323 to 1.62095, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5841 - accuracy: 0.4320 - val_loss: 1.6210 - val_accuracy: 0.4052\n",
      "Epoch 173/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5877 - accuracy: 0.4228\n",
      "Epoch 00173: val_loss improved from 1.62095 to 1.62002, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5841 - accuracy: 0.4235 - val_loss: 1.6200 - val_accuracy: 0.4079\n",
      "Epoch 174/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5804 - accuracy: 0.4342\n",
      "Epoch 00174: val_loss did not improve from 1.62002\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5838 - accuracy: 0.4342 - val_loss: 1.6266 - val_accuracy: 0.3956\n",
      "Epoch 175/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5864 - accuracy: 0.4319\n",
      "Epoch 00175: val_loss improved from 1.62002 to 1.61785, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5844 - accuracy: 0.4320 - val_loss: 1.6179 - val_accuracy: 0.4052\n",
      "Epoch 176/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5806 - accuracy: 0.4295\n",
      "Epoch 00176: val_loss improved from 1.61785 to 1.61780, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5815 - accuracy: 0.4284 - val_loss: 1.6178 - val_accuracy: 0.3943\n",
      "Epoch 177/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5895 - accuracy: 0.4226\n",
      "Epoch 00177: val_loss did not improve from 1.61780\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5859 - accuracy: 0.4240 - val_loss: 1.6182 - val_accuracy: 0.3943\n",
      "Epoch 178/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5775 - accuracy: 0.4272\n",
      "Epoch 00178: val_loss did not improve from 1.61780\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5781 - accuracy: 0.4262 - val_loss: 1.6270 - val_accuracy: 0.3984\n",
      "Epoch 179/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5729 - accuracy: 0.4379\n",
      "Epoch 00179: val_loss did not improve from 1.61780\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5756 - accuracy: 0.4358 - val_loss: 1.6182 - val_accuracy: 0.3950\n",
      "Epoch 180/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5726 - accuracy: 0.4334\n",
      "Epoch 00180: val_loss did not improve from 1.61780\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5761 - accuracy: 0.4312 - val_loss: 1.6198 - val_accuracy: 0.4072\n",
      "Epoch 181/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5799 - accuracy: 0.4295\n",
      "Epoch 00181: val_loss did not improve from 1.61780\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5795 - accuracy: 0.4301 - val_loss: 1.6208 - val_accuracy: 0.4045\n",
      "Epoch 182/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5760 - accuracy: 0.4323\n",
      "Epoch 00182: val_loss improved from 1.61780 to 1.61404, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5766 - accuracy: 0.4330 - val_loss: 1.6140 - val_accuracy: 0.4106\n",
      "Epoch 183/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5772 - accuracy: 0.4314\n",
      "Epoch 00183: val_loss improved from 1.61404 to 1.61344, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5752 - accuracy: 0.4332 - val_loss: 1.6134 - val_accuracy: 0.4147\n",
      "Epoch 184/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 1.5720 - accuracy: 0.4289\n",
      "Epoch 00184: val_loss improved from 1.61344 to 1.61206, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5720 - accuracy: 0.4289 - val_loss: 1.6121 - val_accuracy: 0.4031\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5695 - accuracy: 0.4417\n",
      "Epoch 00185: val_loss improved from 1.61206 to 1.61001, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5695 - accuracy: 0.4417 - val_loss: 1.6100 - val_accuracy: 0.4038\n",
      "Epoch 186/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5687 - accuracy: 0.4363\n",
      "Epoch 00186: val_loss improved from 1.61001 to 1.60979, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5691 - accuracy: 0.4349 - val_loss: 1.6098 - val_accuracy: 0.3950\n",
      "Epoch 187/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5704 - accuracy: 0.4352\n",
      "Epoch 00187: val_loss improved from 1.60979 to 1.60873, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5671 - accuracy: 0.4378 - val_loss: 1.6087 - val_accuracy: 0.4045\n",
      "Epoch 188/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5698 - accuracy: 0.4364\n",
      "Epoch 00188: val_loss did not improve from 1.60873\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5695 - accuracy: 0.4361 - val_loss: 1.6126 - val_accuracy: 0.4141\n",
      "Epoch 189/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5669 - accuracy: 0.4342\n",
      "Epoch 00189: val_loss did not improve from 1.60873\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5682 - accuracy: 0.4313 - val_loss: 1.6100 - val_accuracy: 0.4011\n",
      "Epoch 190/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5624 - accuracy: 0.4369\n",
      "Epoch 00190: val_loss improved from 1.60873 to 1.60786, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.5611 - accuracy: 0.4370 - val_loss: 1.6079 - val_accuracy: 0.4059\n",
      "Epoch 191/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5626 - accuracy: 0.4354\n",
      "Epoch 00191: val_loss did not improve from 1.60786\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5621 - accuracy: 0.4375 - val_loss: 1.6182 - val_accuracy: 0.3936\n",
      "Epoch 192/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5601 - accuracy: 0.4377\n",
      "Epoch 00192: val_loss did not improve from 1.60786\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5647 - accuracy: 0.4361 - val_loss: 1.6105 - val_accuracy: 0.4147\n",
      "Epoch 193/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5649 - accuracy: 0.4438\n",
      "Epoch 00193: val_loss improved from 1.60786 to 1.60419, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5616 - accuracy: 0.4438 - val_loss: 1.6042 - val_accuracy: 0.4031\n",
      "Epoch 194/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5605 - accuracy: 0.4382\n",
      "Epoch 00194: val_loss improved from 1.60419 to 1.60067, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5600 - accuracy: 0.4383 - val_loss: 1.6007 - val_accuracy: 0.4188\n",
      "Epoch 195/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5516 - accuracy: 0.4457\n",
      "Epoch 00195: val_loss improved from 1.60067 to 1.59612, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5549 - accuracy: 0.4441 - val_loss: 1.5961 - val_accuracy: 0.4106\n",
      "Epoch 196/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5630 - accuracy: 0.4382\n",
      "Epoch 00196: val_loss improved from 1.59612 to 1.59242, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5595 - accuracy: 0.4382 - val_loss: 1.5924 - val_accuracy: 0.4243\n",
      "Epoch 197/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5622 - accuracy: 0.4399\n",
      "Epoch 00197: val_loss improved from 1.59242 to 1.59150, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5566 - accuracy: 0.4421 - val_loss: 1.5915 - val_accuracy: 0.4120\n",
      "Epoch 198/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5514 - accuracy: 0.4375\n",
      "Epoch 00198: val_loss improved from 1.59150 to 1.58917, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5548 - accuracy: 0.4376 - val_loss: 1.5892 - val_accuracy: 0.4202\n",
      "Epoch 199/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5537 - accuracy: 0.4398\n",
      "Epoch 00199: val_loss did not improve from 1.58917\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5544 - accuracy: 0.4409 - val_loss: 1.5967 - val_accuracy: 0.4154\n",
      "Epoch 200/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5535 - accuracy: 0.4401\n",
      "Epoch 00200: val_loss did not improve from 1.58917\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5556 - accuracy: 0.4402 - val_loss: 1.5961 - val_accuracy: 0.4243\n",
      "Epoch 201/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5486 - accuracy: 0.4481\n",
      "Epoch 00201: val_loss did not improve from 1.58917\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5490 - accuracy: 0.4469 - val_loss: 1.5898 - val_accuracy: 0.4209\n",
      "Epoch 202/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5459 - accuracy: 0.4475\n",
      "Epoch 00202: val_loss did not improve from 1.58917\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5451 - accuracy: 0.4501 - val_loss: 1.5947 - val_accuracy: 0.4202\n",
      "Epoch 203/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5475 - accuracy: 0.4479\n",
      "Epoch 00203: val_loss improved from 1.58917 to 1.58882, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5502 - accuracy: 0.4470 - val_loss: 1.5888 - val_accuracy: 0.4018\n",
      "Epoch 204/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5530 - accuracy: 0.4399\n",
      "Epoch 00204: val_loss improved from 1.58882 to 1.58653, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5487 - accuracy: 0.4424 - val_loss: 1.5865 - val_accuracy: 0.4134\n",
      "Epoch 205/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5522 - accuracy: 0.4474\n",
      "Epoch 00205: val_loss did not improve from 1.58653\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5471 - accuracy: 0.4450 - val_loss: 1.5873 - val_accuracy: 0.4086\n",
      "Epoch 206/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5444 - accuracy: 0.4485\n",
      "Epoch 00206: val_loss did not improve from 1.58653\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5454 - accuracy: 0.4479 - val_loss: 1.5873 - val_accuracy: 0.4168\n",
      "Epoch 207/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5453 - accuracy: 0.4429\n",
      "Epoch 00207: val_loss did not improve from 1.58653\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5461 - accuracy: 0.4424 - val_loss: 1.5977 - val_accuracy: 0.4127\n",
      "Epoch 208/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5496 - accuracy: 0.4442\n",
      "Epoch 00208: val_loss improved from 1.58653 to 1.58367, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5435 - accuracy: 0.4472 - val_loss: 1.5837 - val_accuracy: 0.4222\n",
      "Epoch 209/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5406 - accuracy: 0.4431\n",
      "Epoch 00209: val_loss improved from 1.58367 to 1.58151, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5429 - accuracy: 0.4419 - val_loss: 1.5815 - val_accuracy: 0.4188\n",
      "Epoch 210/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5448 - accuracy: 0.4432\n",
      "Epoch 00210: val_loss did not improve from 1.58151\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5415 - accuracy: 0.4445 - val_loss: 1.5884 - val_accuracy: 0.4243\n",
      "Epoch 211/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5419 - accuracy: 0.4513\n",
      "Epoch 00211: val_loss did not improve from 1.58151\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5397 - accuracy: 0.4511 - val_loss: 1.5815 - val_accuracy: 0.4175\n",
      "Epoch 212/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5292 - accuracy: 0.4478\n",
      "Epoch 00212: val_loss did not improve from 1.58151\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5387 - accuracy: 0.4453 - val_loss: 1.5839 - val_accuracy: 0.4031\n",
      "Epoch 213/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5376 - accuracy: 0.4513\n",
      "Epoch 00213: val_loss improved from 1.58151 to 1.58117, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5361 - accuracy: 0.4511 - val_loss: 1.5812 - val_accuracy: 0.4195\n",
      "Epoch 214/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5332 - accuracy: 0.4551\n",
      "Epoch 00214: val_loss improved from 1.58117 to 1.58114, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5345 - accuracy: 0.4533 - val_loss: 1.5811 - val_accuracy: 0.4100\n",
      "Epoch 215/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5317 - accuracy: 0.4521\n",
      "Epoch 00215: val_loss improved from 1.58114 to 1.57776, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5336 - accuracy: 0.4482 - val_loss: 1.5778 - val_accuracy: 0.4052\n",
      "Epoch 216/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5299 - accuracy: 0.4526\n",
      "Epoch 00216: val_loss did not improve from 1.57776\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5309 - accuracy: 0.4527 - val_loss: 1.5890 - val_accuracy: 0.3997\n",
      "Epoch 217/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5309 - accuracy: 0.4515\n",
      "Epoch 00217: val_loss did not improve from 1.57776\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5353 - accuracy: 0.4498 - val_loss: 1.5807 - val_accuracy: 0.4161\n",
      "Epoch 218/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5353 - accuracy: 0.4468\n",
      "Epoch 00218: val_loss improved from 1.57776 to 1.57437, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5360 - accuracy: 0.4486 - val_loss: 1.5744 - val_accuracy: 0.4209\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5299 - accuracy: 0.4549\n",
      "Epoch 00219: val_loss did not improve from 1.57437\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5299 - accuracy: 0.4549 - val_loss: 1.5749 - val_accuracy: 0.4161\n",
      "Epoch 220/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5282 - accuracy: 0.4600\n",
      "Epoch 00220: val_loss did not improve from 1.57437\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5326 - accuracy: 0.4590 - val_loss: 1.5830 - val_accuracy: 0.4311\n",
      "Epoch 221/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5318 - accuracy: 0.4509\n",
      "Epoch 00221: val_loss improved from 1.57437 to 1.56796, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5330 - accuracy: 0.4501 - val_loss: 1.5680 - val_accuracy: 0.4236\n",
      "Epoch 222/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5351 - accuracy: 0.4574\n",
      "Epoch 00222: val_loss did not improve from 1.56796\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5276 - accuracy: 0.4605 - val_loss: 1.5811 - val_accuracy: 0.4011\n",
      "Epoch 223/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5334 - accuracy: 0.4488\n",
      "Epoch 00223: val_loss did not improve from 1.56796\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5331 - accuracy: 0.4492 - val_loss: 1.5788 - val_accuracy: 0.4093\n",
      "Epoch 224/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5199 - accuracy: 0.4526\n",
      "Epoch 00224: val_loss did not improve from 1.56796\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5270 - accuracy: 0.4516 - val_loss: 1.5778 - val_accuracy: 0.4154\n",
      "Epoch 225/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5266 - accuracy: 0.4559\n",
      "Epoch 00225: val_loss did not improve from 1.56796\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5244 - accuracy: 0.4566 - val_loss: 1.5699 - val_accuracy: 0.4311\n",
      "Epoch 226/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.5324 - accuracy: 0.4523\n",
      "Epoch 00226: val_loss improved from 1.56796 to 1.56741, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5239 - accuracy: 0.4571 - val_loss: 1.5674 - val_accuracy: 0.4229\n",
      "Epoch 227/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5223 - accuracy: 0.4533\n",
      "Epoch 00227: val_loss improved from 1.56741 to 1.56363, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5195 - accuracy: 0.4530 - val_loss: 1.5636 - val_accuracy: 0.4434\n",
      "Epoch 228/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5261 - accuracy: 0.4521\n",
      "Epoch 00228: val_loss did not improve from 1.56363\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5250 - accuracy: 0.4545 - val_loss: 1.5651 - val_accuracy: 0.4338\n",
      "Epoch 229/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5242 - accuracy: 0.4533\n",
      "Epoch 00229: val_loss did not improve from 1.56363\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5204 - accuracy: 0.4538 - val_loss: 1.5759 - val_accuracy: 0.4236\n",
      "Epoch 230/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5234 - accuracy: 0.4576\n",
      "Epoch 00230: val_loss did not improve from 1.56363\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5205 - accuracy: 0.4588 - val_loss: 1.5636 - val_accuracy: 0.4175\n",
      "Epoch 231/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5190 - accuracy: 0.4587\n",
      "Epoch 00231: val_loss improved from 1.56363 to 1.55847, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5161 - accuracy: 0.4596 - val_loss: 1.5585 - val_accuracy: 0.4277\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5214 - accuracy: 0.4562\n",
      "Epoch 00232: val_loss did not improve from 1.55847\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5214 - accuracy: 0.4562 - val_loss: 1.5597 - val_accuracy: 0.4168\n",
      "Epoch 233/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5089 - accuracy: 0.4636\n",
      "Epoch 00233: val_loss improved from 1.55847 to 1.55615, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5166 - accuracy: 0.4578 - val_loss: 1.5561 - val_accuracy: 0.4379\n",
      "Epoch 234/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5146 - accuracy: 0.4693\n",
      "Epoch 00234: val_loss did not improve from 1.55615\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5134 - accuracy: 0.4699 - val_loss: 1.5562 - val_accuracy: 0.4222\n",
      "Epoch 235/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5130 - accuracy: 0.4583\n",
      "Epoch 00235: val_loss improved from 1.55615 to 1.55423, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5125 - accuracy: 0.4583 - val_loss: 1.5542 - val_accuracy: 0.4461\n",
      "Epoch 236/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5159 - accuracy: 0.4580\n",
      "Epoch 00236: val_loss did not improve from 1.55423\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5150 - accuracy: 0.4579 - val_loss: 1.5561 - val_accuracy: 0.4229\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5105 - accuracy: 0.4591\n",
      "Epoch 00237: val_loss did not improve from 1.55423\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5110 - accuracy: 0.4607 - val_loss: 1.5547 - val_accuracy: 0.4229\n",
      "Epoch 238/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5131 - accuracy: 0.4609\n",
      "Epoch 00238: val_loss improved from 1.55423 to 1.55377, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5134 - accuracy: 0.4610 - val_loss: 1.5538 - val_accuracy: 0.4393\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5116 - accuracy: 0.4596\n",
      "Epoch 00239: val_loss did not improve from 1.55377\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5116 - accuracy: 0.4596 - val_loss: 1.5569 - val_accuracy: 0.4359\n",
      "Epoch 240/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.5000 - accuracy: 0.4624\n",
      "Epoch 00240: val_loss did not improve from 1.55377\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5068 - accuracy: 0.4643 - val_loss: 1.5562 - val_accuracy: 0.4345\n",
      "Epoch 241/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5139 - accuracy: 0.4632\n",
      "Epoch 00241: val_loss improved from 1.55377 to 1.55247, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5085 - accuracy: 0.4641 - val_loss: 1.5525 - val_accuracy: 0.4400\n",
      "Epoch 242/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5044 - accuracy: 0.4627\n",
      "Epoch 00242: val_loss did not improve from 1.55247\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5066 - accuracy: 0.4629 - val_loss: 1.5595 - val_accuracy: 0.4291\n",
      "Epoch 243/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5086 - accuracy: 0.4594\n",
      "Epoch 00243: val_loss did not improve from 1.55247\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5078 - accuracy: 0.4602 - val_loss: 1.5549 - val_accuracy: 0.4236\n",
      "Epoch 244/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5125 - accuracy: 0.4648\n",
      "Epoch 00244: val_loss improved from 1.55247 to 1.54520, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.5080 - accuracy: 0.4660 - val_loss: 1.5452 - val_accuracy: 0.4297\n",
      "Epoch 245/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5063 - accuracy: 0.4660\n",
      "Epoch 00245: val_loss did not improve from 1.54520\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5065 - accuracy: 0.4637 - val_loss: 1.5503 - val_accuracy: 0.4325\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5000 - accuracy: 0.4724\n",
      "Epoch 00246: val_loss improved from 1.54520 to 1.54467, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5000 - accuracy: 0.4724 - val_loss: 1.5447 - val_accuracy: 0.4400\n",
      "Epoch 247/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5038 - accuracy: 0.4620\n",
      "Epoch 00247: val_loss improved from 1.54467 to 1.54388, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5037 - accuracy: 0.4661 - val_loss: 1.5439 - val_accuracy: 0.4270\n",
      "Epoch 248/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4996 - accuracy: 0.4665\n",
      "Epoch 00248: val_loss did not improve from 1.54388\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4987 - accuracy: 0.4684 - val_loss: 1.5459 - val_accuracy: 0.4379\n",
      "Epoch 249/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4994 - accuracy: 0.4701\n",
      "Epoch 00249: val_loss did not improve from 1.54388\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4998 - accuracy: 0.4709 - val_loss: 1.5510 - val_accuracy: 0.4270\n",
      "Epoch 250/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4979 - accuracy: 0.4632\n",
      "Epoch 00250: val_loss did not improve from 1.54388\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5021 - accuracy: 0.4619 - val_loss: 1.5468 - val_accuracy: 0.4325\n",
      "Epoch 251/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4958 - accuracy: 0.4714\n",
      "Epoch 00251: val_loss did not improve from 1.54388\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4995 - accuracy: 0.4687 - val_loss: 1.5459 - val_accuracy: 0.4413\n",
      "Epoch 252/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4977 - accuracy: 0.4684\n",
      "Epoch 00252: val_loss did not improve from 1.54388\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4978 - accuracy: 0.4682 - val_loss: 1.5508 - val_accuracy: 0.4332\n",
      "Epoch 253/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5008 - accuracy: 0.4677\n",
      "Epoch 00253: val_loss improved from 1.54388 to 1.54315, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4954 - accuracy: 0.4699 - val_loss: 1.5432 - val_accuracy: 0.4338\n",
      "Epoch 254/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4939 - accuracy: 0.4714\n",
      "Epoch 00254: val_loss improved from 1.54315 to 1.54253, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4934 - accuracy: 0.4726 - val_loss: 1.5425 - val_accuracy: 0.4550\n",
      "Epoch 255/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.5001 - accuracy: 0.4716\n",
      "Epoch 00255: val_loss improved from 1.54253 to 1.53514, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4962 - accuracy: 0.4723 - val_loss: 1.5351 - val_accuracy: 0.4427\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4953 - accuracy: 0.4682\n",
      "Epoch 00256: val_loss did not improve from 1.53514\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4953 - accuracy: 0.4682 - val_loss: 1.5386 - val_accuracy: 0.4366\n",
      "Epoch 257/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4931 - accuracy: 0.4727\n",
      "Epoch 00257: val_loss did not improve from 1.53514\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4938 - accuracy: 0.4711 - val_loss: 1.5411 - val_accuracy: 0.4277\n",
      "Epoch 258/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4890 - accuracy: 0.4751\n",
      "Epoch 00258: val_loss did not improve from 1.53514\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4925 - accuracy: 0.4743 - val_loss: 1.5491 - val_accuracy: 0.4222\n",
      "Epoch 259/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4913 - accuracy: 0.4667\n",
      "Epoch 00259: val_loss did not improve from 1.53514\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4933 - accuracy: 0.4648 - val_loss: 1.5360 - val_accuracy: 0.4495\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4892 - accuracy: 0.4755\n",
      "Epoch 00260: val_loss improved from 1.53514 to 1.53398, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4892 - accuracy: 0.4755 - val_loss: 1.5340 - val_accuracy: 0.4407\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4911 - accuracy: 0.4742\n",
      "Epoch 00261: val_loss improved from 1.53398 to 1.53098, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4911 - accuracy: 0.4742 - val_loss: 1.5310 - val_accuracy: 0.4447\n",
      "Epoch 262/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4901 - accuracy: 0.4748\n",
      "Epoch 00262: val_loss did not improve from 1.53098\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4900 - accuracy: 0.4757 - val_loss: 1.5362 - val_accuracy: 0.4488\n",
      "Epoch 263/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4868 - accuracy: 0.4805\n",
      "Epoch 00263: val_loss did not improve from 1.53098\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4861 - accuracy: 0.4788 - val_loss: 1.5315 - val_accuracy: 0.4529\n",
      "Epoch 264/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4939 - accuracy: 0.4732\n",
      "Epoch 00264: val_loss did not improve from 1.53098\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4879 - accuracy: 0.4735 - val_loss: 1.5348 - val_accuracy: 0.4304\n",
      "Epoch 265/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4847 - accuracy: 0.4732\n",
      "Epoch 00265: val_loss did not improve from 1.53098\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4838 - accuracy: 0.4743 - val_loss: 1.5386 - val_accuracy: 0.4475\n",
      "Epoch 266/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4874 - accuracy: 0.4779\n",
      "Epoch 00266: val_loss improved from 1.53098 to 1.52976, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4867 - accuracy: 0.4757 - val_loss: 1.5298 - val_accuracy: 0.4366\n",
      "Epoch 267/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4907 - accuracy: 0.4750\n",
      "Epoch 00267: val_loss improved from 1.52976 to 1.52477, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4863 - accuracy: 0.4757 - val_loss: 1.5248 - val_accuracy: 0.4386\n",
      "Epoch 268/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4853 - accuracy: 0.4693\n",
      "Epoch 00268: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4821 - accuracy: 0.4719 - val_loss: 1.5300 - val_accuracy: 0.4413\n",
      "Epoch 269/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4861 - accuracy: 0.4721\n",
      "Epoch 00269: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4871 - accuracy: 0.4726 - val_loss: 1.5329 - val_accuracy: 0.4461\n",
      "Epoch 270/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4857 - accuracy: 0.4754\n",
      "Epoch 00270: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4865 - accuracy: 0.4753 - val_loss: 1.5293 - val_accuracy: 0.4488\n",
      "Epoch 271/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4854 - accuracy: 0.4771\n",
      "Epoch 00271: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4822 - accuracy: 0.4776 - val_loss: 1.5260 - val_accuracy: 0.4529\n",
      "Epoch 272/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4803 - accuracy: 0.4769\n",
      "Epoch 00272: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4808 - accuracy: 0.4777 - val_loss: 1.5263 - val_accuracy: 0.4386\n",
      "Epoch 273/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4824 - accuracy: 0.4717\n",
      "Epoch 00273: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4816 - accuracy: 0.4723 - val_loss: 1.5272 - val_accuracy: 0.4400\n",
      "Epoch 274/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4821 - accuracy: 0.4790\n",
      "Epoch 00274: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4825 - accuracy: 0.4772 - val_loss: 1.5288 - val_accuracy: 0.4488\n",
      "Epoch 275/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4818 - accuracy: 0.4789\n",
      "Epoch 00275: val_loss did not improve from 1.52477\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4809 - accuracy: 0.4803 - val_loss: 1.5255 - val_accuracy: 0.4427\n",
      "Epoch 276/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4860 - accuracy: 0.4786\n",
      "Epoch 00276: val_loss improved from 1.52477 to 1.52048, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4764 - accuracy: 0.4827 - val_loss: 1.5205 - val_accuracy: 0.4420\n",
      "Epoch 277/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4758 - accuracy: 0.4796\n",
      "Epoch 00277: val_loss did not improve from 1.52048\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4775 - accuracy: 0.4776 - val_loss: 1.5239 - val_accuracy: 0.4529\n",
      "Epoch 278/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4745 - accuracy: 0.4781\n",
      "Epoch 00278: val_loss did not improve from 1.52048\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4728 - accuracy: 0.4777 - val_loss: 1.5224 - val_accuracy: 0.4523\n",
      "Epoch 279/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4823 - accuracy: 0.4782\n",
      "Epoch 00279: val_loss did not improve from 1.52048\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4776 - accuracy: 0.4791 - val_loss: 1.5258 - val_accuracy: 0.4482\n",
      "Epoch 280/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4702 - accuracy: 0.4870\n",
      "Epoch 00280: val_loss improved from 1.52048 to 1.52029, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4758 - accuracy: 0.4825 - val_loss: 1.5203 - val_accuracy: 0.4372\n",
      "Epoch 281/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4740 - accuracy: 0.4780\n",
      "Epoch 00281: val_loss did not improve from 1.52029\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4774 - accuracy: 0.4782 - val_loss: 1.5230 - val_accuracy: 0.4386\n",
      "Epoch 282/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4714 - accuracy: 0.4846\n",
      "Epoch 00282: val_loss improved from 1.52029 to 1.51735, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4728 - accuracy: 0.4837 - val_loss: 1.5174 - val_accuracy: 0.4400\n",
      "Epoch 283/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4751 - accuracy: 0.4801\n",
      "Epoch 00283: val_loss improved from 1.51735 to 1.51546, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4729 - accuracy: 0.4822 - val_loss: 1.5155 - val_accuracy: 0.4509\n",
      "Epoch 284/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4731 - accuracy: 0.4789\n",
      "Epoch 00284: val_loss did not improve from 1.51546\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4739 - accuracy: 0.4818 - val_loss: 1.5159 - val_accuracy: 0.4563\n",
      "Epoch 285/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4679 - accuracy: 0.4883\n",
      "Epoch 00285: val_loss improved from 1.51546 to 1.51461, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4699 - accuracy: 0.4876 - val_loss: 1.5146 - val_accuracy: 0.4516\n",
      "Epoch 286/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4736 - accuracy: 0.4801\n",
      "Epoch 00286: val_loss did not improve from 1.51461\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4703 - accuracy: 0.4815 - val_loss: 1.5149 - val_accuracy: 0.4407\n",
      "Epoch 287/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4720 - accuracy: 0.4803\n",
      "Epoch 00287: val_loss did not improve from 1.51461\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4707 - accuracy: 0.4786 - val_loss: 1.5158 - val_accuracy: 0.4516\n",
      "Epoch 288/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4676 - accuracy: 0.4893\n",
      "Epoch 00288: val_loss did not improve from 1.51461\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4656 - accuracy: 0.4893 - val_loss: 1.5228 - val_accuracy: 0.4482\n",
      "Epoch 289/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4723 - accuracy: 0.4814\n",
      "Epoch 00289: val_loss improved from 1.51461 to 1.51322, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4695 - accuracy: 0.4823 - val_loss: 1.5132 - val_accuracy: 0.4400\n",
      "Epoch 290/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4692 - accuracy: 0.4854\n",
      "Epoch 00290: val_loss did not improve from 1.51322\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4665 - accuracy: 0.4863 - val_loss: 1.5197 - val_accuracy: 0.4338\n",
      "Epoch 291/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4772 - accuracy: 0.4833\n",
      "Epoch 00291: val_loss did not improve from 1.51322\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4714 - accuracy: 0.4849 - val_loss: 1.5175 - val_accuracy: 0.4366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4621 - accuracy: 0.4810\n",
      "Epoch 00292: val_loss improved from 1.51322 to 1.51126, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4695 - accuracy: 0.4813 - val_loss: 1.5113 - val_accuracy: 0.4604\n",
      "Epoch 293/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4711 - accuracy: 0.4831\n",
      "Epoch 00293: val_loss improved from 1.51126 to 1.51017, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4680 - accuracy: 0.4859 - val_loss: 1.5102 - val_accuracy: 0.4536\n",
      "Epoch 294/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4616 - accuracy: 0.4890\n",
      "Epoch 00294: val_loss improved from 1.51017 to 1.50785, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4623 - accuracy: 0.4888 - val_loss: 1.5078 - val_accuracy: 0.4536\n",
      "Epoch 295/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4641 - accuracy: 0.4824\n",
      "Epoch 00295: val_loss did not improve from 1.50785\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4646 - accuracy: 0.4832 - val_loss: 1.5123 - val_accuracy: 0.4529\n",
      "Epoch 296/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4623 - accuracy: 0.4875\n",
      "Epoch 00296: val_loss did not improve from 1.50785\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4636 - accuracy: 0.4846 - val_loss: 1.5093 - val_accuracy: 0.4427\n",
      "Epoch 297/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4660 - accuracy: 0.4849\n",
      "Epoch 00297: val_loss improved from 1.50785 to 1.50543, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4613 - accuracy: 0.4878 - val_loss: 1.5054 - val_accuracy: 0.4563\n",
      "Epoch 298/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4658 - accuracy: 0.4825\n",
      "Epoch 00298: val_loss did not improve from 1.50543\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4622 - accuracy: 0.4827 - val_loss: 1.5075 - val_accuracy: 0.4434\n",
      "Epoch 299/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4581 - accuracy: 0.4864\n",
      "Epoch 00299: val_loss did not improve from 1.50543\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4581 - accuracy: 0.4875 - val_loss: 1.5065 - val_accuracy: 0.4461\n",
      "Epoch 300/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4592 - accuracy: 0.4885\n",
      "Epoch 00300: val_loss did not improve from 1.50543\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4608 - accuracy: 0.4852 - val_loss: 1.5085 - val_accuracy: 0.4379\n",
      "Epoch 301/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4567 - accuracy: 0.4903\n",
      "Epoch 00301: val_loss improved from 1.50543 to 1.50488, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4589 - accuracy: 0.4905 - val_loss: 1.5049 - val_accuracy: 0.4420\n",
      "Epoch 302/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4580 - accuracy: 0.4836\n",
      "Epoch 00302: val_loss did not improve from 1.50488\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4589 - accuracy: 0.4846 - val_loss: 1.5066 - val_accuracy: 0.4523\n",
      "Epoch 303/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4541 - accuracy: 0.4887\n",
      "Epoch 00303: val_loss did not improve from 1.50488\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4581 - accuracy: 0.4871 - val_loss: 1.5050 - val_accuracy: 0.4427\n",
      "Epoch 304/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4490 - accuracy: 0.4927\n",
      "Epoch 00304: val_loss did not improve from 1.50488\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4556 - accuracy: 0.4892 - val_loss: 1.5082 - val_accuracy: 0.4495\n",
      "Epoch 305/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4550 - accuracy: 0.4894\n",
      "Epoch 00305: val_loss did not improve from 1.50488\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4548 - accuracy: 0.4880 - val_loss: 1.5067 - val_accuracy: 0.4529\n",
      "Epoch 306/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4517 - accuracy: 0.4918\n",
      "Epoch 00306: val_loss improved from 1.50488 to 1.50365, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4541 - accuracy: 0.4898 - val_loss: 1.5037 - val_accuracy: 0.4570\n",
      "Epoch 307/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4565 - accuracy: 0.4943\n",
      "Epoch 00307: val_loss improved from 1.50365 to 1.50199, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4545 - accuracy: 0.4941 - val_loss: 1.5020 - val_accuracy: 0.4563\n",
      "Epoch 308/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4608 - accuracy: 0.4867\n",
      "Epoch 00308: val_loss did not improve from 1.50199\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4580 - accuracy: 0.4875 - val_loss: 1.5028 - val_accuracy: 0.4591\n",
      "Epoch 309/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4523 - accuracy: 0.4940\n",
      "Epoch 00309: val_loss did not improve from 1.50199\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4537 - accuracy: 0.4926 - val_loss: 1.5032 - val_accuracy: 0.4529\n",
      "Epoch 310/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4527 - accuracy: 0.4915\n",
      "Epoch 00310: val_loss did not improve from 1.50199\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4533 - accuracy: 0.4905 - val_loss: 1.5027 - val_accuracy: 0.4529\n",
      "Epoch 311/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4589 - accuracy: 0.4857\n",
      "Epoch 00311: val_loss did not improve from 1.50199\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4569 - accuracy: 0.4846 - val_loss: 1.5049 - val_accuracy: 0.4570\n",
      "Epoch 312/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4583 - accuracy: 0.4853\n",
      "Epoch 00312: val_loss did not improve from 1.50199\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4563 - accuracy: 0.4881 - val_loss: 1.5072 - val_accuracy: 0.4352\n",
      "Epoch 313/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4534 - accuracy: 0.4786\n",
      "Epoch 00313: val_loss improved from 1.50199 to 1.49903, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4556 - accuracy: 0.4786 - val_loss: 1.4990 - val_accuracy: 0.4420\n",
      "Epoch 314/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4497 - accuracy: 0.4952\n",
      "Epoch 00314: val_loss improved from 1.49903 to 1.49860, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4531 - accuracy: 0.4914 - val_loss: 1.4986 - val_accuracy: 0.4468\n",
      "Epoch 315/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4510 - accuracy: 0.4876\n",
      "Epoch 00315: val_loss did not improve from 1.49860\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4542 - accuracy: 0.4854 - val_loss: 1.5077 - val_accuracy: 0.4598\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4529 - accuracy: 0.4965\n",
      "Epoch 00316: val_loss improved from 1.49860 to 1.49841, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4529 - accuracy: 0.4965 - val_loss: 1.4984 - val_accuracy: 0.4577\n",
      "Epoch 317/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4515 - accuracy: 0.4947\n",
      "Epoch 00317: val_loss improved from 1.49841 to 1.49480, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4534 - accuracy: 0.4929 - val_loss: 1.4948 - val_accuracy: 0.4516\n",
      "Epoch 318/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4445 - accuracy: 0.4896\n",
      "Epoch 00318: val_loss did not improve from 1.49480\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4485 - accuracy: 0.4895 - val_loss: 1.4992 - val_accuracy: 0.4516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4491 - accuracy: 0.4980\n",
      "Epoch 00319: val_loss improved from 1.49480 to 1.49110, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4513 - accuracy: 0.4972 - val_loss: 1.4911 - val_accuracy: 0.4604\n",
      "Epoch 320/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4451 - accuracy: 0.4948\n",
      "Epoch 00320: val_loss improved from 1.49110 to 1.49051, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4499 - accuracy: 0.4934 - val_loss: 1.4905 - val_accuracy: 0.4516\n",
      "Epoch 321/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4521 - accuracy: 0.4903\n",
      "Epoch 00321: val_loss did not improve from 1.49051\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4542 - accuracy: 0.4868 - val_loss: 1.4948 - val_accuracy: 0.4618\n",
      "Epoch 322/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4495 - accuracy: 0.4849\n",
      "Epoch 00322: val_loss did not improve from 1.49051\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4457 - accuracy: 0.4895 - val_loss: 1.4933 - val_accuracy: 0.4570\n",
      "Epoch 323/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4530 - accuracy: 0.4858\n",
      "Epoch 00323: val_loss did not improve from 1.49051\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4511 - accuracy: 0.4878 - val_loss: 1.4999 - val_accuracy: 0.4536\n",
      "Epoch 324/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4388 - accuracy: 0.4948\n",
      "Epoch 00324: val_loss did not improve from 1.49051\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4459 - accuracy: 0.4904 - val_loss: 1.5005 - val_accuracy: 0.4652\n",
      "Epoch 325/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4422 - accuracy: 0.4898\n",
      "Epoch 00325: val_loss did not improve from 1.49051\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4459 - accuracy: 0.4898 - val_loss: 1.4910 - val_accuracy: 0.4570\n",
      "Epoch 326/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4472 - accuracy: 0.4903\n",
      "Epoch 00326: val_loss did not improve from 1.49051\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4439 - accuracy: 0.4907 - val_loss: 1.4935 - val_accuracy: 0.4611\n",
      "Epoch 327/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4474 - accuracy: 0.4924\n",
      "Epoch 00327: val_loss improved from 1.49051 to 1.49007, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4463 - accuracy: 0.4921 - val_loss: 1.4901 - val_accuracy: 0.4536\n",
      "Epoch 328/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4419 - accuracy: 0.4950\n",
      "Epoch 00328: val_loss did not improve from 1.49007\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4444 - accuracy: 0.4946 - val_loss: 1.4902 - val_accuracy: 0.4638\n",
      "Epoch 329/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4383 - accuracy: 0.4952\n",
      "Epoch 00329: val_loss did not improve from 1.49007\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4404 - accuracy: 0.4939 - val_loss: 1.4904 - val_accuracy: 0.4495\n",
      "Epoch 330/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4370 - accuracy: 0.4979\n",
      "Epoch 00330: val_loss did not improve from 1.49007\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4408 - accuracy: 0.4941 - val_loss: 1.4902 - val_accuracy: 0.4625\n",
      "Epoch 331/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4477 - accuracy: 0.4885\n",
      "Epoch 00331: val_loss did not improve from 1.49007\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4448 - accuracy: 0.4909 - val_loss: 1.4926 - val_accuracy: 0.4563\n",
      "Epoch 332/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4456 - accuracy: 0.4836\n",
      "Epoch 00332: val_loss improved from 1.49007 to 1.48599, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4435 - accuracy: 0.4852 - val_loss: 1.4860 - val_accuracy: 0.4707\n",
      "Epoch 333/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4398 - accuracy: 0.4991\n",
      "Epoch 00333: val_loss did not improve from 1.48599\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4378 - accuracy: 0.5001 - val_loss: 1.4862 - val_accuracy: 0.4509\n",
      "Epoch 334/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4418 - accuracy: 0.4929\n",
      "Epoch 00334: val_loss improved from 1.48599 to 1.48334, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4393 - accuracy: 0.4931 - val_loss: 1.4833 - val_accuracy: 0.4686\n",
      "Epoch 335/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4403 - accuracy: 0.4929\n",
      "Epoch 00335: val_loss did not improve from 1.48334\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4418 - accuracy: 0.4910 - val_loss: 1.4881 - val_accuracy: 0.4638\n",
      "Epoch 336/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4407 - accuracy: 0.4898\n",
      "Epoch 00336: val_loss did not improve from 1.48334\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4399 - accuracy: 0.4893 - val_loss: 1.4841 - val_accuracy: 0.4679\n",
      "Epoch 337/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4442 - accuracy: 0.4935\n",
      "Epoch 00337: val_loss did not improve from 1.48334\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4376 - accuracy: 0.4963 - val_loss: 1.4847 - val_accuracy: 0.4523\n",
      "Epoch 338/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4380 - accuracy: 0.5011\n",
      "Epoch 00338: val_loss improved from 1.48334 to 1.48013, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4357 - accuracy: 0.5035 - val_loss: 1.4801 - val_accuracy: 0.4625\n",
      "Epoch 339/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4442 - accuracy: 0.4929\n",
      "Epoch 00339: val_loss improved from 1.48013 to 1.48000, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4425 - accuracy: 0.4924 - val_loss: 1.4800 - val_accuracy: 0.4618\n",
      "Epoch 340/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4273 - accuracy: 0.4981\n",
      "Epoch 00340: val_loss did not improve from 1.48000\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4370 - accuracy: 0.4951 - val_loss: 1.4851 - val_accuracy: 0.4577\n",
      "Epoch 341/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4328 - accuracy: 0.4965\n",
      "Epoch 00341: val_loss did not improve from 1.48000\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4368 - accuracy: 0.4955 - val_loss: 1.4926 - val_accuracy: 0.4577\n",
      "Epoch 342/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4455 - accuracy: 0.4942\n",
      "Epoch 00342: val_loss did not improve from 1.48000\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4372 - accuracy: 0.4960 - val_loss: 1.4872 - val_accuracy: 0.4707\n",
      "Epoch 343/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4341 - accuracy: 0.4991\n",
      "Epoch 00343: val_loss improved from 1.48000 to 1.47898, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4335 - accuracy: 0.4982 - val_loss: 1.4790 - val_accuracy: 0.4645\n",
      "Epoch 344/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4387 - accuracy: 0.4994\n",
      "Epoch 00344: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4390 - accuracy: 0.4938 - val_loss: 1.4831 - val_accuracy: 0.4638\n",
      "Epoch 345/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4288 - accuracy: 0.4975\n",
      "Epoch 00345: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4325 - accuracy: 0.4960 - val_loss: 1.4844 - val_accuracy: 0.4652\n",
      "Epoch 346/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4376 - accuracy: 0.4920\n",
      "Epoch 00346: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4385 - accuracy: 0.4922 - val_loss: 1.4819 - val_accuracy: 0.4652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4280 - accuracy: 0.5050\n",
      "Epoch 00347: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4334 - accuracy: 0.5006 - val_loss: 1.4823 - val_accuracy: 0.4761\n",
      "Epoch 348/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4323 - accuracy: 0.4994\n",
      "Epoch 00348: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4339 - accuracy: 0.4946 - val_loss: 1.4790 - val_accuracy: 0.4495\n",
      "Epoch 349/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4334 - accuracy: 0.4944\n",
      "Epoch 00349: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4359 - accuracy: 0.4936 - val_loss: 1.4792 - val_accuracy: 0.4598\n",
      "Epoch 350/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4362 - accuracy: 0.4970\n",
      "Epoch 00350: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4351 - accuracy: 0.4967 - val_loss: 1.4811 - val_accuracy: 0.4754\n",
      "Epoch 351/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4333 - accuracy: 0.4907\n",
      "Epoch 00351: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4356 - accuracy: 0.4929 - val_loss: 1.4836 - val_accuracy: 0.4693\n",
      "Epoch 352/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4306 - accuracy: 0.5004\n",
      "Epoch 00352: val_loss did not improve from 1.47898\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4337 - accuracy: 0.4985 - val_loss: 1.4790 - val_accuracy: 0.4666\n",
      "Epoch 353/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4378 - accuracy: 0.4939\n",
      "Epoch 00353: val_loss improved from 1.47898 to 1.47459, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4351 - accuracy: 0.4943 - val_loss: 1.4746 - val_accuracy: 0.4714\n",
      "Epoch 354/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4260 - accuracy: 0.5060\n",
      "Epoch 00354: val_loss did not improve from 1.47459\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4275 - accuracy: 0.5049 - val_loss: 1.4832 - val_accuracy: 0.4598\n",
      "Epoch 355/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4285 - accuracy: 0.5015\n",
      "Epoch 00355: val_loss did not improve from 1.47459\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4321 - accuracy: 0.4985 - val_loss: 1.4752 - val_accuracy: 0.4618\n",
      "Epoch 356/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4311 - accuracy: 0.4998\n",
      "Epoch 00356: val_loss did not improve from 1.47459\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4302 - accuracy: 0.4994 - val_loss: 1.4804 - val_accuracy: 0.4679\n",
      "Epoch 357/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4343 - accuracy: 0.4965\n",
      "Epoch 00357: val_loss did not improve from 1.47459\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4316 - accuracy: 0.4980 - val_loss: 1.4816 - val_accuracy: 0.4625\n",
      "Epoch 358/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4311 - accuracy: 0.4979\n",
      "Epoch 00358: val_loss improved from 1.47459 to 1.47299, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4327 - accuracy: 0.4980 - val_loss: 1.4730 - val_accuracy: 0.4659\n",
      "Epoch 359/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4276 - accuracy: 0.4983\n",
      "Epoch 00359: val_loss did not improve from 1.47299\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4263 - accuracy: 0.4977 - val_loss: 1.4764 - val_accuracy: 0.4823\n",
      "Epoch 360/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4245 - accuracy: 0.4989\n",
      "Epoch 00360: val_loss did not improve from 1.47299\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4287 - accuracy: 0.4974 - val_loss: 1.4815 - val_accuracy: 0.4686\n",
      "Epoch 361/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4346 - accuracy: 0.4963\n",
      "Epoch 00361: val_loss improved from 1.47299 to 1.47036, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4304 - accuracy: 0.4980 - val_loss: 1.4704 - val_accuracy: 0.4720\n",
      "Epoch 362/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4228 - accuracy: 0.5076\n",
      "Epoch 00362: val_loss improved from 1.47036 to 1.46995, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4261 - accuracy: 0.5038 - val_loss: 1.4699 - val_accuracy: 0.4611\n",
      "Epoch 363/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4267 - accuracy: 0.5030\n",
      "Epoch 00363: val_loss did not improve from 1.46995\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4278 - accuracy: 0.5009 - val_loss: 1.4702 - val_accuracy: 0.4598\n",
      "Epoch 364/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4264 - accuracy: 0.5020\n",
      "Epoch 00364: val_loss did not improve from 1.46995\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4260 - accuracy: 0.5018 - val_loss: 1.4765 - val_accuracy: 0.4707\n",
      "Epoch 365/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.4207 - accuracy: 0.5043\n",
      "Epoch 00365: val_loss did not improve from 1.46995\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4289 - accuracy: 0.4991 - val_loss: 1.4702 - val_accuracy: 0.4700\n",
      "Epoch 366/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4271 - accuracy: 0.5006\n",
      "Epoch 00366: val_loss improved from 1.46995 to 1.46888, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4265 - accuracy: 0.4997 - val_loss: 1.4689 - val_accuracy: 0.4666\n",
      "Epoch 367/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4250 - accuracy: 0.5089\n",
      "Epoch 00367: val_loss did not improve from 1.46888\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4255 - accuracy: 0.5098 - val_loss: 1.4707 - val_accuracy: 0.4584\n",
      "Epoch 368/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4267 - accuracy: 0.4968\n",
      "Epoch 00368: val_loss did not improve from 1.46888\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4270 - accuracy: 0.4979 - val_loss: 1.4860 - val_accuracy: 0.4557\n",
      "Epoch 369/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4267 - accuracy: 0.5022\n",
      "Epoch 00369: val_loss did not improve from 1.46888\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4255 - accuracy: 0.5013 - val_loss: 1.4703 - val_accuracy: 0.4645\n",
      "Epoch 370/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4247 - accuracy: 0.5033\n",
      "Epoch 00370: val_loss did not improve from 1.46888\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4238 - accuracy: 0.5040 - val_loss: 1.4748 - val_accuracy: 0.4700\n",
      "Epoch 371/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4237 - accuracy: 0.4998\n",
      "Epoch 00371: val_loss did not improve from 1.46888\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4253 - accuracy: 0.5023 - val_loss: 1.4741 - val_accuracy: 0.4789\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4184 - accuracy: 0.5084\n",
      "Epoch 00372: val_loss improved from 1.46888 to 1.46873, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4184 - accuracy: 0.5084 - val_loss: 1.4687 - val_accuracy: 0.4557\n",
      "Epoch 373/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4170 - accuracy: 0.5055\n",
      "Epoch 00373: val_loss did not improve from 1.46873\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4174 - accuracy: 0.5061 - val_loss: 1.4697 - val_accuracy: 0.4768\n",
      "Epoch 374/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4236 - accuracy: 0.5036\n",
      "Epoch 00374: val_loss improved from 1.46873 to 1.46216, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4214 - accuracy: 0.5042 - val_loss: 1.4622 - val_accuracy: 0.4693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4230 - accuracy: 0.5002\n",
      "Epoch 00375: val_loss did not improve from 1.46216\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4225 - accuracy: 0.5003 - val_loss: 1.4697 - val_accuracy: 0.4638\n",
      "Epoch 376/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4238 - accuracy: 0.5061\n",
      "Epoch 00376: val_loss did not improve from 1.46216\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4228 - accuracy: 0.5054 - val_loss: 1.4690 - val_accuracy: 0.4809\n",
      "Epoch 377/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4301 - accuracy: 0.4968\n",
      "Epoch 00377: val_loss did not improve from 1.46216\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4244 - accuracy: 0.5018 - val_loss: 1.4644 - val_accuracy: 0.4611\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4207 - accuracy: 0.5025\n",
      "Epoch 00378: val_loss improved from 1.46216 to 1.46083, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4207 - accuracy: 0.5025 - val_loss: 1.4608 - val_accuracy: 0.4679\n",
      "Epoch 379/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4206 - accuracy: 0.4988\n",
      "Epoch 00379: val_loss did not improve from 1.46083\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4221 - accuracy: 0.4968 - val_loss: 1.4652 - val_accuracy: 0.4645\n",
      "Epoch 380/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4158 - accuracy: 0.5080\n",
      "Epoch 00380: val_loss did not improve from 1.46083\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4152 - accuracy: 0.5069 - val_loss: 1.4728 - val_accuracy: 0.4570\n",
      "Epoch 381/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4228 - accuracy: 0.5045\n",
      "Epoch 00381: val_loss did not improve from 1.46083\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4205 - accuracy: 0.5049 - val_loss: 1.4648 - val_accuracy: 0.4754\n",
      "Epoch 382/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4158 - accuracy: 0.5050\n",
      "Epoch 00382: val_loss did not improve from 1.46083\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4169 - accuracy: 0.5028 - val_loss: 1.4655 - val_accuracy: 0.4754\n",
      "Epoch 383/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4187 - accuracy: 0.5039\n",
      "Epoch 00383: val_loss did not improve from 1.46083\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4173 - accuracy: 0.5050 - val_loss: 1.4681 - val_accuracy: 0.4645\n",
      "Epoch 384/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4249 - accuracy: 0.4998\n",
      "Epoch 00384: val_loss did not improve from 1.46083\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4214 - accuracy: 0.4987 - val_loss: 1.4629 - val_accuracy: 0.4673\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4225 - accuracy: 0.5013\n",
      "Epoch 00385: val_loss improved from 1.46083 to 1.45779, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4225 - accuracy: 0.5013 - val_loss: 1.4578 - val_accuracy: 0.4625\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4226 - accuracy: 0.5032\n",
      "Epoch 00386: val_loss did not improve from 1.45779\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4226 - accuracy: 0.5032 - val_loss: 1.4600 - val_accuracy: 0.4536\n",
      "Epoch 387/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4166 - accuracy: 0.5015\n",
      "Epoch 00387: val_loss did not improve from 1.45779\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4182 - accuracy: 0.5015 - val_loss: 1.4664 - val_accuracy: 0.4604\n",
      "Epoch 388/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4243 - accuracy: 0.4918\n",
      "Epoch 00388: val_loss did not improve from 1.45779\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4196 - accuracy: 0.4938 - val_loss: 1.4622 - val_accuracy: 0.4598\n",
      "Epoch 389/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4159 - accuracy: 0.5020\n",
      "Epoch 00389: val_loss did not improve from 1.45779\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4171 - accuracy: 0.5015 - val_loss: 1.4715 - val_accuracy: 0.4509\n",
      "Epoch 390/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4156 - accuracy: 0.4989\n",
      "Epoch 00390: val_loss did not improve from 1.45779\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4182 - accuracy: 0.4975 - val_loss: 1.4607 - val_accuracy: 0.4707\n",
      "Epoch 391/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4206 - accuracy: 0.5007\n",
      "Epoch 00391: val_loss improved from 1.45779 to 1.45730, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4157 - accuracy: 0.5015 - val_loss: 1.4573 - val_accuracy: 0.4693\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4129 - accuracy: 0.5059\n",
      "Epoch 00392: val_loss did not improve from 1.45730\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4129 - accuracy: 0.5059 - val_loss: 1.4578 - val_accuracy: 0.4761\n",
      "Epoch 393/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4077 - accuracy: 0.5091\n",
      "Epoch 00393: val_loss improved from 1.45730 to 1.45636, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4109 - accuracy: 0.5093 - val_loss: 1.4564 - val_accuracy: 0.4720\n",
      "Epoch 394/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4150 - accuracy: 0.5057\n",
      "Epoch 00394: val_loss did not improve from 1.45636\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4132 - accuracy: 0.5064 - val_loss: 1.4669 - val_accuracy: 0.4693\n",
      "Epoch 395/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4220 - accuracy: 0.5030\n",
      "Epoch 00395: val_loss did not improve from 1.45636\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4184 - accuracy: 0.5052 - val_loss: 1.4693 - val_accuracy: 0.4652\n",
      "Epoch 396/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4159 - accuracy: 0.5102\n",
      "Epoch 00396: val_loss did not improve from 1.45636\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4158 - accuracy: 0.5098 - val_loss: 1.4647 - val_accuracy: 0.4714\n",
      "Epoch 397/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4132 - accuracy: 0.5091\n",
      "Epoch 00397: val_loss improved from 1.45636 to 1.45593, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4128 - accuracy: 0.5071 - val_loss: 1.4559 - val_accuracy: 0.4850\n",
      "Epoch 398/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4140 - accuracy: 0.5069\n",
      "Epoch 00398: val_loss did not improve from 1.45593\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4125 - accuracy: 0.5084 - val_loss: 1.4586 - val_accuracy: 0.4720\n",
      "Epoch 399/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4078 - accuracy: 0.5033\n",
      "Epoch 00399: val_loss improved from 1.45593 to 1.45356, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4123 - accuracy: 0.5026 - val_loss: 1.4536 - val_accuracy: 0.4700\n",
      "Epoch 400/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4136 - accuracy: 0.5071\n",
      "Epoch 00400: val_loss did not improve from 1.45356\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4142 - accuracy: 0.5057 - val_loss: 1.4553 - val_accuracy: 0.4727\n",
      "Epoch 401/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4111 - accuracy: 0.5063\n",
      "Epoch 00401: val_loss did not improve from 1.45356\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4134 - accuracy: 0.5045 - val_loss: 1.4577 - val_accuracy: 0.4686\n",
      "Epoch 402/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4058 - accuracy: 0.5084\n",
      "Epoch 00402: val_loss did not improve from 1.45356\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4128 - accuracy: 0.5069 - val_loss: 1.4642 - val_accuracy: 0.4693\n",
      "Epoch 403/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4128 - accuracy: 0.5058\n",
      "Epoch 00403: val_loss did not improve from 1.45356\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4131 - accuracy: 0.5084 - val_loss: 1.4588 - val_accuracy: 0.4632\n",
      "Epoch 404/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3986 - accuracy: 0.5143\n",
      "Epoch 00404: val_loss did not improve from 1.45356\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4070 - accuracy: 0.5108 - val_loss: 1.4625 - val_accuracy: 0.4707\n",
      "Epoch 405/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4087 - accuracy: 0.5054\n",
      "Epoch 00405: val_loss did not improve from 1.45356\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4123 - accuracy: 0.5038 - val_loss: 1.4547 - val_accuracy: 0.4734\n",
      "Epoch 406/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4113 - accuracy: 0.5000\n",
      "Epoch 00406: val_loss improved from 1.45356 to 1.45351, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4098 - accuracy: 0.5071 - val_loss: 1.4535 - val_accuracy: 0.4734\n",
      "Epoch 407/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4095 - accuracy: 0.5117\n",
      "Epoch 00407: val_loss improved from 1.45351 to 1.45250, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4088 - accuracy: 0.5112 - val_loss: 1.4525 - val_accuracy: 0.4700\n",
      "Epoch 408/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4154 - accuracy: 0.5050\n",
      "Epoch 00408: val_loss did not improve from 1.45250\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4105 - accuracy: 0.5071 - val_loss: 1.4563 - val_accuracy: 0.4761\n",
      "Epoch 409/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4078 - accuracy: 0.5134\n",
      "Epoch 00409: val_loss did not improve from 1.45250\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4103 - accuracy: 0.5108 - val_loss: 1.4582 - val_accuracy: 0.4809\n",
      "Epoch 410/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4149 - accuracy: 0.5048\n",
      "Epoch 00410: val_loss did not improve from 1.45250\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4108 - accuracy: 0.5076 - val_loss: 1.4535 - val_accuracy: 0.4748\n",
      "Epoch 411/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4108 - accuracy: 0.5007\n",
      "Epoch 00411: val_loss did not improve from 1.45250\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4095 - accuracy: 0.5035 - val_loss: 1.4542 - val_accuracy: 0.4857\n",
      "Epoch 412/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4138 - accuracy: 0.5035\n",
      "Epoch 00412: val_loss did not improve from 1.45250\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4036 - accuracy: 0.5091 - val_loss: 1.4585 - val_accuracy: 0.4693\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4085 - accuracy: 0.5093\n",
      "Epoch 00413: val_loss improved from 1.45250 to 1.45174, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4085 - accuracy: 0.5093 - val_loss: 1.4517 - val_accuracy: 0.4754\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4060 - accuracy: 0.5110\n",
      "Epoch 00414: val_loss did not improve from 1.45174\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4060 - accuracy: 0.5110 - val_loss: 1.4523 - val_accuracy: 0.4720\n",
      "Epoch 415/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4063 - accuracy: 0.5058\n",
      "Epoch 00415: val_loss did not improve from 1.45174\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4097 - accuracy: 0.5059 - val_loss: 1.4580 - val_accuracy: 0.4795\n",
      "Epoch 416/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4044 - accuracy: 0.5141\n",
      "Epoch 00416: val_loss improved from 1.45174 to 1.44712, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4032 - accuracy: 0.5122 - val_loss: 1.4471 - val_accuracy: 0.4836\n",
      "Epoch 417/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4076 - accuracy: 0.5135\n",
      "Epoch 00417: val_loss did not improve from 1.44712\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4060 - accuracy: 0.5134 - val_loss: 1.4518 - val_accuracy: 0.4734\n",
      "Epoch 418/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4099 - accuracy: 0.5091\n",
      "Epoch 00418: val_loss did not improve from 1.44712\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4108 - accuracy: 0.5091 - val_loss: 1.4501 - val_accuracy: 0.4884\n",
      "Epoch 419/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4014 - accuracy: 0.5143\n",
      "Epoch 00419: val_loss did not improve from 1.44712\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4051 - accuracy: 0.5112 - val_loss: 1.4485 - val_accuracy: 0.4816\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4020 - accuracy: 0.5129\n",
      "Epoch 00420: val_loss did not improve from 1.44712\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4020 - accuracy: 0.5129 - val_loss: 1.4575 - val_accuracy: 0.4823\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4078 - accuracy: 0.5064\n",
      "Epoch 00421: val_loss improved from 1.44712 to 1.44427, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4078 - accuracy: 0.5064 - val_loss: 1.4443 - val_accuracy: 0.4645\n",
      "Epoch 422/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4049 - accuracy: 0.5066\n",
      "Epoch 00422: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4035 - accuracy: 0.5078 - val_loss: 1.4470 - val_accuracy: 0.4700\n",
      "Epoch 423/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3942 - accuracy: 0.5130\n",
      "Epoch 00423: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4008 - accuracy: 0.5102 - val_loss: 1.4524 - val_accuracy: 0.4816\n",
      "Epoch 424/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4019 - accuracy: 0.5080\n",
      "Epoch 00424: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3981 - accuracy: 0.5091 - val_loss: 1.4508 - val_accuracy: 0.4652\n",
      "Epoch 425/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4007 - accuracy: 0.5043\n",
      "Epoch 00425: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4025 - accuracy: 0.5055 - val_loss: 1.4495 - val_accuracy: 0.4673\n",
      "Epoch 426/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3930 - accuracy: 0.5102\n",
      "Epoch 00426: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3970 - accuracy: 0.5112 - val_loss: 1.4489 - val_accuracy: 0.4789\n",
      "Epoch 427/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4086 - accuracy: 0.5121\n",
      "Epoch 00427: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4068 - accuracy: 0.5115 - val_loss: 1.4538 - val_accuracy: 0.4748\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4065 - accuracy: 0.5107\n",
      "Epoch 00428: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4065 - accuracy: 0.5107 - val_loss: 1.4495 - val_accuracy: 0.4823\n",
      "Epoch 429/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4026 - accuracy: 0.5125\n",
      "Epoch 00429: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4016 - accuracy: 0.5148 - val_loss: 1.4451 - val_accuracy: 0.4727\n",
      "Epoch 430/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4046 - accuracy: 0.5084\n",
      "Epoch 00430: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4012 - accuracy: 0.5096 - val_loss: 1.4451 - val_accuracy: 0.4754\n",
      "Epoch 431/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4037 - accuracy: 0.5100\n",
      "Epoch 00431: val_loss did not improve from 1.44427\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3980 - accuracy: 0.5132 - val_loss: 1.4445 - val_accuracy: 0.4802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 432/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3999 - accuracy: 0.5143\n",
      "Epoch 00432: val_loss improved from 1.44427 to 1.44360, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3985 - accuracy: 0.5154 - val_loss: 1.4436 - val_accuracy: 0.4857\n",
      "Epoch 433/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3985 - accuracy: 0.5113\n",
      "Epoch 00433: val_loss did not improve from 1.44360\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4009 - accuracy: 0.5093 - val_loss: 1.4503 - val_accuracy: 0.4686\n",
      "Epoch 434/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4005 - accuracy: 0.5167\n",
      "Epoch 00434: val_loss did not improve from 1.44360\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4012 - accuracy: 0.5142 - val_loss: 1.4471 - val_accuracy: 0.4754\n",
      "Epoch 435/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3984 - accuracy: 0.5123\n",
      "Epoch 00435: val_loss did not improve from 1.44360\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4010 - accuracy: 0.5113 - val_loss: 1.4510 - val_accuracy: 0.4782\n",
      "Epoch 436/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3989 - accuracy: 0.5134\n",
      "Epoch 00436: val_loss did not improve from 1.44360\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3997 - accuracy: 0.5122 - val_loss: 1.4503 - val_accuracy: 0.4809\n",
      "Epoch 437/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4010 - accuracy: 0.5106\n",
      "Epoch 00437: val_loss did not improve from 1.44360\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4004 - accuracy: 0.5124 - val_loss: 1.4550 - val_accuracy: 0.4864\n",
      "Epoch 438/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4033 - accuracy: 0.5083\n",
      "Epoch 00438: val_loss did not improve from 1.44360\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4005 - accuracy: 0.5093 - val_loss: 1.4453 - val_accuracy: 0.4802\n",
      "Epoch 439/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3982 - accuracy: 0.5099\n",
      "Epoch 00439: val_loss improved from 1.44360 to 1.44029, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3999 - accuracy: 0.5095 - val_loss: 1.4403 - val_accuracy: 0.4843\n",
      "Epoch 440/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3979 - accuracy: 0.5154\n",
      "Epoch 00440: val_loss improved from 1.44029 to 1.43831, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3975 - accuracy: 0.5144 - val_loss: 1.4383 - val_accuracy: 0.4775\n",
      "Epoch 441/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.3879 - accuracy: 0.5126\n",
      "Epoch 00441: val_loss did not improve from 1.43831\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3983 - accuracy: 0.5076 - val_loss: 1.4423 - val_accuracy: 0.4829\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3961 - accuracy: 0.5074\n",
      "Epoch 00442: val_loss did not improve from 1.43831\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3961 - accuracy: 0.5074 - val_loss: 1.4399 - val_accuracy: 0.4870\n",
      "Epoch 443/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3993 - accuracy: 0.5119\n",
      "Epoch 00443: val_loss did not improve from 1.43831\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3998 - accuracy: 0.5125 - val_loss: 1.4400 - val_accuracy: 0.4864\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4004 - accuracy: 0.5086\n",
      "Epoch 00444: val_loss did not improve from 1.43831\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4004 - accuracy: 0.5086 - val_loss: 1.4427 - val_accuracy: 0.4918\n",
      "Epoch 445/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3960 - accuracy: 0.5108\n",
      "Epoch 00445: val_loss improved from 1.43831 to 1.43739, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3981 - accuracy: 0.5083 - val_loss: 1.4374 - val_accuracy: 0.4816\n",
      "Epoch 446/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.3896 - accuracy: 0.5189\n",
      "Epoch 00446: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3900 - accuracy: 0.5199 - val_loss: 1.4453 - val_accuracy: 0.4843\n",
      "Epoch 447/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3904 - accuracy: 0.5210\n",
      "Epoch 00447: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3966 - accuracy: 0.5180 - val_loss: 1.4463 - val_accuracy: 0.4734\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3968 - accuracy: 0.5153\n",
      "Epoch 00448: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3968 - accuracy: 0.5153 - val_loss: 1.4404 - val_accuracy: 0.4823\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3983 - accuracy: 0.5132\n",
      "Epoch 00449: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3983 - accuracy: 0.5132 - val_loss: 1.4415 - val_accuracy: 0.4693\n",
      "Epoch 450/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4060 - accuracy: 0.5119\n",
      "Epoch 00450: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3954 - accuracy: 0.5160 - val_loss: 1.4407 - val_accuracy: 0.4809\n",
      "Epoch 451/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3858 - accuracy: 0.5175\n",
      "Epoch 00451: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3976 - accuracy: 0.5144 - val_loss: 1.4416 - val_accuracy: 0.4754\n",
      "Epoch 452/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3909 - accuracy: 0.5106\n",
      "Epoch 00452: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3929 - accuracy: 0.5127 - val_loss: 1.4381 - val_accuracy: 0.4843\n",
      "Epoch 453/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3869 - accuracy: 0.5214\n",
      "Epoch 00453: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3931 - accuracy: 0.5197 - val_loss: 1.4405 - val_accuracy: 0.4857\n",
      "Epoch 454/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3990 - accuracy: 0.5156\n",
      "Epoch 00454: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3972 - accuracy: 0.5144 - val_loss: 1.4411 - val_accuracy: 0.4714\n",
      "Epoch 455/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3978 - accuracy: 0.5142\n",
      "Epoch 00455: val_loss did not improve from 1.43739\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3957 - accuracy: 0.5156 - val_loss: 1.4408 - val_accuracy: 0.4905\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3939 - accuracy: 0.5160\n",
      "Epoch 00456: val_loss improved from 1.43739 to 1.43545, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3939 - accuracy: 0.5160 - val_loss: 1.4354 - val_accuracy: 0.4891\n",
      "Epoch 457/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3975 - accuracy: 0.5163\n",
      "Epoch 00457: val_loss did not improve from 1.43545\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3982 - accuracy: 0.5149 - val_loss: 1.4368 - val_accuracy: 0.4775\n",
      "Epoch 458/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3950 - accuracy: 0.5164\n",
      "Epoch 00458: val_loss improved from 1.43545 to 1.43397, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3952 - accuracy: 0.5166 - val_loss: 1.4340 - val_accuracy: 0.4918\n",
      "Epoch 459/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3945 - accuracy: 0.5119\n",
      "Epoch 00459: val_loss did not improve from 1.43397\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3933 - accuracy: 0.5141 - val_loss: 1.4443 - val_accuracy: 0.4768\n",
      "Epoch 460/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3876 - accuracy: 0.5167\n",
      "Epoch 00460: val_loss did not improve from 1.43397\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3913 - accuracy: 0.5161 - val_loss: 1.4342 - val_accuracy: 0.4864\n",
      "Epoch 461/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3911 - accuracy: 0.5141\n",
      "Epoch 00461: val_loss improved from 1.43397 to 1.43217, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3929 - accuracy: 0.5125 - val_loss: 1.4322 - val_accuracy: 0.4870\n",
      "Epoch 462/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.3899 - accuracy: 0.5158\n",
      "Epoch 00462: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3871 - accuracy: 0.5165 - val_loss: 1.4391 - val_accuracy: 0.4870\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3941 - accuracy: 0.5216\n",
      "Epoch 00463: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3941 - accuracy: 0.5216 - val_loss: 1.4385 - val_accuracy: 0.4679\n",
      "Epoch 464/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4001 - accuracy: 0.5119\n",
      "Epoch 00464: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3926 - accuracy: 0.5129 - val_loss: 1.4333 - val_accuracy: 0.4864\n",
      "Epoch 465/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3782 - accuracy: 0.5166\n",
      "Epoch 00465: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.5146 - val_loss: 1.4360 - val_accuracy: 0.4700\n",
      "Epoch 466/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3873 - accuracy: 0.5108\n",
      "Epoch 00466: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3931 - accuracy: 0.5107 - val_loss: 1.4407 - val_accuracy: 0.4761\n",
      "Epoch 467/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3904 - accuracy: 0.5140\n",
      "Epoch 00467: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3901 - accuracy: 0.5132 - val_loss: 1.4380 - val_accuracy: 0.4775\n",
      "Epoch 468/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3873 - accuracy: 0.5182\n",
      "Epoch 00468: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3894 - accuracy: 0.5183 - val_loss: 1.4353 - val_accuracy: 0.4925\n",
      "Epoch 469/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3895 - accuracy: 0.5146\n",
      "Epoch 00469: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3894 - accuracy: 0.5149 - val_loss: 1.4361 - val_accuracy: 0.4802\n",
      "Epoch 470/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.3798 - accuracy: 0.5189\n",
      "Epoch 00470: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3850 - accuracy: 0.5189 - val_loss: 1.4352 - val_accuracy: 0.4898\n",
      "Epoch 471/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3848 - accuracy: 0.5272\n",
      "Epoch 00471: val_loss did not improve from 1.43217\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3883 - accuracy: 0.5250 - val_loss: 1.4441 - val_accuracy: 0.4918\n",
      "Epoch 472/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3849 - accuracy: 0.5247\n",
      "Epoch 00472: val_loss improved from 1.43217 to 1.43088, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3896 - accuracy: 0.5218 - val_loss: 1.4309 - val_accuracy: 0.4884\n",
      "Epoch 473/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3817 - accuracy: 0.5225\n",
      "Epoch 00473: val_loss did not improve from 1.43088\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3839 - accuracy: 0.5211 - val_loss: 1.4326 - val_accuracy: 0.4911\n",
      "Epoch 474/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3860 - accuracy: 0.5199\n",
      "Epoch 00474: val_loss did not improve from 1.43088\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3888 - accuracy: 0.5180 - val_loss: 1.4343 - val_accuracy: 0.4911\n",
      "Epoch 475/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3884 - accuracy: 0.5192\n",
      "Epoch 00475: val_loss did not improve from 1.43088\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3876 - accuracy: 0.5202 - val_loss: 1.4321 - val_accuracy: 0.4795\n",
      "Epoch 476/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3866 - accuracy: 0.5176\n",
      "Epoch 00476: val_loss did not improve from 1.43088\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3862 - accuracy: 0.5187 - val_loss: 1.4315 - val_accuracy: 0.4870\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3848 - accuracy: 0.5211\n",
      "Epoch 00477: val_loss did not improve from 1.43088\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3848 - accuracy: 0.5211 - val_loss: 1.4319 - val_accuracy: 0.4823\n",
      "Epoch 478/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3943 - accuracy: 0.5167\n",
      "Epoch 00478: val_loss improved from 1.43088 to 1.42899, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3879 - accuracy: 0.5202 - val_loss: 1.4290 - val_accuracy: 0.4816\n",
      "Epoch 479/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3864 - accuracy: 0.5197\n",
      "Epoch 00479: val_loss improved from 1.42899 to 1.42865, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3848 - accuracy: 0.5190 - val_loss: 1.4286 - val_accuracy: 0.4905\n",
      "Epoch 480/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3849 - accuracy: 0.5208\n",
      "Epoch 00480: val_loss did not improve from 1.42865\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3842 - accuracy: 0.5209 - val_loss: 1.4317 - val_accuracy: 0.4891\n",
      "Epoch 481/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3814 - accuracy: 0.5214\n",
      "Epoch 00481: val_loss did not improve from 1.42865\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3832 - accuracy: 0.5212 - val_loss: 1.4319 - val_accuracy: 0.4809\n",
      "Epoch 482/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3901 - accuracy: 0.5218\n",
      "Epoch 00482: val_loss improved from 1.42865 to 1.42481, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3852 - accuracy: 0.5221 - val_loss: 1.4248 - val_accuracy: 0.4836\n",
      "Epoch 483/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3831 - accuracy: 0.5186\n",
      "Epoch 00483: val_loss did not improve from 1.42481\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3834 - accuracy: 0.5190 - val_loss: 1.4291 - val_accuracy: 0.4939\n",
      "Epoch 484/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3824 - accuracy: 0.5174\n",
      "Epoch 00484: val_loss did not improve from 1.42481\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3824 - accuracy: 0.5166 - val_loss: 1.4269 - val_accuracy: 0.4911\n",
      "Epoch 485/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3836 - accuracy: 0.5197\n",
      "Epoch 00485: val_loss improved from 1.42481 to 1.42433, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3832 - accuracy: 0.5207 - val_loss: 1.4243 - val_accuracy: 0.4898\n",
      "Epoch 486/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3875 - accuracy: 0.5163\n",
      "Epoch 00486: val_loss did not improve from 1.42433\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3849 - accuracy: 0.5177 - val_loss: 1.4280 - val_accuracy: 0.4966\n",
      "Epoch 487/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3773 - accuracy: 0.5270\n",
      "Epoch 00487: val_loss did not improve from 1.42433\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3799 - accuracy: 0.5279 - val_loss: 1.4283 - val_accuracy: 0.4993\n",
      "Epoch 488/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3804 - accuracy: 0.5201\n",
      "Epoch 00488: val_loss improved from 1.42433 to 1.42310, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3814 - accuracy: 0.5189 - val_loss: 1.4231 - val_accuracy: 0.4877\n",
      "Epoch 489/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3843 - accuracy: 0.5229\n",
      "Epoch 00489: val_loss did not improve from 1.42310\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3831 - accuracy: 0.5211 - val_loss: 1.4288 - val_accuracy: 0.4809\n",
      "Epoch 490/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3841 - accuracy: 0.5195\n",
      "Epoch 00490: val_loss improved from 1.42310 to 1.42162, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3822 - accuracy: 0.5204 - val_loss: 1.4216 - val_accuracy: 0.4952\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3826 - accuracy: 0.5177\n",
      "Epoch 00491: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3826 - accuracy: 0.5177 - val_loss: 1.4270 - val_accuracy: 0.4884\n",
      "Epoch 492/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3782 - accuracy: 0.5210\n",
      "Epoch 00492: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3833 - accuracy: 0.5200 - val_loss: 1.4259 - val_accuracy: 0.4939\n",
      "Epoch 493/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3725 - accuracy: 0.5320\n",
      "Epoch 00493: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3805 - accuracy: 0.5272 - val_loss: 1.4245 - val_accuracy: 0.4891\n",
      "Epoch 494/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3836 - accuracy: 0.5199\n",
      "Epoch 00494: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3810 - accuracy: 0.5192 - val_loss: 1.4218 - val_accuracy: 0.4925\n",
      "Epoch 495/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3758 - accuracy: 0.5288\n",
      "Epoch 00495: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3795 - accuracy: 0.5276 - val_loss: 1.4253 - val_accuracy: 0.4891\n",
      "Epoch 496/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3793 - accuracy: 0.5231\n",
      "Epoch 00496: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3785 - accuracy: 0.5276 - val_loss: 1.4286 - val_accuracy: 0.4761\n",
      "Epoch 497/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3837 - accuracy: 0.5137\n",
      "Epoch 00497: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3854 - accuracy: 0.5129 - val_loss: 1.4302 - val_accuracy: 0.4714\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3844 - accuracy: 0.5190\n",
      "Epoch 00498: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3844 - accuracy: 0.5190 - val_loss: 1.4254 - val_accuracy: 0.5061\n",
      "Epoch 499/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3769 - accuracy: 0.5259\n",
      "Epoch 00499: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3793 - accuracy: 0.5255 - val_loss: 1.4220 - val_accuracy: 0.4945\n",
      "Epoch 500/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3781 - accuracy: 0.5245\n",
      "Epoch 00500: val_loss did not improve from 1.42162\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3792 - accuracy: 0.5250 - val_loss: 1.4290 - val_accuracy: 0.4870\n",
      "Training for model  1  completed in time:  0:02:36.865201 seconds\n",
      "Training for model  2  has started.\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.0936 - accuracy: 0.1723\n",
      "Epoch 00001: val_loss improved from inf to 1.91358, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 3.0936 - accuracy: 0.1723 - val_loss: 1.9136 - val_accuracy: 0.2606\n",
      "Epoch 2/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 1.8158 - accuracy: 0.3061\n",
      "Epoch 00002: val_loss improved from 1.91358 to 1.78205, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.8005 - accuracy: 0.3155 - val_loss: 1.7820 - val_accuracy: 0.3472\n",
      "Epoch 3/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6870 - accuracy: 0.3888\n",
      "Epoch 00003: val_loss improved from 1.78205 to 1.68543, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.6845 - accuracy: 0.3907 - val_loss: 1.6854 - val_accuracy: 0.3813\n",
      "Epoch 4/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.6041 - accuracy: 0.4210\n",
      "Epoch 00004: val_loss improved from 1.68543 to 1.60622, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.6065 - accuracy: 0.4204 - val_loss: 1.6062 - val_accuracy: 0.4065\n",
      "Epoch 5/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5548 - accuracy: 0.4345\n",
      "Epoch 00005: val_loss improved from 1.60622 to 1.56705, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.5530 - accuracy: 0.4378 - val_loss: 1.5671 - val_accuracy: 0.4250\n",
      "Epoch 6/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5049 - accuracy: 0.4611\n",
      "Epoch 00006: val_loss improved from 1.56705 to 1.50003, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.5018 - accuracy: 0.4607 - val_loss: 1.5000 - val_accuracy: 0.4502\n",
      "Epoch 7/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.4620 - accuracy: 0.4691\n",
      "Epoch 00007: val_loss improved from 1.50003 to 1.47519, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4597 - accuracy: 0.4699 - val_loss: 1.4752 - val_accuracy: 0.4454\n",
      "Epoch 8/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4367 - accuracy: 0.4734\n",
      "Epoch 00008: val_loss improved from 1.47519 to 1.42870, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.4256 - accuracy: 0.4779 - val_loss: 1.4287 - val_accuracy: 0.4768\n",
      "Epoch 9/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3796 - accuracy: 0.4913\n",
      "Epoch 00009: val_loss improved from 1.42870 to 1.39144, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.3795 - accuracy: 0.4916 - val_loss: 1.3914 - val_accuracy: 0.4761\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3611 - accuracy: 0.4905\n",
      "Epoch 00010: val_loss improved from 1.39144 to 1.37147, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.3574 - accuracy: 0.4950 - val_loss: 1.3715 - val_accuracy: 0.4911\n",
      "Epoch 11/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3312 - accuracy: 0.5143\n",
      "Epoch 00011: val_loss improved from 1.37147 to 1.35577, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3318 - accuracy: 0.5142 - val_loss: 1.3558 - val_accuracy: 0.4898\n",
      "Epoch 12/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3234 - accuracy: 0.5190\n",
      "Epoch 00012: val_loss improved from 1.35577 to 1.33004, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3225 - accuracy: 0.5197 - val_loss: 1.3300 - val_accuracy: 0.5089\n",
      "Epoch 13/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.2886 - accuracy: 0.5247\n",
      "Epoch 00013: val_loss improved from 1.33004 to 1.31960, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2936 - accuracy: 0.5236 - val_loss: 1.3196 - val_accuracy: 0.5177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2880 - accuracy: 0.5246\n",
      "Epoch 00014: val_loss improved from 1.31960 to 1.30283, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.2866 - accuracy: 0.5236 - val_loss: 1.3028 - val_accuracy: 0.5300\n",
      "Epoch 15/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.2759 - accuracy: 0.5304\n",
      "Epoch 00015: val_loss improved from 1.30283 to 1.28639, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2721 - accuracy: 0.5335 - val_loss: 1.2864 - val_accuracy: 0.5423\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2495 - accuracy: 0.5444\n",
      "Epoch 00016: val_loss improved from 1.28639 to 1.26113, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2495 - accuracy: 0.5444 - val_loss: 1.2611 - val_accuracy: 0.5491\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2329 - accuracy: 0.5535\n",
      "Epoch 00017: val_loss did not improve from 1.26113\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.2329 - accuracy: 0.5535 - val_loss: 1.2794 - val_accuracy: 0.5341\n",
      "Epoch 18/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.2280 - accuracy: 0.5588\n",
      "Epoch 00018: val_loss improved from 1.26113 to 1.24207, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2281 - accuracy: 0.5595 - val_loss: 1.2421 - val_accuracy: 0.5484\n",
      "Epoch 19/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.2143 - accuracy: 0.5629\n",
      "Epoch 00019: val_loss improved from 1.24207 to 1.22488, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2144 - accuracy: 0.5624 - val_loss: 1.2249 - val_accuracy: 0.5709\n",
      "Epoch 20/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1983 - accuracy: 0.5719\n",
      "Epoch 00020: val_loss improved from 1.22488 to 1.22077, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.2009 - accuracy: 0.5721 - val_loss: 1.2208 - val_accuracy: 0.5744\n",
      "Epoch 21/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2012 - accuracy: 0.5653\n",
      "Epoch 00021: val_loss improved from 1.22077 to 1.20929, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1966 - accuracy: 0.5704 - val_loss: 1.2093 - val_accuracy: 0.5859\n",
      "Epoch 22/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1799 - accuracy: 0.5758\n",
      "Epoch 00022: val_loss did not improve from 1.20929\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1792 - accuracy: 0.5777 - val_loss: 1.2411 - val_accuracy: 0.5668\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1793 - accuracy: 0.5777\n",
      "Epoch 00023: val_loss improved from 1.20929 to 1.18677, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1793 - accuracy: 0.5777 - val_loss: 1.1868 - val_accuracy: 0.5900\n",
      "Epoch 24/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1707 - accuracy: 0.5807\n",
      "Epoch 00024: val_loss improved from 1.18677 to 1.17333, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1703 - accuracy: 0.5818 - val_loss: 1.1733 - val_accuracy: 0.5805\n",
      "Epoch 25/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1575 - accuracy: 0.5887\n",
      "Epoch 00025: val_loss did not improve from 1.17333\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1541 - accuracy: 0.5929 - val_loss: 1.1764 - val_accuracy: 0.6085\n",
      "Epoch 26/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1543 - accuracy: 0.5878\n",
      "Epoch 00026: val_loss did not improve from 1.17333\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1532 - accuracy: 0.5900 - val_loss: 1.1817 - val_accuracy: 0.5907\n",
      "Epoch 27/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1436 - accuracy: 0.5926\n",
      "Epoch 00027: val_loss improved from 1.17333 to 1.17297, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1443 - accuracy: 0.5931 - val_loss: 1.1730 - val_accuracy: 0.5982\n",
      "Epoch 28/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1289 - accuracy: 0.5971\n",
      "Epoch 00028: val_loss improved from 1.17297 to 1.15204, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1296 - accuracy: 0.5973 - val_loss: 1.1520 - val_accuracy: 0.6078\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1323 - accuracy: 0.6025\n",
      "Epoch 00029: val_loss did not improve from 1.15204\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1323 - accuracy: 0.6025 - val_loss: 1.1718 - val_accuracy: 0.5907\n",
      "Epoch 30/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1379 - accuracy: 0.5928\n",
      "Epoch 00030: val_loss improved from 1.15204 to 1.14917, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1318 - accuracy: 0.5977 - val_loss: 1.1492 - val_accuracy: 0.6221\n",
      "Epoch 31/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1153 - accuracy: 0.6085\n",
      "Epoch 00031: val_loss improved from 1.14917 to 1.14358, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.1100 - accuracy: 0.6105 - val_loss: 1.1436 - val_accuracy: 0.6091\n",
      "Epoch 32/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.1099 - accuracy: 0.6069\n",
      "Epoch 00032: val_loss improved from 1.14358 to 1.13150, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.1090 - accuracy: 0.6076 - val_loss: 1.1315 - val_accuracy: 0.6105\n",
      "Epoch 33/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1122 - accuracy: 0.6125\n",
      "Epoch 00033: val_loss did not improve from 1.13150\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.1097 - accuracy: 0.6132 - val_loss: 1.1327 - val_accuracy: 0.6119\n",
      "Epoch 34/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0953 - accuracy: 0.6188\n",
      "Epoch 00034: val_loss improved from 1.13150 to 1.12881, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.0936 - accuracy: 0.6182 - val_loss: 1.1288 - val_accuracy: 0.6160\n",
      "Epoch 35/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0896 - accuracy: 0.6151\n",
      "Epoch 00035: val_loss improved from 1.12881 to 1.12095, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0869 - accuracy: 0.6164 - val_loss: 1.1209 - val_accuracy: 0.6282\n",
      "Epoch 36/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0841 - accuracy: 0.6250\n",
      "Epoch 00036: val_loss improved from 1.12095 to 1.12026, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0829 - accuracy: 0.6263 - val_loss: 1.1203 - val_accuracy: 0.6085\n",
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0759 - accuracy: 0.6185\n",
      "Epoch 00037: val_loss improved from 1.12026 to 1.11700, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0690 - accuracy: 0.6226 - val_loss: 1.1170 - val_accuracy: 0.6241\n",
      "Epoch 38/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0658 - accuracy: 0.6263\n",
      "Epoch 00038: val_loss improved from 1.11700 to 1.09702, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0724 - accuracy: 0.6234 - val_loss: 1.0970 - val_accuracy: 0.6180\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 1.0724 - accuracy: 0.6243\n",
      "Epoch 00039: val_loss improved from 1.09702 to 1.07761, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0724 - accuracy: 0.6243 - val_loss: 1.0776 - val_accuracy: 0.6228\n",
      "Epoch 40/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0584 - accuracy: 0.6346\n",
      "Epoch 00040: val_loss did not improve from 1.07761\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0573 - accuracy: 0.6347 - val_loss: 1.0865 - val_accuracy: 0.6289\n",
      "Epoch 41/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0494 - accuracy: 0.6356\n",
      "Epoch 00041: val_loss did not improve from 1.07761\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0541 - accuracy: 0.6327 - val_loss: 1.0954 - val_accuracy: 0.6276\n",
      "Epoch 42/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0555 - accuracy: 0.6321\n",
      "Epoch 00042: val_loss did not improve from 1.07761\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0565 - accuracy: 0.6301 - val_loss: 1.0796 - val_accuracy: 0.6357\n",
      "Epoch 43/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0387 - accuracy: 0.6375\n",
      "Epoch 00043: val_loss improved from 1.07761 to 1.06716, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0459 - accuracy: 0.6352 - val_loss: 1.0672 - val_accuracy: 0.6351\n",
      "Epoch 44/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0355 - accuracy: 0.6349\n",
      "Epoch 00044: val_loss did not improve from 1.06716\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.0368 - accuracy: 0.6340 - val_loss: 1.0832 - val_accuracy: 0.6276\n",
      "Epoch 45/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0335 - accuracy: 0.6397\n",
      "Epoch 00045: val_loss improved from 1.06716 to 1.05705, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0367 - accuracy: 0.6381 - val_loss: 1.0570 - val_accuracy: 0.6392\n",
      "Epoch 46/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0415 - accuracy: 0.6358\n",
      "Epoch 00046: val_loss improved from 1.05705 to 1.05400, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0366 - accuracy: 0.6386 - val_loss: 1.0540 - val_accuracy: 0.6432\n",
      "Epoch 47/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0243 - accuracy: 0.6481\n",
      "Epoch 00047: val_loss did not improve from 1.05400\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0248 - accuracy: 0.6477 - val_loss: 1.0570 - val_accuracy: 0.6453\n",
      "Epoch 48/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0200 - accuracy: 0.6442\n",
      "Epoch 00048: val_loss did not improve from 1.05400\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0206 - accuracy: 0.6424 - val_loss: 1.0620 - val_accuracy: 0.6378\n",
      "Epoch 49/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0293 - accuracy: 0.6410\n",
      "Epoch 00049: val_loss did not improve from 1.05400\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0234 - accuracy: 0.6429 - val_loss: 1.0601 - val_accuracy: 0.6364\n",
      "Epoch 50/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0092 - accuracy: 0.6488\n",
      "Epoch 00050: val_loss improved from 1.05400 to 1.05110, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0090 - accuracy: 0.6485 - val_loss: 1.0511 - val_accuracy: 0.6439\n",
      "Epoch 51/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0093 - accuracy: 0.6502\n",
      "Epoch 00051: val_loss improved from 1.05110 to 1.03347, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0120 - accuracy: 0.6497 - val_loss: 1.0335 - val_accuracy: 0.6583\n",
      "Epoch 52/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.0018 - accuracy: 0.6554\n",
      "Epoch 00052: val_loss did not improve from 1.03347\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.0010 - accuracy: 0.6550 - val_loss: 1.0629 - val_accuracy: 0.6282\n",
      "Epoch 53/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0011 - accuracy: 0.6475\n",
      "Epoch 00053: val_loss improved from 1.03347 to 1.02112, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0104 - accuracy: 0.6441 - val_loss: 1.0211 - val_accuracy: 0.6610\n",
      "Epoch 54/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9994 - accuracy: 0.6509\n",
      "Epoch 00054: val_loss did not improve from 1.02112\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9991 - accuracy: 0.6521 - val_loss: 1.0482 - val_accuracy: 0.6467\n",
      "Epoch 55/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9855 - accuracy: 0.6562\n",
      "Epoch 00055: val_loss did not improve from 1.02112\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9894 - accuracy: 0.6560 - val_loss: 1.0466 - val_accuracy: 0.6555\n",
      "Epoch 56/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9974 - accuracy: 0.6568\n",
      "Epoch 00056: val_loss did not improve from 1.02112\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9981 - accuracy: 0.6560 - val_loss: 1.0212 - val_accuracy: 0.6446\n",
      "Epoch 57/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9828 - accuracy: 0.6607\n",
      "Epoch 00057: val_loss improved from 1.02112 to 1.01769, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9831 - accuracy: 0.6586 - val_loss: 1.0177 - val_accuracy: 0.6596\n",
      "Epoch 58/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9942 - accuracy: 0.6543\n",
      "Epoch 00058: val_loss did not improve from 1.01769\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9938 - accuracy: 0.6555 - val_loss: 1.0268 - val_accuracy: 0.6562\n",
      "Epoch 59/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9845 - accuracy: 0.6603\n",
      "Epoch 00059: val_loss improved from 1.01769 to 1.01056, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9806 - accuracy: 0.6618 - val_loss: 1.0106 - val_accuracy: 0.6617\n",
      "Epoch 60/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9847 - accuracy: 0.6616\n",
      "Epoch 00060: val_loss did not improve from 1.01056\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9827 - accuracy: 0.6627 - val_loss: 1.0169 - val_accuracy: 0.6569\n",
      "Epoch 61/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9915 - accuracy: 0.6547\n",
      "Epoch 00061: val_loss did not improve from 1.01056\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9886 - accuracy: 0.6560 - val_loss: 1.0176 - val_accuracy: 0.6555\n",
      "Epoch 62/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9720 - accuracy: 0.6600\n",
      "Epoch 00062: val_loss did not improve from 1.01056\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9738 - accuracy: 0.6606 - val_loss: 1.0381 - val_accuracy: 0.6508\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9779 - accuracy: 0.6642\n",
      "Epoch 00063: val_loss improved from 1.01056 to 0.99816, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9736 - accuracy: 0.6641 - val_loss: 0.9982 - val_accuracy: 0.6705\n",
      "Epoch 64/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9711 - accuracy: 0.6660\n",
      "Epoch 00064: val_loss improved from 0.99816 to 0.98518, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9677 - accuracy: 0.6666 - val_loss: 0.9852 - val_accuracy: 0.6726\n",
      "Epoch 65/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9649 - accuracy: 0.6696\n",
      "Epoch 00065: val_loss did not improve from 0.98518\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9621 - accuracy: 0.6705 - val_loss: 0.9887 - val_accuracy: 0.6726\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9577 - accuracy: 0.6694\n",
      "Epoch 00066: val_loss improved from 0.98518 to 0.98333, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9598 - accuracy: 0.6690 - val_loss: 0.9833 - val_accuracy: 0.6692\n",
      "Epoch 67/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9533 - accuracy: 0.6651\n",
      "Epoch 00067: val_loss did not improve from 0.98333\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9525 - accuracy: 0.6670 - val_loss: 0.9977 - val_accuracy: 0.6726\n",
      "Epoch 68/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9466 - accuracy: 0.6749\n",
      "Epoch 00068: val_loss did not improve from 0.98333\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9482 - accuracy: 0.6728 - val_loss: 1.0308 - val_accuracy: 0.6569\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9569 - accuracy: 0.6702\n",
      "Epoch 00069: val_loss improved from 0.98333 to 0.98181, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9566 - accuracy: 0.6699 - val_loss: 0.9818 - val_accuracy: 0.6801\n",
      "Epoch 70/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9469 - accuracy: 0.6699\n",
      "Epoch 00070: val_loss did not improve from 0.98181\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9440 - accuracy: 0.6709 - val_loss: 0.9830 - val_accuracy: 0.6698\n",
      "Epoch 71/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9475 - accuracy: 0.6680\n",
      "Epoch 00071: val_loss did not improve from 0.98181\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9477 - accuracy: 0.6687 - val_loss: 0.9854 - val_accuracy: 0.6685\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9359 - accuracy: 0.6722\n",
      "Epoch 00072: val_loss did not improve from 0.98181\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9359 - accuracy: 0.6722 - val_loss: 0.9984 - val_accuracy: 0.6651\n",
      "Epoch 73/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9387 - accuracy: 0.6758\n",
      "Epoch 00073: val_loss did not improve from 0.98181\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9366 - accuracy: 0.6767 - val_loss: 0.9863 - val_accuracy: 0.6692\n",
      "Epoch 74/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9343 - accuracy: 0.6778\n",
      "Epoch 00074: val_loss improved from 0.98181 to 0.97579, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9314 - accuracy: 0.6777 - val_loss: 0.9758 - val_accuracy: 0.6835\n",
      "Epoch 75/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9392 - accuracy: 0.6765\n",
      "Epoch 00075: val_loss improved from 0.97579 to 0.96694, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9394 - accuracy: 0.6758 - val_loss: 0.9669 - val_accuracy: 0.6787\n",
      "Epoch 76/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9450 - accuracy: 0.6715\n",
      "Epoch 00076: val_loss did not improve from 0.96694\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9404 - accuracy: 0.6731 - val_loss: 0.9753 - val_accuracy: 0.6842\n",
      "Epoch 77/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9328 - accuracy: 0.6797\n",
      "Epoch 00077: val_loss did not improve from 0.96694\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9290 - accuracy: 0.6808 - val_loss: 0.9672 - val_accuracy: 0.6753\n",
      "Epoch 78/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9271 - accuracy: 0.6777\n",
      "Epoch 00078: val_loss did not improve from 0.96694\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9313 - accuracy: 0.6768 - val_loss: 0.9793 - val_accuracy: 0.6753\n",
      "Epoch 79/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9219 - accuracy: 0.6892\n",
      "Epoch 00079: val_loss improved from 0.96694 to 0.95767, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9240 - accuracy: 0.6857 - val_loss: 0.9577 - val_accuracy: 0.6767\n",
      "Epoch 80/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9273 - accuracy: 0.6802\n",
      "Epoch 00080: val_loss did not improve from 0.95767\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9160 - accuracy: 0.6842 - val_loss: 0.9718 - val_accuracy: 0.6774\n",
      "Epoch 81/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9150 - accuracy: 0.6866\n",
      "Epoch 00081: val_loss improved from 0.95767 to 0.94194, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9172 - accuracy: 0.6873 - val_loss: 0.9419 - val_accuracy: 0.6951\n",
      "Epoch 82/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9211 - accuracy: 0.6836\n",
      "Epoch 00082: val_loss did not improve from 0.94194\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.9225 - accuracy: 0.6833 - val_loss: 0.9916 - val_accuracy: 0.6712\n",
      "Epoch 83/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9235 - accuracy: 0.6784\n",
      "Epoch 00083: val_loss did not improve from 0.94194\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9170 - accuracy: 0.6826 - val_loss: 0.9625 - val_accuracy: 0.6862\n",
      "Epoch 84/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9107 - accuracy: 0.6873\n",
      "Epoch 00084: val_loss did not improve from 0.94194\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9128 - accuracy: 0.6854 - val_loss: 0.9429 - val_accuracy: 0.6849\n",
      "Epoch 85/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9126 - accuracy: 0.6847\n",
      "Epoch 00085: val_loss improved from 0.94194 to 0.93027, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9099 - accuracy: 0.6871 - val_loss: 0.9303 - val_accuracy: 0.6924\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9120 - accuracy: 0.6862\n",
      "Epoch 00086: val_loss improved from 0.93027 to 0.92773, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9149 - accuracy: 0.6838 - val_loss: 0.9277 - val_accuracy: 0.6924\n",
      "Epoch 87/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9093 - accuracy: 0.6873\n",
      "Epoch 00087: val_loss did not improve from 0.92773\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9089 - accuracy: 0.6876 - val_loss: 0.9444 - val_accuracy: 0.6896\n",
      "Epoch 88/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9065 - accuracy: 0.6875\n",
      "Epoch 00088: val_loss did not improve from 0.92773\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9069 - accuracy: 0.6878 - val_loss: 0.9394 - val_accuracy: 0.6917\n",
      "Epoch 89/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.9059 - accuracy: 0.6926\n",
      "Epoch 00089: val_loss did not improve from 0.92773\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.9070 - accuracy: 0.6912 - val_loss: 0.9281 - val_accuracy: 0.7067\n",
      "Epoch 90/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8977 - accuracy: 0.6944\n",
      "Epoch 00090: val_loss did not improve from 0.92773\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8980 - accuracy: 0.6949 - val_loss: 0.9512 - val_accuracy: 0.6937\n",
      "Epoch 91/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9064 - accuracy: 0.6923\n",
      "Epoch 00091: val_loss did not improve from 0.92773\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8989 - accuracy: 0.6953 - val_loss: 0.9358 - val_accuracy: 0.6958\n",
      "Epoch 92/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8896 - accuracy: 0.6951\n",
      "Epoch 00092: val_loss improved from 0.92773 to 0.92576, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8913 - accuracy: 0.6934 - val_loss: 0.9258 - val_accuracy: 0.6958\n",
      "Epoch 93/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8925 - accuracy: 0.6971\n",
      "Epoch 00093: val_loss did not improve from 0.92576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8946 - accuracy: 0.6953 - val_loss: 0.9385 - val_accuracy: 0.6903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8894 - accuracy: 0.6929\n",
      "Epoch 00094: val_loss did not improve from 0.92576\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8879 - accuracy: 0.6907 - val_loss: 0.9451 - val_accuracy: 0.6849\n",
      "Epoch 95/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8741 - accuracy: 0.6948\n",
      "Epoch 00095: val_loss did not improve from 0.92576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8809 - accuracy: 0.6937 - val_loss: 0.9299 - val_accuracy: 0.7026\n",
      "Epoch 96/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8861 - accuracy: 0.7015\n",
      "Epoch 00096: val_loss did not improve from 0.92576\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8863 - accuracy: 0.7002 - val_loss: 0.9329 - val_accuracy: 0.6876\n",
      "Epoch 97/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8910 - accuracy: 0.6923\n",
      "Epoch 00097: val_loss improved from 0.92576 to 0.91944, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8895 - accuracy: 0.6924 - val_loss: 0.9194 - val_accuracy: 0.7012\n",
      "Epoch 98/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8768 - accuracy: 0.7005\n",
      "Epoch 00098: val_loss did not improve from 0.91944\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8759 - accuracy: 0.7018 - val_loss: 0.9250 - val_accuracy: 0.7005\n",
      "Epoch 99/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8693 - accuracy: 0.7041\n",
      "Epoch 00099: val_loss improved from 0.91944 to 0.90542, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8715 - accuracy: 0.7018 - val_loss: 0.9054 - val_accuracy: 0.7074\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8662 - accuracy: 0.7053\n",
      "Epoch 00100: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8662 - accuracy: 0.7053 - val_loss: 0.9194 - val_accuracy: 0.6937\n",
      "Epoch 101/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8756 - accuracy: 0.6976\n",
      "Epoch 00101: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8745 - accuracy: 0.6985 - val_loss: 0.9147 - val_accuracy: 0.7060\n",
      "Epoch 102/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8706 - accuracy: 0.7054\n",
      "Epoch 00102: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8745 - accuracy: 0.7026 - val_loss: 0.9067 - val_accuracy: 0.7012\n",
      "Epoch 103/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8636 - accuracy: 0.7057\n",
      "Epoch 00103: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8672 - accuracy: 0.7052 - val_loss: 0.9141 - val_accuracy: 0.6903\n",
      "Epoch 104/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8695 - accuracy: 0.7042\n",
      "Epoch 00104: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8679 - accuracy: 0.7026 - val_loss: 0.9366 - val_accuracy: 0.6958\n",
      "Epoch 105/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8690 - accuracy: 0.7109\n",
      "Epoch 00105: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8653 - accuracy: 0.7113 - val_loss: 0.9169 - val_accuracy: 0.6944\n",
      "Epoch 106/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8525 - accuracy: 0.7091\n",
      "Epoch 00106: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8597 - accuracy: 0.7064 - val_loss: 0.9111 - val_accuracy: 0.7012\n",
      "Epoch 107/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8654 - accuracy: 0.7091\n",
      "Epoch 00107: val_loss did not improve from 0.90542\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8689 - accuracy: 0.7050 - val_loss: 0.9083 - val_accuracy: 0.7040\n",
      "Epoch 108/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8677 - accuracy: 0.7024\n",
      "Epoch 00108: val_loss improved from 0.90542 to 0.90163, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8685 - accuracy: 0.7033 - val_loss: 0.9016 - val_accuracy: 0.6985\n",
      "Epoch 109/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8620 - accuracy: 0.7095\n",
      "Epoch 00109: val_loss improved from 0.90163 to 0.89266, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.8580 - accuracy: 0.7111 - val_loss: 0.8927 - val_accuracy: 0.7094\n",
      "Epoch 110/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8648 - accuracy: 0.7038\n",
      "Epoch 00110: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8674 - accuracy: 0.7031 - val_loss: 0.9067 - val_accuracy: 0.7005\n",
      "Epoch 111/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8544 - accuracy: 0.7102\n",
      "Epoch 00111: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8530 - accuracy: 0.7105 - val_loss: 0.9014 - val_accuracy: 0.7101\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8517 - accuracy: 0.7134\n",
      "Epoch 00112: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8517 - accuracy: 0.7134 - val_loss: 0.8977 - val_accuracy: 0.6965\n",
      "Epoch 113/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8536 - accuracy: 0.7070\n",
      "Epoch 00113: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8549 - accuracy: 0.7064 - val_loss: 0.8998 - val_accuracy: 0.7128\n",
      "Epoch 114/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8532 - accuracy: 0.7093\n",
      "Epoch 00114: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8559 - accuracy: 0.7089 - val_loss: 0.9095 - val_accuracy: 0.6951\n",
      "Epoch 115/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8477 - accuracy: 0.7100\n",
      "Epoch 00115: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8518 - accuracy: 0.7086 - val_loss: 0.8973 - val_accuracy: 0.7135\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8483 - accuracy: 0.7152\n",
      "Epoch 00116: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8432 - accuracy: 0.7173 - val_loss: 0.9017 - val_accuracy: 0.6971\n",
      "Epoch 117/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8444 - accuracy: 0.7131\n",
      "Epoch 00117: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8434 - accuracy: 0.7123 - val_loss: 0.8931 - val_accuracy: 0.7108\n",
      "Epoch 118/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8498 - accuracy: 0.7140\n",
      "Epoch 00118: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8506 - accuracy: 0.7149 - val_loss: 0.8963 - val_accuracy: 0.7121\n",
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8450 - accuracy: 0.7106\n",
      "Epoch 00119: val_loss did not improve from 0.89266\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8417 - accuracy: 0.7128 - val_loss: 0.8954 - val_accuracy: 0.7046\n",
      "Epoch 120/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8548 - accuracy: 0.7048\n",
      "Epoch 00120: val_loss improved from 0.89266 to 0.89036, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8494 - accuracy: 0.7081 - val_loss: 0.8904 - val_accuracy: 0.7074\n",
      "Epoch 121/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8437 - accuracy: 0.7111\n",
      "Epoch 00121: val_loss improved from 0.89036 to 0.88745, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8457 - accuracy: 0.7105 - val_loss: 0.8874 - val_accuracy: 0.7053\n",
      "Epoch 122/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8358 - accuracy: 0.7161\n",
      "Epoch 00122: val_loss did not improve from 0.88745\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8343 - accuracy: 0.7168 - val_loss: 0.8972 - val_accuracy: 0.6978\n",
      "Epoch 123/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8331 - accuracy: 0.7209\n",
      "Epoch 00123: val_loss did not improve from 0.88745\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8364 - accuracy: 0.7181 - val_loss: 0.8973 - val_accuracy: 0.7115\n",
      "Epoch 124/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8592 - accuracy: 0.7091\n",
      "Epoch 00124: val_loss improved from 0.88745 to 0.87425, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8508 - accuracy: 0.7130 - val_loss: 0.8743 - val_accuracy: 0.7053\n",
      "Epoch 125/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8332 - accuracy: 0.7152\n",
      "Epoch 00125: val_loss improved from 0.87425 to 0.86960, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8373 - accuracy: 0.7132 - val_loss: 0.8696 - val_accuracy: 0.7156\n",
      "Epoch 126/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8351 - accuracy: 0.7223\n",
      "Epoch 00126: val_loss did not improve from 0.86960\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8361 - accuracy: 0.7209 - val_loss: 0.8730 - val_accuracy: 0.7156\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8396 - accuracy: 0.7137\n",
      "Epoch 00127: val_loss did not improve from 0.86960\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8336 - accuracy: 0.7152 - val_loss: 0.8920 - val_accuracy: 0.7046\n",
      "Epoch 128/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8369 - accuracy: 0.7096\n",
      "Epoch 00128: val_loss did not improve from 0.86960\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8299 - accuracy: 0.7135 - val_loss: 0.9111 - val_accuracy: 0.6971\n",
      "Epoch 129/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.8389 - accuracy: 0.7152\n",
      "Epoch 00129: val_loss did not improve from 0.86960\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8312 - accuracy: 0.7185 - val_loss: 0.8792 - val_accuracy: 0.7196\n",
      "Epoch 130/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8368 - accuracy: 0.7173\n",
      "Epoch 00130: val_loss improved from 0.86960 to 0.86664, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8402 - accuracy: 0.7157 - val_loss: 0.8666 - val_accuracy: 0.7080\n",
      "Epoch 131/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8411 - accuracy: 0.7070\n",
      "Epoch 00131: val_loss did not improve from 0.86664\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8388 - accuracy: 0.7076 - val_loss: 0.8682 - val_accuracy: 0.7149\n",
      "Epoch 132/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8227 - accuracy: 0.7199\n",
      "Epoch 00132: val_loss improved from 0.86664 to 0.86613, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.8236 - accuracy: 0.7190 - val_loss: 0.8661 - val_accuracy: 0.7101\n",
      "Epoch 133/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8270 - accuracy: 0.7211\n",
      "Epoch 00133: val_loss did not improve from 0.86613\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8256 - accuracy: 0.7210 - val_loss: 0.8776 - val_accuracy: 0.7074\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.7185\n",
      "Epoch 00134: val_loss did not improve from 0.86613\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8303 - accuracy: 0.7185 - val_loss: 0.9239 - val_accuracy: 0.6985\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8248 - accuracy: 0.7186\n",
      "Epoch 00135: val_loss did not improve from 0.86613\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8207 - accuracy: 0.7186 - val_loss: 0.8862 - val_accuracy: 0.7019\n",
      "Epoch 136/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8190 - accuracy: 0.7227\n",
      "Epoch 00136: val_loss did not improve from 0.86613\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8203 - accuracy: 0.7214 - val_loss: 0.8894 - val_accuracy: 0.6924\n",
      "Epoch 137/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8263 - accuracy: 0.7238\n",
      "Epoch 00137: val_loss did not improve from 0.86613\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8244 - accuracy: 0.7243 - val_loss: 0.8907 - val_accuracy: 0.7026\n",
      "Epoch 138/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8203 - accuracy: 0.7184\n",
      "Epoch 00138: val_loss did not improve from 0.86613\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8189 - accuracy: 0.7176 - val_loss: 0.8726 - val_accuracy: 0.7087\n",
      "Epoch 139/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8295 - accuracy: 0.7158\n",
      "Epoch 00139: val_loss improved from 0.86613 to 0.84703, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8261 - accuracy: 0.7144 - val_loss: 0.8470 - val_accuracy: 0.7169\n",
      "Epoch 140/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8184 - accuracy: 0.7178\n",
      "Epoch 00140: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8193 - accuracy: 0.7175 - val_loss: 0.8675 - val_accuracy: 0.6978\n",
      "Epoch 141/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8130 - accuracy: 0.7212\n",
      "Epoch 00141: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8162 - accuracy: 0.7209 - val_loss: 0.8550 - val_accuracy: 0.7135\n",
      "Epoch 142/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7983 - accuracy: 0.7338\n",
      "Epoch 00142: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8037 - accuracy: 0.7297 - val_loss: 0.8555 - val_accuracy: 0.7121\n",
      "Epoch 143/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8124 - accuracy: 0.7256\n",
      "Epoch 00143: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8086 - accuracy: 0.7274 - val_loss: 0.8580 - val_accuracy: 0.7080\n",
      "Epoch 144/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8015 - accuracy: 0.7258\n",
      "Epoch 00144: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8001 - accuracy: 0.7260 - val_loss: 0.8490 - val_accuracy: 0.7203\n",
      "Epoch 145/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8101 - accuracy: 0.7245\n",
      "Epoch 00145: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8152 - accuracy: 0.7234 - val_loss: 0.8575 - val_accuracy: 0.7210\n",
      "Epoch 146/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8141 - accuracy: 0.7202\n",
      "Epoch 00146: val_loss did not improve from 0.84703\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8118 - accuracy: 0.7207 - val_loss: 0.8580 - val_accuracy: 0.7162\n",
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8025 - accuracy: 0.7294\n",
      "Epoch 00147: val_loss improved from 0.84703 to 0.84684, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8050 - accuracy: 0.7301 - val_loss: 0.8468 - val_accuracy: 0.7162\n",
      "Epoch 148/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8141 - accuracy: 0.7292\n",
      "Epoch 00148: val_loss did not improve from 0.84684\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8112 - accuracy: 0.7304 - val_loss: 0.8551 - val_accuracy: 0.7121\n",
      "Epoch 149/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8000 - accuracy: 0.7347\n",
      "Epoch 00149: val_loss did not improve from 0.84684\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8005 - accuracy: 0.7321 - val_loss: 0.8889 - val_accuracy: 0.6971\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7960 - accuracy: 0.7329\n",
      "Epoch 00150: val_loss improved from 0.84684 to 0.84427, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7988 - accuracy: 0.7311 - val_loss: 0.8443 - val_accuracy: 0.7251\n",
      "Epoch 151/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7931 - accuracy: 0.7264\n",
      "Epoch 00151: val_loss improved from 0.84427 to 0.83446, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7941 - accuracy: 0.7277 - val_loss: 0.8345 - val_accuracy: 0.7231\n",
      "Epoch 152/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.8007 - accuracy: 0.7179\n",
      "Epoch 00152: val_loss did not improve from 0.83446\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8001 - accuracy: 0.7205 - val_loss: 0.8545 - val_accuracy: 0.7101\n",
      "Epoch 153/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8061 - accuracy: 0.7264\n",
      "Epoch 00153: val_loss did not improve from 0.83446\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8026 - accuracy: 0.7280 - val_loss: 0.8634 - val_accuracy: 0.7121\n",
      "Epoch 154/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8002 - accuracy: 0.7295\n",
      "Epoch 00154: val_loss did not improve from 0.83446\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7956 - accuracy: 0.7314 - val_loss: 0.8537 - val_accuracy: 0.7156\n",
      "Epoch 155/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8049 - accuracy: 0.7292\n",
      "Epoch 00155: val_loss did not improve from 0.83446\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8059 - accuracy: 0.7279 - val_loss: 0.8519 - val_accuracy: 0.7067\n",
      "Epoch 156/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7910 - accuracy: 0.7303\n",
      "Epoch 00156: val_loss improved from 0.83446 to 0.82614, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7976 - accuracy: 0.7279 - val_loss: 0.8261 - val_accuracy: 0.7203\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7970 - accuracy: 0.7314\n",
      "Epoch 00157: val_loss did not improve from 0.82614\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7970 - accuracy: 0.7314 - val_loss: 0.8430 - val_accuracy: 0.7251\n",
      "Epoch 158/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7957 - accuracy: 0.7344\n",
      "Epoch 00158: val_loss did not improve from 0.82614\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7948 - accuracy: 0.7349 - val_loss: 0.8570 - val_accuracy: 0.7237\n",
      "Epoch 159/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7927 - accuracy: 0.7307\n",
      "Epoch 00159: val_loss did not improve from 0.82614\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7910 - accuracy: 0.7309 - val_loss: 0.8645 - val_accuracy: 0.7005\n",
      "Epoch 160/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8044 - accuracy: 0.7232\n",
      "Epoch 00160: val_loss did not improve from 0.82614\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7972 - accuracy: 0.7280 - val_loss: 0.8396 - val_accuracy: 0.7142\n",
      "Epoch 161/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7978 - accuracy: 0.7288\n",
      "Epoch 00161: val_loss improved from 0.82614 to 0.82356, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7973 - accuracy: 0.7291 - val_loss: 0.8236 - val_accuracy: 0.7271\n",
      "Epoch 162/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7902 - accuracy: 0.7335\n",
      "Epoch 00162: val_loss did not improve from 0.82356\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7886 - accuracy: 0.7343 - val_loss: 0.8418 - val_accuracy: 0.7251\n",
      "Epoch 163/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7843 - accuracy: 0.7353\n",
      "Epoch 00163: val_loss did not improve from 0.82356\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7827 - accuracy: 0.7364 - val_loss: 0.8503 - val_accuracy: 0.7217\n",
      "Epoch 164/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7834 - accuracy: 0.7407\n",
      "Epoch 00164: val_loss did not improve from 0.82356\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7827 - accuracy: 0.7393 - val_loss: 0.8561 - val_accuracy: 0.6958\n",
      "Epoch 165/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7856 - accuracy: 0.7331\n",
      "Epoch 00165: val_loss did not improve from 0.82356\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7881 - accuracy: 0.7320 - val_loss: 0.8497 - val_accuracy: 0.7115\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7927 - accuracy: 0.7292\n",
      "Epoch 00166: val_loss did not improve from 0.82356\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7877 - accuracy: 0.7304 - val_loss: 0.8679 - val_accuracy: 0.7060\n",
      "Epoch 167/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8018 - accuracy: 0.7325\n",
      "Epoch 00167: val_loss improved from 0.82356 to 0.81949, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7983 - accuracy: 0.7320 - val_loss: 0.8195 - val_accuracy: 0.7312\n",
      "Epoch 168/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7742 - accuracy: 0.7368\n",
      "Epoch 00168: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7818 - accuracy: 0.7342 - val_loss: 0.8468 - val_accuracy: 0.7067\n",
      "Epoch 169/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7701 - accuracy: 0.7433\n",
      "Epoch 00169: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7745 - accuracy: 0.7398 - val_loss: 0.8369 - val_accuracy: 0.7203\n",
      "Epoch 170/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7818 - accuracy: 0.7364\n",
      "Epoch 00170: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7789 - accuracy: 0.7378 - val_loss: 0.8227 - val_accuracy: 0.7244\n",
      "Epoch 171/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7838 - accuracy: 0.7327\n",
      "Epoch 00171: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7787 - accuracy: 0.7345 - val_loss: 0.8547 - val_accuracy: 0.7156\n",
      "Epoch 172/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7883 - accuracy: 0.7347\n",
      "Epoch 00172: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7822 - accuracy: 0.7371 - val_loss: 0.8568 - val_accuracy: 0.7162\n",
      "Epoch 173/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7717 - accuracy: 0.7401\n",
      "Epoch 00173: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7772 - accuracy: 0.7384 - val_loss: 0.8227 - val_accuracy: 0.7237\n",
      "Epoch 174/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7695 - accuracy: 0.7437\n",
      "Epoch 00174: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7720 - accuracy: 0.7417 - val_loss: 0.8433 - val_accuracy: 0.7080\n",
      "Epoch 175/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7751 - accuracy: 0.7409\n",
      "Epoch 00175: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7810 - accuracy: 0.7384 - val_loss: 0.8290 - val_accuracy: 0.7121\n",
      "Epoch 176/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7818 - accuracy: 0.7327\n",
      "Epoch 00176: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7802 - accuracy: 0.7349 - val_loss: 0.8320 - val_accuracy: 0.7258\n",
      "Epoch 177/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7755 - accuracy: 0.7422\n",
      "Epoch 00177: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7761 - accuracy: 0.7408 - val_loss: 0.8515 - val_accuracy: 0.7237\n",
      "Epoch 178/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7770 - accuracy: 0.7346\n",
      "Epoch 00178: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7760 - accuracy: 0.7359 - val_loss: 0.8389 - val_accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7715 - accuracy: 0.7387\n",
      "Epoch 00179: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7696 - accuracy: 0.7381 - val_loss: 0.8460 - val_accuracy: 0.7231\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7614 - accuracy: 0.7420\n",
      "Epoch 00180: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7614 - accuracy: 0.7420 - val_loss: 0.8259 - val_accuracy: 0.7244\n",
      "Epoch 181/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7664 - accuracy: 0.7483\n",
      "Epoch 00181: val_loss did not improve from 0.81949\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7704 - accuracy: 0.7466 - val_loss: 0.8291 - val_accuracy: 0.7149\n",
      "Epoch 182/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7719 - accuracy: 0.7355\n",
      "Epoch 00182: val_loss improved from 0.81949 to 0.80610, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7656 - accuracy: 0.7372 - val_loss: 0.8061 - val_accuracy: 0.7340\n",
      "Epoch 183/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7715 - accuracy: 0.7420\n",
      "Epoch 00183: val_loss did not improve from 0.80610\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7660 - accuracy: 0.7424 - val_loss: 0.8204 - val_accuracy: 0.7258\n",
      "Epoch 184/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7635 - accuracy: 0.7455\n",
      "Epoch 00184: val_loss did not improve from 0.80610\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7632 - accuracy: 0.7453 - val_loss: 0.8468 - val_accuracy: 0.7217\n",
      "Epoch 185/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7723 - accuracy: 0.7418\n",
      "Epoch 00185: val_loss did not improve from 0.80610\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7710 - accuracy: 0.7424 - val_loss: 0.8315 - val_accuracy: 0.7265\n",
      "Epoch 186/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7677 - accuracy: 0.7401\n",
      "Epoch 00186: val_loss did not improve from 0.80610\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7636 - accuracy: 0.7405 - val_loss: 0.8458 - val_accuracy: 0.7033\n",
      "Epoch 187/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7648 - accuracy: 0.7444\n",
      "Epoch 00187: val_loss improved from 0.80610 to 0.80294, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7693 - accuracy: 0.7430 - val_loss: 0.8029 - val_accuracy: 0.7374\n",
      "Epoch 188/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7573 - accuracy: 0.7457\n",
      "Epoch 00188: val_loss did not improve from 0.80294\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7599 - accuracy: 0.7451 - val_loss: 0.8405 - val_accuracy: 0.7231\n",
      "Epoch 189/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7730 - accuracy: 0.7390\n",
      "Epoch 00189: val_loss did not improve from 0.80294\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7669 - accuracy: 0.7401 - val_loss: 0.8178 - val_accuracy: 0.7340\n",
      "Epoch 190/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7622 - accuracy: 0.7440\n",
      "Epoch 00190: val_loss did not improve from 0.80294\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7616 - accuracy: 0.7432 - val_loss: 0.8110 - val_accuracy: 0.7278\n",
      "Epoch 191/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7646 - accuracy: 0.7408\n",
      "Epoch 00191: val_loss did not improve from 0.80294\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7618 - accuracy: 0.7441 - val_loss: 0.8079 - val_accuracy: 0.7367\n",
      "Epoch 192/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7680 - accuracy: 0.7411\n",
      "Epoch 00192: val_loss improved from 0.80294 to 0.80009, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7602 - accuracy: 0.7432 - val_loss: 0.8001 - val_accuracy: 0.7333\n",
      "Epoch 193/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7563 - accuracy: 0.7413\n",
      "Epoch 00193: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7614 - accuracy: 0.7403 - val_loss: 0.8229 - val_accuracy: 0.7258\n",
      "Epoch 194/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7536 - accuracy: 0.7455\n",
      "Epoch 00194: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7536 - accuracy: 0.7478 - val_loss: 0.8111 - val_accuracy: 0.7285\n",
      "Epoch 195/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7487 - accuracy: 0.7511\n",
      "Epoch 00195: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7510 - accuracy: 0.7502 - val_loss: 0.8514 - val_accuracy: 0.7094\n",
      "Epoch 196/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7417 - accuracy: 0.7489\n",
      "Epoch 00196: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7485 - accuracy: 0.7468 - val_loss: 0.8529 - val_accuracy: 0.7087\n",
      "Epoch 197/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7627 - accuracy: 0.7439\n",
      "Epoch 00197: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7552 - accuracy: 0.7458 - val_loss: 0.8290 - val_accuracy: 0.7237\n",
      "Epoch 198/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7535 - accuracy: 0.7472\n",
      "Epoch 00198: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7539 - accuracy: 0.7471 - val_loss: 0.8155 - val_accuracy: 0.7251\n",
      "Epoch 199/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7624 - accuracy: 0.7426\n",
      "Epoch 00199: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7564 - accuracy: 0.7454 - val_loss: 0.8082 - val_accuracy: 0.7374\n",
      "Epoch 200/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7531 - accuracy: 0.7547\n",
      "Epoch 00200: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7535 - accuracy: 0.7529 - val_loss: 0.8218 - val_accuracy: 0.7285\n",
      "Epoch 201/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7547 - accuracy: 0.7474\n",
      "Epoch 00201: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7551 - accuracy: 0.7458 - val_loss: 0.8031 - val_accuracy: 0.7347\n",
      "Epoch 202/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7487 - accuracy: 0.7505\n",
      "Epoch 00202: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7463 - accuracy: 0.7517 - val_loss: 0.8033 - val_accuracy: 0.7326\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7476 - accuracy: 0.7473\n",
      "Epoch 00203: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7476 - accuracy: 0.7473 - val_loss: 0.8252 - val_accuracy: 0.7265\n",
      "Epoch 204/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7520 - accuracy: 0.7442\n",
      "Epoch 00204: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7494 - accuracy: 0.7458 - val_loss: 0.8178 - val_accuracy: 0.7251\n",
      "Epoch 205/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7512 - accuracy: 0.7459\n",
      "Epoch 00205: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7474 - accuracy: 0.7485 - val_loss: 0.8458 - val_accuracy: 0.7121\n",
      "Epoch 206/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7475 - accuracy: 0.7511\n",
      "Epoch 00206: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7466 - accuracy: 0.7490 - val_loss: 0.8259 - val_accuracy: 0.7176\n",
      "Epoch 207/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7458 - accuracy: 0.7522\n",
      "Epoch 00207: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7448 - accuracy: 0.7529 - val_loss: 0.8144 - val_accuracy: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.7486 - accuracy: 0.7414\n",
      "Epoch 00208: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7489 - accuracy: 0.7463 - val_loss: 0.8180 - val_accuracy: 0.7265\n",
      "Epoch 209/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7502 - accuracy: 0.7500\n",
      "Epoch 00209: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7498 - accuracy: 0.7492 - val_loss: 0.8002 - val_accuracy: 0.7306\n",
      "Epoch 210/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7456 - accuracy: 0.7496\n",
      "Epoch 00210: val_loss did not improve from 0.80009\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7444 - accuracy: 0.7485 - val_loss: 0.8096 - val_accuracy: 0.7196\n",
      "Epoch 211/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7551 - accuracy: 0.7506\n",
      "Epoch 00211: val_loss improved from 0.80009 to 0.79929, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7514 - accuracy: 0.7506 - val_loss: 0.7993 - val_accuracy: 0.7360\n",
      "Epoch 212/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7397 - accuracy: 0.7473\n",
      "Epoch 00212: val_loss did not improve from 0.79929\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7413 - accuracy: 0.7463 - val_loss: 0.8058 - val_accuracy: 0.7292\n",
      "Epoch 213/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7439 - accuracy: 0.7522\n",
      "Epoch 00213: val_loss did not improve from 0.79929\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7423 - accuracy: 0.7517 - val_loss: 0.8328 - val_accuracy: 0.7149\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7414 - accuracy: 0.7492\n",
      "Epoch 00214: val_loss improved from 0.79929 to 0.78985, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7414 - accuracy: 0.7492 - val_loss: 0.7899 - val_accuracy: 0.7381\n",
      "Epoch 215/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7347 - accuracy: 0.7559\n",
      "Epoch 00215: val_loss did not improve from 0.78985\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7359 - accuracy: 0.7557 - val_loss: 0.8092 - val_accuracy: 0.7251\n",
      "Epoch 216/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7386 - accuracy: 0.7500\n",
      "Epoch 00216: val_loss did not improve from 0.78985\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7371 - accuracy: 0.7507 - val_loss: 0.8198 - val_accuracy: 0.7271\n",
      "Epoch 217/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7336 - accuracy: 0.7532\n",
      "Epoch 00217: val_loss did not improve from 0.78985\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7403 - accuracy: 0.7497 - val_loss: 0.8195 - val_accuracy: 0.7190\n",
      "Epoch 218/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7456 - accuracy: 0.7480\n",
      "Epoch 00218: val_loss did not improve from 0.78985\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7441 - accuracy: 0.7475 - val_loss: 0.8208 - val_accuracy: 0.7251\n",
      "Epoch 219/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7482 - accuracy: 0.7443\n",
      "Epoch 00219: val_loss did not improve from 0.78985\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7484 - accuracy: 0.7441 - val_loss: 0.7972 - val_accuracy: 0.7306\n",
      "Epoch 220/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7311 - accuracy: 0.7534\n",
      "Epoch 00220: val_loss improved from 0.78985 to 0.78780, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7354 - accuracy: 0.7512 - val_loss: 0.7878 - val_accuracy: 0.7374\n",
      "Epoch 221/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7440 - accuracy: 0.7427\n",
      "Epoch 00221: val_loss did not improve from 0.78780\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7478 - accuracy: 0.7429 - val_loss: 0.8174 - val_accuracy: 0.7251\n",
      "Epoch 222/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7521 - accuracy: 0.7452\n",
      "Epoch 00222: val_loss did not improve from 0.78780\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7499 - accuracy: 0.7463 - val_loss: 0.8050 - val_accuracy: 0.7312\n",
      "Epoch 223/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7322 - accuracy: 0.7444\n",
      "Epoch 00223: val_loss improved from 0.78780 to 0.77384, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7339 - accuracy: 0.7449 - val_loss: 0.7738 - val_accuracy: 0.7538\n",
      "Epoch 224/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7428 - accuracy: 0.7511\n",
      "Epoch 00224: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7403 - accuracy: 0.7507 - val_loss: 0.7999 - val_accuracy: 0.7401\n",
      "Epoch 225/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7270 - accuracy: 0.7514\n",
      "Epoch 00225: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7280 - accuracy: 0.7524 - val_loss: 0.8188 - val_accuracy: 0.7278\n",
      "Epoch 226/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7345 - accuracy: 0.7552\n",
      "Epoch 00226: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7370 - accuracy: 0.7538 - val_loss: 0.7916 - val_accuracy: 0.7347\n",
      "Epoch 227/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7214 - accuracy: 0.7576\n",
      "Epoch 00227: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7275 - accuracy: 0.7572 - val_loss: 0.8127 - val_accuracy: 0.7333\n",
      "Epoch 228/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7407 - accuracy: 0.7487\n",
      "Epoch 00228: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7401 - accuracy: 0.7490 - val_loss: 0.7984 - val_accuracy: 0.7360\n",
      "Epoch 229/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7248 - accuracy: 0.7554\n",
      "Epoch 00229: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7274 - accuracy: 0.7552 - val_loss: 0.8159 - val_accuracy: 0.7333\n",
      "Epoch 230/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7337 - accuracy: 0.7578\n",
      "Epoch 00230: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7305 - accuracy: 0.7569 - val_loss: 0.8383 - val_accuracy: 0.7196\n",
      "Epoch 231/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7416 - accuracy: 0.7544\n",
      "Epoch 00231: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7396 - accuracy: 0.7545 - val_loss: 0.8089 - val_accuracy: 0.7244\n",
      "Epoch 232/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7277 - accuracy: 0.7532\n",
      "Epoch 00232: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7331 - accuracy: 0.7511 - val_loss: 0.7945 - val_accuracy: 0.7271\n",
      "Epoch 233/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7277 - accuracy: 0.7506\n",
      "Epoch 00233: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7286 - accuracy: 0.7495 - val_loss: 0.7801 - val_accuracy: 0.7401\n",
      "Epoch 234/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7313 - accuracy: 0.7483\n",
      "Epoch 00234: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7324 - accuracy: 0.7492 - val_loss: 0.7951 - val_accuracy: 0.7340\n",
      "Epoch 235/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7231 - accuracy: 0.7586\n",
      "Epoch 00235: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7230 - accuracy: 0.7586 - val_loss: 0.8241 - val_accuracy: 0.7265\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7317 - accuracy: 0.7543\n",
      "Epoch 00236: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7317 - accuracy: 0.7543 - val_loss: 0.7996 - val_accuracy: 0.7306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7163 - accuracy: 0.7599\n",
      "Epoch 00237: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7231 - accuracy: 0.7575 - val_loss: 0.7913 - val_accuracy: 0.7381\n",
      "Epoch 238/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7253 - accuracy: 0.7608\n",
      "Epoch 00238: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7277 - accuracy: 0.7603 - val_loss: 0.8041 - val_accuracy: 0.7196\n",
      "Epoch 239/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7230 - accuracy: 0.7539\n",
      "Epoch 00239: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7290 - accuracy: 0.7509 - val_loss: 0.7983 - val_accuracy: 0.7326\n",
      "Epoch 240/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7296 - accuracy: 0.7576\n",
      "Epoch 00240: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7331 - accuracy: 0.7553 - val_loss: 0.7975 - val_accuracy: 0.7408\n",
      "Epoch 241/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7193 - accuracy: 0.7589\n",
      "Epoch 00241: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7225 - accuracy: 0.7567 - val_loss: 0.7792 - val_accuracy: 0.7408\n",
      "Epoch 242/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7234 - accuracy: 0.7518\n",
      "Epoch 00242: val_loss did not improve from 0.77384\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7201 - accuracy: 0.7526 - val_loss: 0.7813 - val_accuracy: 0.7415\n",
      "Epoch 243/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7230 - accuracy: 0.7576\n",
      "Epoch 00243: val_loss improved from 0.77384 to 0.77372, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7243 - accuracy: 0.7565 - val_loss: 0.7737 - val_accuracy: 0.7394\n",
      "Epoch 244/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7299 - accuracy: 0.7519\n",
      "Epoch 00244: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7314 - accuracy: 0.7517 - val_loss: 0.8169 - val_accuracy: 0.7326\n",
      "Epoch 245/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7179 - accuracy: 0.7573\n",
      "Epoch 00245: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7246 - accuracy: 0.7552 - val_loss: 0.8281 - val_accuracy: 0.7285\n",
      "Epoch 246/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7184 - accuracy: 0.7535\n",
      "Epoch 00246: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7233 - accuracy: 0.7521 - val_loss: 0.7786 - val_accuracy: 0.7394\n",
      "Epoch 247/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7274 - accuracy: 0.7552\n",
      "Epoch 00247: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7291 - accuracy: 0.7541 - val_loss: 0.7861 - val_accuracy: 0.7367\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.7589\n",
      "Epoch 00248: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7235 - accuracy: 0.7589 - val_loss: 0.8120 - val_accuracy: 0.7271\n",
      "Epoch 249/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7196 - accuracy: 0.7537\n",
      "Epoch 00249: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7158 - accuracy: 0.7555 - val_loss: 0.7928 - val_accuracy: 0.7326\n",
      "Epoch 250/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7198 - accuracy: 0.7571\n",
      "Epoch 00250: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7200 - accuracy: 0.7572 - val_loss: 0.8309 - val_accuracy: 0.7217\n",
      "Epoch 251/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7127 - accuracy: 0.7626\n",
      "Epoch 00251: val_loss did not improve from 0.77372\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7182 - accuracy: 0.7589 - val_loss: 0.8058 - val_accuracy: 0.7292\n",
      "Epoch 252/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7162 - accuracy: 0.7578\n",
      "Epoch 00252: val_loss improved from 0.77372 to 0.76371, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7186 - accuracy: 0.7567 - val_loss: 0.7637 - val_accuracy: 0.7469\n",
      "Epoch 253/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7175 - accuracy: 0.7543\n",
      "Epoch 00253: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7196 - accuracy: 0.7521 - val_loss: 0.7639 - val_accuracy: 0.7497\n",
      "Epoch 254/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7041 - accuracy: 0.7599\n",
      "Epoch 00254: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7100 - accuracy: 0.7577 - val_loss: 0.7935 - val_accuracy: 0.7333\n",
      "Epoch 255/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7110 - accuracy: 0.7582\n",
      "Epoch 00255: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7076 - accuracy: 0.7584 - val_loss: 0.7901 - val_accuracy: 0.7353\n",
      "Epoch 256/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7092 - accuracy: 0.7615\n",
      "Epoch 00256: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7098 - accuracy: 0.7608 - val_loss: 0.7667 - val_accuracy: 0.7456\n",
      "Epoch 257/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6981 - accuracy: 0.7610\n",
      "Epoch 00257: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7047 - accuracy: 0.7599 - val_loss: 0.7962 - val_accuracy: 0.7347\n",
      "Epoch 258/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7083 - accuracy: 0.7662\n",
      "Epoch 00258: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7037 - accuracy: 0.7681 - val_loss: 0.7801 - val_accuracy: 0.7401\n",
      "Epoch 259/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.7058 - accuracy: 0.7584\n",
      "Epoch 00259: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7060 - accuracy: 0.7596 - val_loss: 0.7816 - val_accuracy: 0.7408\n",
      "Epoch 260/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7076 - accuracy: 0.7617\n",
      "Epoch 00260: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7076 - accuracy: 0.7627 - val_loss: 0.7863 - val_accuracy: 0.7387\n",
      "Epoch 261/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7052 - accuracy: 0.7569\n",
      "Epoch 00261: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7060 - accuracy: 0.7575 - val_loss: 0.7911 - val_accuracy: 0.7340\n",
      "Epoch 262/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7051 - accuracy: 0.7587\n",
      "Epoch 00262: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7030 - accuracy: 0.7594 - val_loss: 0.8024 - val_accuracy: 0.7278\n",
      "Epoch 263/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7082 - accuracy: 0.7636\n",
      "Epoch 00263: val_loss did not improve from 0.76371\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.6989 - accuracy: 0.7674 - val_loss: 0.7741 - val_accuracy: 0.7353\n",
      "Epoch 264/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7044 - accuracy: 0.7571\n",
      "Epoch 00264: val_loss improved from 0.76371 to 0.76273, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7062 - accuracy: 0.7562 - val_loss: 0.7627 - val_accuracy: 0.7422\n",
      "Epoch 265/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7101 - accuracy: 0.7573\n",
      "Epoch 00265: val_loss improved from 0.76273 to 0.76140, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7092 - accuracy: 0.7574 - val_loss: 0.7614 - val_accuracy: 0.7401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7046 - accuracy: 0.7623\n",
      "Epoch 00266: val_loss did not improve from 0.76140\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7049 - accuracy: 0.7623 - val_loss: 0.7681 - val_accuracy: 0.7408\n",
      "Epoch 267/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7028 - accuracy: 0.7682\n",
      "Epoch 00267: val_loss did not improve from 0.76140\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7019 - accuracy: 0.7692 - val_loss: 0.7936 - val_accuracy: 0.7374\n",
      "Epoch 268/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7052 - accuracy: 0.7619\n",
      "Epoch 00268: val_loss did not improve from 0.76140\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7002 - accuracy: 0.7632 - val_loss: 0.8057 - val_accuracy: 0.7333\n",
      "Epoch 269/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7079 - accuracy: 0.7654\n",
      "Epoch 00269: val_loss improved from 0.76140 to 0.75691, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7078 - accuracy: 0.7639 - val_loss: 0.7569 - val_accuracy: 0.7476\n",
      "Epoch 270/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7065 - accuracy: 0.7644\n",
      "Epoch 00270: val_loss did not improve from 0.75691\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7045 - accuracy: 0.7647 - val_loss: 0.7774 - val_accuracy: 0.7306\n",
      "Epoch 271/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7021 - accuracy: 0.7641\n",
      "Epoch 00271: val_loss did not improve from 0.75691\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6988 - accuracy: 0.7644 - val_loss: 0.7583 - val_accuracy: 0.7497\n",
      "Epoch 272/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6981 - accuracy: 0.7617\n",
      "Epoch 00272: val_loss did not improve from 0.75691\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6967 - accuracy: 0.7628 - val_loss: 0.7631 - val_accuracy: 0.7462\n",
      "Epoch 273/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6971 - accuracy: 0.7712\n",
      "Epoch 00273: val_loss did not improve from 0.75691\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6962 - accuracy: 0.7692 - val_loss: 0.7737 - val_accuracy: 0.7435\n",
      "Epoch 00273: early stopping\n",
      "Training for model  2  completed in time:  0:01:45.864726 seconds\n",
      "Training for model  3  has started.\n",
      "Epoch 1/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 3.6935 - accuracy: 0.1817\n",
      "Epoch 00001: val_loss improved from inf to 2.15968, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 3.5631 - accuracy: 0.1858 - val_loss: 2.1597 - val_accuracy: 0.2360\n",
      "Epoch 2/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.9733 - accuracy: 0.2561\n",
      "Epoch 00002: val_loss improved from 2.15968 to 1.89093, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.9629 - accuracy: 0.2592 - val_loss: 1.8909 - val_accuracy: 0.3308\n",
      "Epoch 3/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.7117 - accuracy: 0.3890\n",
      "Epoch 00003: val_loss improved from 1.89093 to 1.61845, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.6956 - accuracy: 0.3943 - val_loss: 1.6184 - val_accuracy: 0.4161\n",
      "Epoch 4/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.5132 - accuracy: 0.4680\n",
      "Epoch 00004: val_loss improved from 1.61845 to 1.51253, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.5141 - accuracy: 0.4673 - val_loss: 1.5125 - val_accuracy: 0.4645\n",
      "Epoch 5/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4202 - accuracy: 0.4994\n",
      "Epoch 00005: val_loss improved from 1.51253 to 1.42468, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.4171 - accuracy: 0.5009 - val_loss: 1.4247 - val_accuracy: 0.5191\n",
      "Epoch 6/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3340 - accuracy: 0.5257\n",
      "Epoch 00006: val_loss improved from 1.42468 to 1.38040, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3369 - accuracy: 0.5252 - val_loss: 1.3804 - val_accuracy: 0.5321\n",
      "Epoch 7/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2796 - accuracy: 0.5538\n",
      "Epoch 00007: val_loss improved from 1.38040 to 1.28677, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.2749 - accuracy: 0.5547 - val_loss: 1.2868 - val_accuracy: 0.5539\n",
      "Epoch 8/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2178 - accuracy: 0.5724\n",
      "Epoch 00008: val_loss improved from 1.28677 to 1.22516, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.2139 - accuracy: 0.5728 - val_loss: 1.2252 - val_accuracy: 0.5784\n",
      "Epoch 9/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1870 - accuracy: 0.5858\n",
      "Epoch 00009: val_loss did not improve from 1.22516\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.1857 - accuracy: 0.5859 - val_loss: 1.2525 - val_accuracy: 0.5587\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1672 - accuracy: 0.5835\n",
      "Epoch 00010: val_loss improved from 1.22516 to 1.17664, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.1618 - accuracy: 0.5840 - val_loss: 1.1766 - val_accuracy: 0.5941\n",
      "Epoch 11/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1219 - accuracy: 0.6084\n",
      "Epoch 00011: val_loss did not improve from 1.17664\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.1179 - accuracy: 0.6105 - val_loss: 1.1893 - val_accuracy: 0.5839\n",
      "Epoch 12/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0977 - accuracy: 0.6181\n",
      "Epoch 00012: val_loss improved from 1.17664 to 1.10166, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.0938 - accuracy: 0.6200 - val_loss: 1.1017 - val_accuracy: 0.6378\n",
      "Epoch 13/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0773 - accuracy: 0.6248\n",
      "Epoch 00013: val_loss improved from 1.10166 to 1.09669, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.0744 - accuracy: 0.6258 - val_loss: 1.0967 - val_accuracy: 0.6330\n",
      "Epoch 14/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0508 - accuracy: 0.6373\n",
      "Epoch 00014: val_loss did not improve from 1.09669\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.0535 - accuracy: 0.6354 - val_loss: 1.1159 - val_accuracy: 0.6112\n",
      "Epoch 15/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0235 - accuracy: 0.6460\n",
      "Epoch 00015: val_loss improved from 1.09669 to 1.05135, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.0226 - accuracy: 0.6475 - val_loss: 1.0513 - val_accuracy: 0.6480\n",
      "Epoch 16/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0168 - accuracy: 0.6427\n",
      "Epoch 00016: val_loss did not improve from 1.05135\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.0154 - accuracy: 0.6451 - val_loss: 1.0596 - val_accuracy: 0.6501\n",
      "Epoch 17/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0036 - accuracy: 0.6443\n",
      "Epoch 00017: val_loss improved from 1.05135 to 1.02788, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.0066 - accuracy: 0.6439 - val_loss: 1.0279 - val_accuracy: 0.6637\n",
      "Epoch 18/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9906 - accuracy: 0.6577\n",
      "Epoch 00018: val_loss did not improve from 1.02788\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9940 - accuracy: 0.6572 - val_loss: 1.0424 - val_accuracy: 0.6501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9783 - accuracy: 0.6570\n",
      "Epoch 00019: val_loss did not improve from 1.02788\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9732 - accuracy: 0.6606 - val_loss: 1.0651 - val_accuracy: 0.6310\n",
      "Epoch 20/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9529 - accuracy: 0.6707\n",
      "Epoch 00020: val_loss improved from 1.02788 to 0.98757, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.9521 - accuracy: 0.6726 - val_loss: 0.9876 - val_accuracy: 0.6685\n",
      "Epoch 21/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9451 - accuracy: 0.6802\n",
      "Epoch 00021: val_loss improved from 0.98757 to 0.96812, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9459 - accuracy: 0.6796 - val_loss: 0.9681 - val_accuracy: 0.6821\n",
      "Epoch 22/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9422 - accuracy: 0.6788\n",
      "Epoch 00022: val_loss did not improve from 0.96812\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.9386 - accuracy: 0.6803 - val_loss: 0.9866 - val_accuracy: 0.6664\n",
      "Epoch 23/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9286 - accuracy: 0.6812\n",
      "Epoch 00023: val_loss did not improve from 0.96812\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9338 - accuracy: 0.6832 - val_loss: 0.9867 - val_accuracy: 0.6780\n",
      "Epoch 24/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.9286 - accuracy: 0.6773\n",
      "Epoch 00024: val_loss improved from 0.96812 to 0.94326, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.9236 - accuracy: 0.6784 - val_loss: 0.9433 - val_accuracy: 0.6917\n",
      "Epoch 25/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8934 - accuracy: 0.6938\n",
      "Epoch 00025: val_loss did not improve from 0.94326\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.8941 - accuracy: 0.6953 - val_loss: 0.9608 - val_accuracy: 0.6712\n",
      "Epoch 26/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8873 - accuracy: 0.6966\n",
      "Epoch 00026: val_loss did not improve from 0.94326\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8889 - accuracy: 0.6978 - val_loss: 0.9620 - val_accuracy: 0.6787\n",
      "Epoch 27/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8903 - accuracy: 0.6875\n",
      "Epoch 00027: val_loss improved from 0.94326 to 0.93345, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.8857 - accuracy: 0.6884 - val_loss: 0.9335 - val_accuracy: 0.6849\n",
      "Epoch 28/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8826 - accuracy: 0.7015\n",
      "Epoch 00028: val_loss improved from 0.93345 to 0.92281, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.8862 - accuracy: 0.6995 - val_loss: 0.9228 - val_accuracy: 0.6924\n",
      "Epoch 29/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8600 - accuracy: 0.7080\n",
      "Epoch 00029: val_loss improved from 0.92281 to 0.91626, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.8727 - accuracy: 0.7045 - val_loss: 0.9163 - val_accuracy: 0.6842\n",
      "Epoch 30/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8669 - accuracy: 0.6970\n",
      "Epoch 00030: val_loss improved from 0.91626 to 0.89027, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.8675 - accuracy: 0.6999 - val_loss: 0.8903 - val_accuracy: 0.7060\n",
      "Epoch 31/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8452 - accuracy: 0.7135\n",
      "Epoch 00031: val_loss did not improve from 0.89027\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8515 - accuracy: 0.7103 - val_loss: 0.8934 - val_accuracy: 0.7026\n",
      "Epoch 32/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8327 - accuracy: 0.7165\n",
      "Epoch 00032: val_loss improved from 0.89027 to 0.87791, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.8342 - accuracy: 0.7175 - val_loss: 0.8779 - val_accuracy: 0.7135\n",
      "Epoch 33/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8317 - accuracy: 0.7126\n",
      "Epoch 00033: val_loss did not improve from 0.87791\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8312 - accuracy: 0.7144 - val_loss: 0.8886 - val_accuracy: 0.7026\n",
      "Epoch 34/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8386 - accuracy: 0.7180\n",
      "Epoch 00034: val_loss did not improve from 0.87791\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.8387 - accuracy: 0.7195 - val_loss: 0.9057 - val_accuracy: 0.6924\n",
      "Epoch 35/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8161 - accuracy: 0.7256\n",
      "Epoch 00035: val_loss improved from 0.87791 to 0.84929, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.8171 - accuracy: 0.7248 - val_loss: 0.8493 - val_accuracy: 0.7162\n",
      "Epoch 36/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7941 - accuracy: 0.7347\n",
      "Epoch 00036: val_loss improved from 0.84929 to 0.84804, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7962 - accuracy: 0.7345 - val_loss: 0.8480 - val_accuracy: 0.7203\n",
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7932 - accuracy: 0.7323\n",
      "Epoch 00037: val_loss improved from 0.84804 to 0.82578, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7924 - accuracy: 0.7308 - val_loss: 0.8258 - val_accuracy: 0.7292\n",
      "Epoch 38/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7980 - accuracy: 0.7294\n",
      "Epoch 00038: val_loss improved from 0.82578 to 0.81736, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.7954 - accuracy: 0.7321 - val_loss: 0.8174 - val_accuracy: 0.7381\n",
      "Epoch 39/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7895 - accuracy: 0.7329\n",
      "Epoch 00039: val_loss did not improve from 0.81736\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7874 - accuracy: 0.7332 - val_loss: 0.8400 - val_accuracy: 0.7162\n",
      "Epoch 40/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7770 - accuracy: 0.7342\n",
      "Epoch 00040: val_loss did not improve from 0.81736\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7781 - accuracy: 0.7357 - val_loss: 0.8361 - val_accuracy: 0.7156\n",
      "Epoch 41/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7693 - accuracy: 0.7427\n",
      "Epoch 00041: val_loss improved from 0.81736 to 0.79699, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7642 - accuracy: 0.7465 - val_loss: 0.7970 - val_accuracy: 0.7387\n",
      "Epoch 42/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7597 - accuracy: 0.7440\n",
      "Epoch 00042: val_loss did not improve from 0.79699\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7619 - accuracy: 0.7427 - val_loss: 0.8203 - val_accuracy: 0.7333\n",
      "Epoch 43/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7485 - accuracy: 0.7452\n",
      "Epoch 00043: val_loss did not improve from 0.79699\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7509 - accuracy: 0.7442 - val_loss: 0.8020 - val_accuracy: 0.7347\n",
      "Epoch 44/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7480 - accuracy: 0.7468\n",
      "Epoch 00044: val_loss did not improve from 0.79699\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7449 - accuracy: 0.7488 - val_loss: 0.8026 - val_accuracy: 0.7319\n",
      "Epoch 45/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7344 - accuracy: 0.7589\n",
      "Epoch 00045: val_loss did not improve from 0.79699\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7364 - accuracy: 0.7584 - val_loss: 0.8020 - val_accuracy: 0.7353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7267 - accuracy: 0.7537\n",
      "Epoch 00046: val_loss improved from 0.79699 to 0.78848, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.7346 - accuracy: 0.7521 - val_loss: 0.7885 - val_accuracy: 0.7422\n",
      "Epoch 47/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.7326 - accuracy: 0.7509\n",
      "Epoch 00047: val_loss did not improve from 0.78848\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.7314 - accuracy: 0.7521 - val_loss: 0.7885 - val_accuracy: 0.7428\n",
      "Epoch 48/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7272 - accuracy: 0.7574\n",
      "Epoch 00048: val_loss improved from 0.78848 to 0.78461, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.7237 - accuracy: 0.7591 - val_loss: 0.7846 - val_accuracy: 0.7435\n",
      "Epoch 49/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7424 - accuracy: 0.7526\n",
      "Epoch 00049: val_loss did not improve from 0.78461\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7325 - accuracy: 0.7550 - val_loss: 0.8000 - val_accuracy: 0.7442\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7060 - accuracy: 0.7643\n",
      "Epoch 00050: val_loss improved from 0.78461 to 0.75042, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.7108 - accuracy: 0.7625 - val_loss: 0.7504 - val_accuracy: 0.7578\n",
      "Epoch 51/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7024 - accuracy: 0.7621\n",
      "Epoch 00051: val_loss did not improve from 0.75042\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7023 - accuracy: 0.7632 - val_loss: 0.7555 - val_accuracy: 0.7524\n",
      "Epoch 52/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6873 - accuracy: 0.7714\n",
      "Epoch 00052: val_loss did not improve from 0.75042\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6889 - accuracy: 0.7712 - val_loss: 0.7589 - val_accuracy: 0.7558\n",
      "Epoch 53/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7001 - accuracy: 0.7699\n",
      "Epoch 00053: val_loss improved from 0.75042 to 0.74816, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6943 - accuracy: 0.7709 - val_loss: 0.7482 - val_accuracy: 0.7572\n",
      "Epoch 54/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6955 - accuracy: 0.7662\n",
      "Epoch 00054: val_loss did not improve from 0.74816\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6901 - accuracy: 0.7674 - val_loss: 0.7945 - val_accuracy: 0.7367\n",
      "Epoch 55/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6821 - accuracy: 0.7716\n",
      "Epoch 00055: val_loss improved from 0.74816 to 0.72887, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.6822 - accuracy: 0.7722 - val_loss: 0.7289 - val_accuracy: 0.7729\n",
      "Epoch 56/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6797 - accuracy: 0.7751\n",
      "Epoch 00056: val_loss did not improve from 0.72887\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6827 - accuracy: 0.7727 - val_loss: 0.7373 - val_accuracy: 0.7694\n",
      "Epoch 57/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6718 - accuracy: 0.7785\n",
      "Epoch 00057: val_loss improved from 0.72887 to 0.72056, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6679 - accuracy: 0.7799 - val_loss: 0.7206 - val_accuracy: 0.7722\n",
      "Epoch 58/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6775 - accuracy: 0.7755\n",
      "Epoch 00058: val_loss did not improve from 0.72056\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6795 - accuracy: 0.7729 - val_loss: 0.7524 - val_accuracy: 0.7599\n",
      "Epoch 59/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6613 - accuracy: 0.7818\n",
      "Epoch 00059: val_loss improved from 0.72056 to 0.71364, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.6664 - accuracy: 0.7808 - val_loss: 0.7136 - val_accuracy: 0.7708\n",
      "Epoch 60/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6618 - accuracy: 0.7753\n",
      "Epoch 00060: val_loss did not improve from 0.71364\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6689 - accuracy: 0.7714 - val_loss: 0.7412 - val_accuracy: 0.7558\n",
      "Epoch 61/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6618 - accuracy: 0.7790\n",
      "Epoch 00061: val_loss did not improve from 0.71364\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6563 - accuracy: 0.7816 - val_loss: 0.7451 - val_accuracy: 0.7619\n",
      "Epoch 62/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6714 - accuracy: 0.7721\n",
      "Epoch 00062: val_loss did not improve from 0.71364\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6672 - accuracy: 0.7743 - val_loss: 0.7156 - val_accuracy: 0.7701\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6392 - accuracy: 0.7889\n",
      "Epoch 00063: val_loss improved from 0.71364 to 0.69141, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.6406 - accuracy: 0.7895 - val_loss: 0.6914 - val_accuracy: 0.7729\n",
      "Epoch 64/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6346 - accuracy: 0.7913\n",
      "Epoch 00064: val_loss improved from 0.69141 to 0.68562, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6305 - accuracy: 0.7927 - val_loss: 0.6856 - val_accuracy: 0.7817\n",
      "Epoch 65/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6281 - accuracy: 0.7902\n",
      "Epoch 00065: val_loss did not improve from 0.68562\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6253 - accuracy: 0.7907 - val_loss: 0.7042 - val_accuracy: 0.7715\n",
      "Epoch 66/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6274 - accuracy: 0.7881\n",
      "Epoch 00066: val_loss did not improve from 0.68562\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6314 - accuracy: 0.7869 - val_loss: 0.7107 - val_accuracy: 0.7694\n",
      "Epoch 67/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6099 - accuracy: 0.8006\n",
      "Epoch 00067: val_loss did not improve from 0.68562\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6123 - accuracy: 0.8002 - val_loss: 0.6929 - val_accuracy: 0.7763\n",
      "Epoch 68/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6095 - accuracy: 0.7954\n",
      "Epoch 00068: val_loss improved from 0.68562 to 0.68189, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.6042 - accuracy: 0.7975 - val_loss: 0.6819 - val_accuracy: 0.7865\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5918 - accuracy: 0.8019\n",
      "Epoch 00069: val_loss improved from 0.68189 to 0.67116, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5946 - accuracy: 0.8011 - val_loss: 0.6712 - val_accuracy: 0.7844\n",
      "Epoch 70/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5973 - accuracy: 0.8073\n",
      "Epoch 00070: val_loss improved from 0.67116 to 0.67065, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.6044 - accuracy: 0.8040 - val_loss: 0.6707 - val_accuracy: 0.7831\n",
      "Epoch 71/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5971 - accuracy: 0.8004\n",
      "Epoch 00071: val_loss did not improve from 0.67065\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5986 - accuracy: 0.8002 - val_loss: 0.6714 - val_accuracy: 0.7851\n",
      "Epoch 72/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5951 - accuracy: 0.8039\n",
      "Epoch 00072: val_loss did not improve from 0.67065\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5993 - accuracy: 0.8021 - val_loss: 0.6826 - val_accuracy: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5708 - accuracy: 0.8151\n",
      "Epoch 00073: val_loss improved from 0.67065 to 0.63547, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5779 - accuracy: 0.8127 - val_loss: 0.6355 - val_accuracy: 0.7899\n",
      "Epoch 74/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5779 - accuracy: 0.8071\n",
      "Epoch 00074: val_loss did not improve from 0.63547\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5825 - accuracy: 0.8052 - val_loss: 0.6538 - val_accuracy: 0.7858\n",
      "Epoch 75/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5714 - accuracy: 0.8093\n",
      "Epoch 00075: val_loss did not improve from 0.63547\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5680 - accuracy: 0.8110 - val_loss: 0.6534 - val_accuracy: 0.7865\n",
      "Epoch 76/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5661 - accuracy: 0.8132\n",
      "Epoch 00076: val_loss improved from 0.63547 to 0.62454, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5662 - accuracy: 0.8142 - val_loss: 0.6245 - val_accuracy: 0.8008\n",
      "Epoch 77/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5615 - accuracy: 0.8183\n",
      "Epoch 00077: val_loss did not improve from 0.62454\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5650 - accuracy: 0.8168 - val_loss: 0.6472 - val_accuracy: 0.7851\n",
      "Epoch 78/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5686 - accuracy: 0.8136\n",
      "Epoch 00078: val_loss did not improve from 0.62454\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5668 - accuracy: 0.8135 - val_loss: 0.6291 - val_accuracy: 0.7940\n",
      "Epoch 79/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5648 - accuracy: 0.8106\n",
      "Epoch 00079: val_loss improved from 0.62454 to 0.61272, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5607 - accuracy: 0.8139 - val_loss: 0.6127 - val_accuracy: 0.7920\n",
      "Epoch 80/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5446 - accuracy: 0.8251\n",
      "Epoch 00080: val_loss did not improve from 0.61272\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5437 - accuracy: 0.8253 - val_loss: 0.6185 - val_accuracy: 0.7865\n",
      "Epoch 81/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5429 - accuracy: 0.8188\n",
      "Epoch 00081: val_loss did not improve from 0.61272\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5442 - accuracy: 0.8188 - val_loss: 0.6409 - val_accuracy: 0.7872\n",
      "Epoch 82/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5455 - accuracy: 0.8155\n",
      "Epoch 00082: val_loss did not improve from 0.61272\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5467 - accuracy: 0.8159 - val_loss: 0.6311 - val_accuracy: 0.7885\n",
      "Epoch 83/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5445 - accuracy: 0.8216\n",
      "Epoch 00083: val_loss did not improve from 0.61272\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5436 - accuracy: 0.8219 - val_loss: 0.6398 - val_accuracy: 0.7851\n",
      "Epoch 84/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5459 - accuracy: 0.8179\n",
      "Epoch 00084: val_loss did not improve from 0.61272\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5447 - accuracy: 0.8193 - val_loss: 0.6161 - val_accuracy: 0.8063\n",
      "Epoch 85/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5283 - accuracy: 0.8246\n",
      "Epoch 00085: val_loss improved from 0.61272 to 0.60277, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5353 - accuracy: 0.8220 - val_loss: 0.6028 - val_accuracy: 0.8035\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5357 - accuracy: 0.8207\n",
      "Epoch 00086: val_loss did not improve from 0.60277\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5327 - accuracy: 0.8229 - val_loss: 0.6141 - val_accuracy: 0.7926\n",
      "Epoch 87/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5322 - accuracy: 0.8216\n",
      "Epoch 00087: val_loss did not improve from 0.60277\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5324 - accuracy: 0.8215 - val_loss: 0.6048 - val_accuracy: 0.8042\n",
      "Epoch 88/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5281 - accuracy: 0.8190\n",
      "Epoch 00088: val_loss improved from 0.60277 to 0.60231, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5188 - accuracy: 0.8236 - val_loss: 0.6023 - val_accuracy: 0.7981\n",
      "Epoch 89/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5116 - accuracy: 0.8294\n",
      "Epoch 00089: val_loss did not improve from 0.60231\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5164 - accuracy: 0.8277 - val_loss: 0.6157 - val_accuracy: 0.7981\n",
      "Epoch 90/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5099 - accuracy: 0.8263\n",
      "Epoch 00090: val_loss did not improve from 0.60231\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5129 - accuracy: 0.8244 - val_loss: 0.6167 - val_accuracy: 0.7954\n",
      "Epoch 91/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5127 - accuracy: 0.8281\n",
      "Epoch 00091: val_loss improved from 0.60231 to 0.58275, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5173 - accuracy: 0.8280 - val_loss: 0.5828 - val_accuracy: 0.8042\n",
      "Epoch 92/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5116 - accuracy: 0.8335\n",
      "Epoch 00092: val_loss improved from 0.58275 to 0.57390, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5087 - accuracy: 0.8338 - val_loss: 0.5739 - val_accuracy: 0.8138\n",
      "Epoch 93/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5032 - accuracy: 0.8317\n",
      "Epoch 00093: val_loss did not improve from 0.57390\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5107 - accuracy: 0.8299 - val_loss: 0.5812 - val_accuracy: 0.8015\n",
      "Epoch 94/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5256 - accuracy: 0.8172\n",
      "Epoch 00094: val_loss did not improve from 0.57390\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5220 - accuracy: 0.8188 - val_loss: 0.5884 - val_accuracy: 0.8008\n",
      "Epoch 95/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5149 - accuracy: 0.8233\n",
      "Epoch 00095: val_loss improved from 0.57390 to 0.56264, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5118 - accuracy: 0.8246 - val_loss: 0.5626 - val_accuracy: 0.8131\n",
      "Epoch 96/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4920 - accuracy: 0.8374\n",
      "Epoch 00096: val_loss did not improve from 0.56264\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4908 - accuracy: 0.8381 - val_loss: 0.5655 - val_accuracy: 0.8097\n",
      "Epoch 97/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4945 - accuracy: 0.8296\n",
      "Epoch 00097: val_loss did not improve from 0.56264\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4983 - accuracy: 0.8270 - val_loss: 0.5676 - val_accuracy: 0.8049\n",
      "Epoch 98/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4982 - accuracy: 0.8335\n",
      "Epoch 00098: val_loss did not improve from 0.56264\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4996 - accuracy: 0.8338 - val_loss: 0.5813 - val_accuracy: 0.8042\n",
      "Epoch 99/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5016 - accuracy: 0.8296\n",
      "Epoch 00099: val_loss improved from 0.56264 to 0.55284, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4979 - accuracy: 0.8318 - val_loss: 0.5528 - val_accuracy: 0.8117\n",
      "Epoch 100/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4829 - accuracy: 0.8413\n",
      "Epoch 00100: val_loss did not improve from 0.55284\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4877 - accuracy: 0.8405 - val_loss: 0.5731 - val_accuracy: 0.8056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4797 - accuracy: 0.8400\n",
      "Epoch 00101: val_loss improved from 0.55284 to 0.54917, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4747 - accuracy: 0.8417 - val_loss: 0.5492 - val_accuracy: 0.8124\n",
      "Epoch 102/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4836 - accuracy: 0.8382\n",
      "Epoch 00102: val_loss did not improve from 0.54917\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4858 - accuracy: 0.8371 - val_loss: 0.5671 - val_accuracy: 0.8117\n",
      "Epoch 103/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4772 - accuracy: 0.8395\n",
      "Epoch 00103: val_loss improved from 0.54917 to 0.54302, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4798 - accuracy: 0.8400 - val_loss: 0.5430 - val_accuracy: 0.8124\n",
      "Epoch 104/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4830 - accuracy: 0.8375\n",
      "Epoch 00104: val_loss did not improve from 0.54302\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4838 - accuracy: 0.8376 - val_loss: 0.5750 - val_accuracy: 0.8063\n",
      "Epoch 105/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4679 - accuracy: 0.8421\n",
      "Epoch 00105: val_loss did not improve from 0.54302\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4740 - accuracy: 0.8403 - val_loss: 0.5518 - val_accuracy: 0.8111\n",
      "Epoch 106/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4557 - accuracy: 0.8514\n",
      "Epoch 00106: val_loss improved from 0.54302 to 0.53484, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4656 - accuracy: 0.8466 - val_loss: 0.5348 - val_accuracy: 0.8124\n",
      "Epoch 107/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4610 - accuracy: 0.8514\n",
      "Epoch 00107: val_loss did not improve from 0.53484\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4583 - accuracy: 0.8529 - val_loss: 0.5924 - val_accuracy: 0.8097\n",
      "Epoch 108/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4661 - accuracy: 0.8393\n",
      "Epoch 00108: val_loss did not improve from 0.53484\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4618 - accuracy: 0.8413 - val_loss: 0.5367 - val_accuracy: 0.8192\n",
      "Epoch 109/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4594 - accuracy: 0.8486\n",
      "Epoch 00109: val_loss did not improve from 0.53484\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4573 - accuracy: 0.8493 - val_loss: 0.5625 - val_accuracy: 0.8076\n",
      "Epoch 110/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4591 - accuracy: 0.8458\n",
      "Epoch 00110: val_loss did not improve from 0.53484\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4566 - accuracy: 0.8476 - val_loss: 0.5467 - val_accuracy: 0.8158\n",
      "Epoch 111/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4468 - accuracy: 0.8490\n",
      "Epoch 00111: val_loss did not improve from 0.53484\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4529 - accuracy: 0.8487 - val_loss: 0.5396 - val_accuracy: 0.8104\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4526 - accuracy: 0.8495\n",
      "Epoch 00112: val_loss improved from 0.53484 to 0.53434, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4543 - accuracy: 0.8492 - val_loss: 0.5343 - val_accuracy: 0.8199\n",
      "Epoch 113/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4409 - accuracy: 0.8484\n",
      "Epoch 00113: val_loss did not improve from 0.53434\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4414 - accuracy: 0.8490 - val_loss: 0.5436 - val_accuracy: 0.8158\n",
      "Epoch 114/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4425 - accuracy: 0.8514\n",
      "Epoch 00114: val_loss improved from 0.53434 to 0.52119, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4503 - accuracy: 0.8495 - val_loss: 0.5212 - val_accuracy: 0.8233\n",
      "Epoch 115/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4496 - accuracy: 0.8449\n",
      "Epoch 00115: val_loss did not improve from 0.52119\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4425 - accuracy: 0.8480 - val_loss: 0.5592 - val_accuracy: 0.8151\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4508 - accuracy: 0.8471\n",
      "Epoch 00116: val_loss improved from 0.52119 to 0.51409, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4467 - accuracy: 0.8493 - val_loss: 0.5141 - val_accuracy: 0.8226\n",
      "Epoch 117/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4267 - accuracy: 0.8590\n",
      "Epoch 00117: val_loss did not improve from 0.51409\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4345 - accuracy: 0.8560 - val_loss: 0.5233 - val_accuracy: 0.8233\n",
      "Epoch 118/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4461 - accuracy: 0.8454\n",
      "Epoch 00118: val_loss did not improve from 0.51409\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4459 - accuracy: 0.8464 - val_loss: 0.5238 - val_accuracy: 0.8274\n",
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4525 - accuracy: 0.8436\n",
      "Epoch 00119: val_loss improved from 0.51409 to 0.50798, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4535 - accuracy: 0.8427 - val_loss: 0.5080 - val_accuracy: 0.8288\n",
      "Epoch 120/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4254 - accuracy: 0.8583\n",
      "Epoch 00120: val_loss did not improve from 0.50798\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4208 - accuracy: 0.8591 - val_loss: 0.5185 - val_accuracy: 0.8363\n",
      "Epoch 121/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4103 - accuracy: 0.8620\n",
      "Epoch 00121: val_loss improved from 0.50798 to 0.50682, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4180 - accuracy: 0.8596 - val_loss: 0.5068 - val_accuracy: 0.8267\n",
      "Epoch 122/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4228 - accuracy: 0.8594\n",
      "Epoch 00122: val_loss did not improve from 0.50682\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4278 - accuracy: 0.8567 - val_loss: 0.5121 - val_accuracy: 0.8302\n",
      "Epoch 123/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4313 - accuracy: 0.8575\n",
      "Epoch 00123: val_loss did not improve from 0.50682\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4334 - accuracy: 0.8567 - val_loss: 0.5105 - val_accuracy: 0.8261\n",
      "Epoch 124/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4175 - accuracy: 0.8586\n",
      "Epoch 00124: val_loss improved from 0.50682 to 0.50033, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4153 - accuracy: 0.8592 - val_loss: 0.5003 - val_accuracy: 0.8281\n",
      "Epoch 125/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4170 - accuracy: 0.8604\n",
      "Epoch 00125: val_loss did not improve from 0.50033\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4147 - accuracy: 0.8603 - val_loss: 0.5048 - val_accuracy: 0.8336\n",
      "Epoch 126/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4170 - accuracy: 0.8540\n",
      "Epoch 00126: val_loss improved from 0.50033 to 0.48992, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4165 - accuracy: 0.8539 - val_loss: 0.4899 - val_accuracy: 0.8308\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4092 - accuracy: 0.8596\n",
      "Epoch 00127: val_loss did not improve from 0.48992\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4130 - accuracy: 0.8586 - val_loss: 0.5290 - val_accuracy: 0.8186\n",
      "Epoch 128/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4160 - accuracy: 0.8618\n",
      "Epoch 00128: val_loss improved from 0.48992 to 0.48882, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4149 - accuracy: 0.8616 - val_loss: 0.4888 - val_accuracy: 0.8336\n",
      "Epoch 129/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4230 - accuracy: 0.8504\n",
      "Epoch 00129: val_loss did not improve from 0.48882\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4242 - accuracy: 0.8505 - val_loss: 0.5220 - val_accuracy: 0.8247\n",
      "Epoch 130/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4143 - accuracy: 0.8605\n",
      "Epoch 00130: val_loss did not improve from 0.48882\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4156 - accuracy: 0.8613 - val_loss: 0.5096 - val_accuracy: 0.8267\n",
      "Epoch 131/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4150 - accuracy: 0.8622\n",
      "Epoch 00131: val_loss did not improve from 0.48882\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4148 - accuracy: 0.8618 - val_loss: 0.4896 - val_accuracy: 0.8390\n",
      "Epoch 132/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4135 - accuracy: 0.8605\n",
      "Epoch 00132: val_loss did not improve from 0.48882\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4111 - accuracy: 0.8618 - val_loss: 0.4930 - val_accuracy: 0.8383\n",
      "Epoch 133/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4114 - accuracy: 0.8622\n",
      "Epoch 00133: val_loss did not improve from 0.48882\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.4087 - accuracy: 0.8642 - val_loss: 0.5151 - val_accuracy: 0.8124\n",
      "Epoch 134/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4087 - accuracy: 0.8607\n",
      "Epoch 00134: val_loss improved from 0.48882 to 0.46814, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4106 - accuracy: 0.8604 - val_loss: 0.4681 - val_accuracy: 0.8411\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3941 - accuracy: 0.8664\n",
      "Epoch 00135: val_loss did not improve from 0.46814\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3965 - accuracy: 0.8642 - val_loss: 0.4774 - val_accuracy: 0.8349\n",
      "Epoch 136/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3902 - accuracy: 0.8703\n",
      "Epoch 00136: val_loss did not improve from 0.46814\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3860 - accuracy: 0.8724 - val_loss: 0.4881 - val_accuracy: 0.8342\n",
      "Epoch 137/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4077 - accuracy: 0.8670\n",
      "Epoch 00137: val_loss did not improve from 0.46814\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4023 - accuracy: 0.8679 - val_loss: 0.5048 - val_accuracy: 0.8315\n",
      "Epoch 138/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3893 - accuracy: 0.8702\n",
      "Epoch 00138: val_loss improved from 0.46814 to 0.45455, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3922 - accuracy: 0.8674 - val_loss: 0.4546 - val_accuracy: 0.8445\n",
      "Epoch 139/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3944 - accuracy: 0.8631\n",
      "Epoch 00139: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3916 - accuracy: 0.8652 - val_loss: 0.4957 - val_accuracy: 0.8288\n",
      "Epoch 140/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3884 - accuracy: 0.8692\n",
      "Epoch 00140: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3950 - accuracy: 0.8673 - val_loss: 0.4634 - val_accuracy: 0.8370\n",
      "Epoch 141/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3983 - accuracy: 0.8635\n",
      "Epoch 00141: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3993 - accuracy: 0.8632 - val_loss: 0.4914 - val_accuracy: 0.8342\n",
      "Epoch 142/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3875 - accuracy: 0.8664\n",
      "Epoch 00142: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3871 - accuracy: 0.8686 - val_loss: 0.4769 - val_accuracy: 0.8342\n",
      "Epoch 143/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3748 - accuracy: 0.8726\n",
      "Epoch 00143: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3811 - accuracy: 0.8703 - val_loss: 0.4654 - val_accuracy: 0.8397\n",
      "Epoch 144/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3897 - accuracy: 0.8644\n",
      "Epoch 00144: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3896 - accuracy: 0.8652 - val_loss: 0.4950 - val_accuracy: 0.8336\n",
      "Epoch 145/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3799 - accuracy: 0.8765\n",
      "Epoch 00145: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3792 - accuracy: 0.8760 - val_loss: 0.4656 - val_accuracy: 0.8417\n",
      "Epoch 146/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3774 - accuracy: 0.8724\n",
      "Epoch 00146: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3780 - accuracy: 0.8715 - val_loss: 0.4675 - val_accuracy: 0.8417\n",
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3636 - accuracy: 0.8810\n",
      "Epoch 00147: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3620 - accuracy: 0.8807 - val_loss: 0.4576 - val_accuracy: 0.8458\n",
      "Epoch 148/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3697 - accuracy: 0.8739\n",
      "Epoch 00148: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3704 - accuracy: 0.8749 - val_loss: 0.4690 - val_accuracy: 0.8445\n",
      "Epoch 149/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3717 - accuracy: 0.8767\n",
      "Epoch 00149: val_loss did not improve from 0.45455\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3679 - accuracy: 0.8780 - val_loss: 0.4678 - val_accuracy: 0.8431\n",
      "Epoch 150/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3616 - accuracy: 0.8815\n",
      "Epoch 00150: val_loss improved from 0.45455 to 0.45215, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3614 - accuracy: 0.8824 - val_loss: 0.4521 - val_accuracy: 0.8458\n",
      "Epoch 151/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3637 - accuracy: 0.8803\n",
      "Epoch 00151: val_loss did not improve from 0.45215\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3597 - accuracy: 0.8816 - val_loss: 0.4634 - val_accuracy: 0.8472\n",
      "Epoch 152/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3607 - accuracy: 0.8782\n",
      "Epoch 00152: val_loss did not improve from 0.45215\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3648 - accuracy: 0.8780 - val_loss: 0.4710 - val_accuracy: 0.8390\n",
      "Epoch 153/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3760 - accuracy: 0.8711\n",
      "Epoch 00153: val_loss improved from 0.45215 to 0.44856, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3787 - accuracy: 0.8702 - val_loss: 0.4486 - val_accuracy: 0.8554\n",
      "Epoch 154/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3657 - accuracy: 0.8759\n",
      "Epoch 00154: val_loss did not improve from 0.44856\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3666 - accuracy: 0.8765 - val_loss: 0.4622 - val_accuracy: 0.8390\n",
      "Epoch 155/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3564 - accuracy: 0.8767\n",
      "Epoch 00155: val_loss did not improve from 0.44856\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3615 - accuracy: 0.8748 - val_loss: 0.4681 - val_accuracy: 0.8438\n",
      "Epoch 156/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3481 - accuracy: 0.8876\n",
      "Epoch 00156: val_loss did not improve from 0.44856\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3507 - accuracy: 0.8855 - val_loss: 0.4814 - val_accuracy: 0.8397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3687 - accuracy: 0.8776\n",
      "Epoch 00157: val_loss did not improve from 0.44856\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3701 - accuracy: 0.8761 - val_loss: 0.4743 - val_accuracy: 0.8349\n",
      "Epoch 158/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3609 - accuracy: 0.8776\n",
      "Epoch 00158: val_loss did not improve from 0.44856\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3590 - accuracy: 0.8782 - val_loss: 0.4669 - val_accuracy: 0.8445\n",
      "Epoch 159/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3535 - accuracy: 0.8852\n",
      "Epoch 00159: val_loss improved from 0.44856 to 0.44745, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3544 - accuracy: 0.8852 - val_loss: 0.4475 - val_accuracy: 0.8554\n",
      "Epoch 160/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3449 - accuracy: 0.8837\n",
      "Epoch 00160: val_loss did not improve from 0.44745\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3460 - accuracy: 0.8830 - val_loss: 0.4581 - val_accuracy: 0.8445\n",
      "Epoch 161/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3539 - accuracy: 0.8804\n",
      "Epoch 00161: val_loss did not improve from 0.44745\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3530 - accuracy: 0.8806 - val_loss: 0.4618 - val_accuracy: 0.8458\n",
      "Epoch 162/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3554 - accuracy: 0.8798\n",
      "Epoch 00162: val_loss did not improve from 0.44745\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3534 - accuracy: 0.8812 - val_loss: 0.4644 - val_accuracy: 0.8499\n",
      "Epoch 163/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3702 - accuracy: 0.8770\n",
      "Epoch 00163: val_loss did not improve from 0.44745\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3663 - accuracy: 0.8789 - val_loss: 0.4690 - val_accuracy: 0.8424\n",
      "Epoch 164/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3494 - accuracy: 0.8847\n",
      "Epoch 00164: val_loss improved from 0.44745 to 0.44187, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3520 - accuracy: 0.8824 - val_loss: 0.4419 - val_accuracy: 0.8472\n",
      "Epoch 165/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3419 - accuracy: 0.8826\n",
      "Epoch 00165: val_loss did not improve from 0.44187\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3414 - accuracy: 0.8814 - val_loss: 0.4461 - val_accuracy: 0.8520\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3544 - accuracy: 0.8811\n",
      "Epoch 00166: val_loss did not improve from 0.44187\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3554 - accuracy: 0.8801 - val_loss: 0.4448 - val_accuracy: 0.8492\n",
      "Epoch 167/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3442 - accuracy: 0.8841\n",
      "Epoch 00167: val_loss did not improve from 0.44187\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3434 - accuracy: 0.8840 - val_loss: 0.4547 - val_accuracy: 0.8438\n",
      "Epoch 168/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3309 - accuracy: 0.8878\n",
      "Epoch 00168: val_loss did not improve from 0.44187\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3360 - accuracy: 0.8876 - val_loss: 0.4710 - val_accuracy: 0.8397\n",
      "Epoch 169/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3334 - accuracy: 0.8901\n",
      "Epoch 00169: val_loss improved from 0.44187 to 0.42645, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3323 - accuracy: 0.8900 - val_loss: 0.4265 - val_accuracy: 0.8568\n",
      "Epoch 170/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3397 - accuracy: 0.8852\n",
      "Epoch 00170: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3376 - accuracy: 0.8853 - val_loss: 0.4483 - val_accuracy: 0.8506\n",
      "Epoch 171/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3496 - accuracy: 0.8811\n",
      "Epoch 00171: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3479 - accuracy: 0.8814 - val_loss: 0.4490 - val_accuracy: 0.8465\n",
      "Epoch 172/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3513 - accuracy: 0.8824\n",
      "Epoch 00172: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3484 - accuracy: 0.8821 - val_loss: 0.4299 - val_accuracy: 0.8574\n",
      "Epoch 173/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3347 - accuracy: 0.8871\n",
      "Epoch 00173: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3360 - accuracy: 0.8855 - val_loss: 0.4345 - val_accuracy: 0.8533\n",
      "Epoch 174/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3362 - accuracy: 0.8893\n",
      "Epoch 00174: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3338 - accuracy: 0.8898 - val_loss: 0.4657 - val_accuracy: 0.8431\n",
      "Epoch 175/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3255 - accuracy: 0.8901\n",
      "Epoch 00175: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3288 - accuracy: 0.8901 - val_loss: 0.4268 - val_accuracy: 0.8561\n",
      "Epoch 176/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3280 - accuracy: 0.8893\n",
      "Epoch 00176: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3302 - accuracy: 0.8889 - val_loss: 0.4298 - val_accuracy: 0.8561\n",
      "Epoch 177/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3251 - accuracy: 0.8908\n",
      "Epoch 00177: val_loss did not improve from 0.42645\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3222 - accuracy: 0.8918 - val_loss: 0.4270 - val_accuracy: 0.8595\n",
      "Epoch 178/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3347 - accuracy: 0.8869\n",
      "Epoch 00178: val_loss improved from 0.42645 to 0.40787, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3359 - accuracy: 0.8867 - val_loss: 0.4079 - val_accuracy: 0.8574\n",
      "Epoch 179/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3164 - accuracy: 0.8947\n",
      "Epoch 00179: val_loss did not improve from 0.40787\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3170 - accuracy: 0.8930 - val_loss: 0.4186 - val_accuracy: 0.8574\n",
      "Epoch 180/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3169 - accuracy: 0.8929\n",
      "Epoch 00180: val_loss did not improve from 0.40787\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3214 - accuracy: 0.8925 - val_loss: 0.4297 - val_accuracy: 0.8479\n",
      "Epoch 181/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3116 - accuracy: 0.8956\n",
      "Epoch 00181: val_loss did not improve from 0.40787\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3140 - accuracy: 0.8954 - val_loss: 0.4443 - val_accuracy: 0.8506\n",
      "Epoch 182/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3230 - accuracy: 0.8865\n",
      "Epoch 00182: val_loss did not improve from 0.40787\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3266 - accuracy: 0.8862 - val_loss: 0.4262 - val_accuracy: 0.8663\n",
      "Epoch 183/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3228 - accuracy: 0.8903\n",
      "Epoch 00183: val_loss improved from 0.40787 to 0.39821, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3200 - accuracy: 0.8911 - val_loss: 0.3982 - val_accuracy: 0.8670\n",
      "Epoch 184/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3126 - accuracy: 0.8977\n",
      "Epoch 00184: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3141 - accuracy: 0.8975 - val_loss: 0.4147 - val_accuracy: 0.8554\n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3138 - accuracy: 0.8964\n",
      "Epoch 00185: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3155 - accuracy: 0.8958 - val_loss: 0.4523 - val_accuracy: 0.8499\n",
      "Epoch 186/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3137 - accuracy: 0.8904\n",
      "Epoch 00186: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3164 - accuracy: 0.8894 - val_loss: 0.4152 - val_accuracy: 0.8643\n",
      "Epoch 187/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3100 - accuracy: 0.8882\n",
      "Epoch 00187: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3066 - accuracy: 0.8913 - val_loss: 0.4193 - val_accuracy: 0.8629\n",
      "Epoch 188/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3140 - accuracy: 0.8973\n",
      "Epoch 00188: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3116 - accuracy: 0.8966 - val_loss: 0.4042 - val_accuracy: 0.8622\n",
      "Epoch 189/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8956\n",
      "Epoch 00189: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3122 - accuracy: 0.8937 - val_loss: 0.4127 - val_accuracy: 0.8615\n",
      "Epoch 190/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3085 - accuracy: 0.8930\n",
      "Epoch 00190: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3111 - accuracy: 0.8927 - val_loss: 0.4428 - val_accuracy: 0.8554\n",
      "Epoch 191/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3050 - accuracy: 0.8990\n",
      "Epoch 00191: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3064 - accuracy: 0.8983 - val_loss: 0.4295 - val_accuracy: 0.8602\n",
      "Epoch 192/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3084 - accuracy: 0.8917\n",
      "Epoch 00192: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3067 - accuracy: 0.8927 - val_loss: 0.4021 - val_accuracy: 0.8683\n",
      "Epoch 193/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2886 - accuracy: 0.9062\n",
      "Epoch 00193: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2920 - accuracy: 0.9045 - val_loss: 0.4095 - val_accuracy: 0.8622\n",
      "Epoch 194/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3115 - accuracy: 0.8921\n",
      "Epoch 00194: val_loss did not improve from 0.39821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3079 - accuracy: 0.8923 - val_loss: 0.4126 - val_accuracy: 0.8595\n",
      "Epoch 195/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3054 - accuracy: 0.8962\n",
      "Epoch 00195: val_loss improved from 0.39821 to 0.38869, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3036 - accuracy: 0.8966 - val_loss: 0.3887 - val_accuracy: 0.8765\n",
      "Epoch 196/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3060 - accuracy: 0.8955\n",
      "Epoch 00196: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3066 - accuracy: 0.8969 - val_loss: 0.4061 - val_accuracy: 0.8649\n",
      "Epoch 197/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3002 - accuracy: 0.8999\n",
      "Epoch 00197: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2964 - accuracy: 0.9005 - val_loss: 0.4041 - val_accuracy: 0.8643\n",
      "Epoch 198/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2998 - accuracy: 0.8979\n",
      "Epoch 00198: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2991 - accuracy: 0.8971 - val_loss: 0.4060 - val_accuracy: 0.8649\n",
      "Epoch 199/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2786 - accuracy: 0.9040\n",
      "Epoch 00199: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2795 - accuracy: 0.9048 - val_loss: 0.3950 - val_accuracy: 0.8704\n",
      "Epoch 200/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3054 - accuracy: 0.8997\n",
      "Epoch 00200: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3052 - accuracy: 0.9000 - val_loss: 0.4306 - val_accuracy: 0.8574\n",
      "Epoch 201/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2928 - accuracy: 0.9016\n",
      "Epoch 00201: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2924 - accuracy: 0.9014 - val_loss: 0.3980 - val_accuracy: 0.8649\n",
      "Epoch 202/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2882 - accuracy: 0.9016\n",
      "Epoch 00202: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2898 - accuracy: 0.9016 - val_loss: 0.4389 - val_accuracy: 0.8615\n",
      "Epoch 203/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2904 - accuracy: 0.9068\n",
      "Epoch 00203: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2891 - accuracy: 0.9063 - val_loss: 0.4214 - val_accuracy: 0.8622\n",
      "Epoch 204/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2731 - accuracy: 0.9079\n",
      "Epoch 00204: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2757 - accuracy: 0.9050 - val_loss: 0.4172 - val_accuracy: 0.8602\n",
      "Epoch 205/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2866 - accuracy: 0.9061\n",
      "Epoch 00205: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2917 - accuracy: 0.9038 - val_loss: 0.4053 - val_accuracy: 0.8772\n",
      "Epoch 206/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2825 - accuracy: 0.9036\n",
      "Epoch 00206: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2857 - accuracy: 0.9024 - val_loss: 0.4219 - val_accuracy: 0.8622\n",
      "Epoch 207/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2775 - accuracy: 0.9046\n",
      "Epoch 00207: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2771 - accuracy: 0.9055 - val_loss: 0.4189 - val_accuracy: 0.8636\n",
      "Epoch 208/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2753 - accuracy: 0.9055\n",
      "Epoch 00208: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2735 - accuracy: 0.9068 - val_loss: 0.4073 - val_accuracy: 0.8568\n",
      "Epoch 209/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2709 - accuracy: 0.9096\n",
      "Epoch 00209: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2716 - accuracy: 0.9097 - val_loss: 0.4245 - val_accuracy: 0.8595\n",
      "Epoch 210/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2767 - accuracy: 0.9074\n",
      "Epoch 00210: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2761 - accuracy: 0.9070 - val_loss: 0.4140 - val_accuracy: 0.8649\n",
      "Epoch 211/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2703 - accuracy: 0.9102\n",
      "Epoch 00211: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2710 - accuracy: 0.9089 - val_loss: 0.4378 - val_accuracy: 0.8588\n",
      "Epoch 212/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2727 - accuracy: 0.9092\n",
      "Epoch 00212: val_loss did not improve from 0.38869\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2728 - accuracy: 0.9082 - val_loss: 0.3956 - val_accuracy: 0.8759\n",
      "Epoch 213/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2644 - accuracy: 0.9100\n",
      "Epoch 00213: val_loss improved from 0.38869 to 0.38702, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2673 - accuracy: 0.9096 - val_loss: 0.3870 - val_accuracy: 0.8677\n",
      "Epoch 214/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2633 - accuracy: 0.9076\n",
      "Epoch 00214: val_loss did not improve from 0.38702\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2644 - accuracy: 0.9084 - val_loss: 0.3894 - val_accuracy: 0.8683\n",
      "Epoch 215/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2722 - accuracy: 0.9089\n",
      "Epoch 00215: val_loss did not improve from 0.38702\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2708 - accuracy: 0.9094 - val_loss: 0.3964 - val_accuracy: 0.8690\n",
      "Epoch 216/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2735 - accuracy: 0.9087\n",
      "Epoch 00216: val_loss did not improve from 0.38702\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2791 - accuracy: 0.9065 - val_loss: 0.3910 - val_accuracy: 0.8704\n",
      "Epoch 217/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2817 - accuracy: 0.9014\n",
      "Epoch 00217: val_loss improved from 0.38702 to 0.37509, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2819 - accuracy: 0.9014 - val_loss: 0.3751 - val_accuracy: 0.8813\n",
      "Epoch 218/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2676 - accuracy: 0.9103\n",
      "Epoch 00218: val_loss improved from 0.37509 to 0.36837, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2719 - accuracy: 0.9091 - val_loss: 0.3684 - val_accuracy: 0.8799\n",
      "Epoch 219/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2731 - accuracy: 0.9051\n",
      "Epoch 00219: val_loss did not improve from 0.36837\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2763 - accuracy: 0.9043 - val_loss: 0.4154 - val_accuracy: 0.8622\n",
      "Epoch 220/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2711 - accuracy: 0.9098\n",
      "Epoch 00220: val_loss did not improve from 0.36837\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2693 - accuracy: 0.9094 - val_loss: 0.4149 - val_accuracy: 0.8718\n",
      "Epoch 221/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2902 - accuracy: 0.8997\n",
      "Epoch 00221: val_loss did not improve from 0.36837\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2879 - accuracy: 0.9007 - val_loss: 0.3991 - val_accuracy: 0.8649\n",
      "Epoch 222/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2542 - accuracy: 0.9163\n",
      "Epoch 00222: val_loss did not improve from 0.36837\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2523 - accuracy: 0.9174 - val_loss: 0.3703 - val_accuracy: 0.8765\n",
      "Epoch 223/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2577 - accuracy: 0.9139\n",
      "Epoch 00223: val_loss improved from 0.36837 to 0.36721, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2606 - accuracy: 0.9128 - val_loss: 0.3672 - val_accuracy: 0.8847\n",
      "Epoch 224/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2552 - accuracy: 0.9124\n",
      "Epoch 00224: val_loss did not improve from 0.36721\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2567 - accuracy: 0.9114 - val_loss: 0.3764 - val_accuracy: 0.8793\n",
      "Epoch 225/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2642 - accuracy: 0.9113\n",
      "Epoch 00225: val_loss did not improve from 0.36721\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2614 - accuracy: 0.9121 - val_loss: 0.4496 - val_accuracy: 0.8636\n",
      "Epoch 226/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2745 - accuracy: 0.9061\n",
      "Epoch 00226: val_loss did not improve from 0.36721\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2729 - accuracy: 0.9072 - val_loss: 0.3873 - val_accuracy: 0.8731\n",
      "Epoch 227/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2673 - accuracy: 0.9111\n",
      "Epoch 00227: val_loss improved from 0.36721 to 0.36445, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2660 - accuracy: 0.9113 - val_loss: 0.3645 - val_accuracy: 0.8840\n",
      "Epoch 228/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2501 - accuracy: 0.9144\n",
      "Epoch 00228: val_loss did not improve from 0.36445\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2496 - accuracy: 0.9149 - val_loss: 0.3694 - val_accuracy: 0.8772\n",
      "Epoch 229/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2418 - accuracy: 0.9189\n",
      "Epoch 00229: val_loss did not improve from 0.36445\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2433 - accuracy: 0.9181 - val_loss: 0.4164 - val_accuracy: 0.8636\n",
      "Epoch 230/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2678 - accuracy: 0.9070\n",
      "Epoch 00230: val_loss did not improve from 0.36445\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2716 - accuracy: 0.9067 - val_loss: 0.3872 - val_accuracy: 0.8738\n",
      "Epoch 231/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2592 - accuracy: 0.9128\n",
      "Epoch 00231: val_loss improved from 0.36445 to 0.35890, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2577 - accuracy: 0.9126 - val_loss: 0.3589 - val_accuracy: 0.8868\n",
      "Epoch 232/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2469 - accuracy: 0.9150\n",
      "Epoch 00232: val_loss did not improve from 0.35890\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2510 - accuracy: 0.9149 - val_loss: 0.3907 - val_accuracy: 0.8786\n",
      "Epoch 233/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2468 - accuracy: 0.9154\n",
      "Epoch 00233: val_loss improved from 0.35890 to 0.35720, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2443 - accuracy: 0.9166 - val_loss: 0.3572 - val_accuracy: 0.8874\n",
      "Epoch 234/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2535 - accuracy: 0.9102\n",
      "Epoch 00234: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2515 - accuracy: 0.9113 - val_loss: 0.3759 - val_accuracy: 0.8813\n",
      "Epoch 235/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2425 - accuracy: 0.9189\n",
      "Epoch 00235: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2427 - accuracy: 0.9191 - val_loss: 0.3740 - val_accuracy: 0.8827\n",
      "Epoch 236/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2511 - accuracy: 0.9129\n",
      "Epoch 00236: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2495 - accuracy: 0.9128 - val_loss: 0.3738 - val_accuracy: 0.8847\n",
      "Epoch 237/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2417 - accuracy: 0.9183\n",
      "Epoch 00237: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2384 - accuracy: 0.9191 - val_loss: 0.3670 - val_accuracy: 0.8813\n",
      "Epoch 238/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2343 - accuracy: 0.9219\n",
      "Epoch 00238: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2388 - accuracy: 0.9203 - val_loss: 0.3844 - val_accuracy: 0.8765\n",
      "Epoch 239/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2379 - accuracy: 0.9180\n",
      "Epoch 00239: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2355 - accuracy: 0.9191 - val_loss: 0.3720 - val_accuracy: 0.8820\n",
      "Epoch 240/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2433 - accuracy: 0.9137\n",
      "Epoch 00240: val_loss did not improve from 0.35720\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2427 - accuracy: 0.9138 - val_loss: 0.3794 - val_accuracy: 0.8813\n",
      "Epoch 241/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2439 - accuracy: 0.9170\n",
      "Epoch 00241: val_loss improved from 0.35720 to 0.33755, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2454 - accuracy: 0.9162 - val_loss: 0.3375 - val_accuracy: 0.8915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2391 - accuracy: 0.9163\n",
      "Epoch 00242: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2408 - accuracy: 0.9152 - val_loss: 0.3579 - val_accuracy: 0.8868\n",
      "Epoch 243/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2503 - accuracy: 0.9154\n",
      "Epoch 00243: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2489 - accuracy: 0.9159 - val_loss: 0.3729 - val_accuracy: 0.8847\n",
      "Epoch 244/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2472 - accuracy: 0.9148\n",
      "Epoch 00244: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2462 - accuracy: 0.9155 - val_loss: 0.3909 - val_accuracy: 0.8806\n",
      "Epoch 245/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2413 - accuracy: 0.9178\n",
      "Epoch 00245: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2417 - accuracy: 0.9190 - val_loss: 0.3740 - val_accuracy: 0.8868\n",
      "Epoch 246/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2445 - accuracy: 0.9189\n",
      "Epoch 00246: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2446 - accuracy: 0.9195 - val_loss: 0.3928 - val_accuracy: 0.8724\n",
      "Epoch 247/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2378 - accuracy: 0.9129\n",
      "Epoch 00247: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2382 - accuracy: 0.9132 - val_loss: 0.3703 - val_accuracy: 0.8827\n",
      "Epoch 248/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2211 - accuracy: 0.9213\n",
      "Epoch 00248: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2222 - accuracy: 0.9215 - val_loss: 0.3401 - val_accuracy: 0.8881\n",
      "Epoch 249/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2353 - accuracy: 0.9183\n",
      "Epoch 00249: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2398 - accuracy: 0.9179 - val_loss: 0.3799 - val_accuracy: 0.8772\n",
      "Epoch 250/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2398 - accuracy: 0.9204\n",
      "Epoch 00250: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2396 - accuracy: 0.9205 - val_loss: 0.3526 - val_accuracy: 0.8909\n",
      "Epoch 251/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2305 - accuracy: 0.9213\n",
      "Epoch 00251: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2304 - accuracy: 0.9220 - val_loss: 0.3472 - val_accuracy: 0.8909\n",
      "Epoch 252/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2231 - accuracy: 0.9241\n",
      "Epoch 00252: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2232 - accuracy: 0.9241 - val_loss: 0.3571 - val_accuracy: 0.8909\n",
      "Epoch 253/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2220 - accuracy: 0.9221\n",
      "Epoch 00253: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2244 - accuracy: 0.9217 - val_loss: 0.3544 - val_accuracy: 0.8895\n",
      "Epoch 254/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2288 - accuracy: 0.9215\n",
      "Epoch 00254: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2303 - accuracy: 0.9203 - val_loss: 0.3388 - val_accuracy: 0.8888\n",
      "Epoch 255/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2359 - accuracy: 0.9196\n",
      "Epoch 00255: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2343 - accuracy: 0.9196 - val_loss: 0.3482 - val_accuracy: 0.8909\n",
      "Epoch 256/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2240 - accuracy: 0.9170\n",
      "Epoch 00256: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2309 - accuracy: 0.9172 - val_loss: 0.3534 - val_accuracy: 0.8895\n",
      "Epoch 257/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2198 - accuracy: 0.9258\n",
      "Epoch 00257: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2186 - accuracy: 0.9273 - val_loss: 0.3394 - val_accuracy: 0.8997\n",
      "Epoch 258/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2238 - accuracy: 0.9228\n",
      "Epoch 00258: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2264 - accuracy: 0.9229 - val_loss: 0.3426 - val_accuracy: 0.8943\n",
      "Epoch 259/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2317 - accuracy: 0.9235\n",
      "Epoch 00259: val_loss did not improve from 0.33755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2287 - accuracy: 0.9239 - val_loss: 0.3553 - val_accuracy: 0.8847\n",
      "Epoch 260/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2284 - accuracy: 0.9230\n",
      "Epoch 00260: val_loss improved from 0.33755 to 0.33032, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2271 - accuracy: 0.9227 - val_loss: 0.3303 - val_accuracy: 0.8984\n",
      "Epoch 261/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2178 - accuracy: 0.9234\n",
      "Epoch 00261: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2178 - accuracy: 0.9229 - val_loss: 0.3823 - val_accuracy: 0.8806\n",
      "Epoch 262/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2090 - accuracy: 0.9284\n",
      "Epoch 00262: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2113 - accuracy: 0.9271 - val_loss: 0.3494 - val_accuracy: 0.8922\n",
      "Epoch 263/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2227 - accuracy: 0.9289\n",
      "Epoch 00263: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2225 - accuracy: 0.9278 - val_loss: 0.3353 - val_accuracy: 0.8881\n",
      "Epoch 264/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2119 - accuracy: 0.9345\n",
      "Epoch 00264: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2119 - accuracy: 0.9340 - val_loss: 0.3322 - val_accuracy: 0.8936\n",
      "Epoch 265/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2109 - accuracy: 0.9278\n",
      "Epoch 00265: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2122 - accuracy: 0.9275 - val_loss: 0.3399 - val_accuracy: 0.8929\n",
      "Epoch 266/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2183 - accuracy: 0.9230\n",
      "Epoch 00266: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2175 - accuracy: 0.9237 - val_loss: 0.3698 - val_accuracy: 0.8806\n",
      "Epoch 267/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2159 - accuracy: 0.9247\n",
      "Epoch 00267: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2147 - accuracy: 0.9249 - val_loss: 0.3672 - val_accuracy: 0.8847\n",
      "Epoch 268/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2139 - accuracy: 0.9245\n",
      "Epoch 00268: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2125 - accuracy: 0.9244 - val_loss: 0.3684 - val_accuracy: 0.8915\n",
      "Epoch 269/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2360 - accuracy: 0.9156\n",
      "Epoch 00269: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2371 - accuracy: 0.9155 - val_loss: 0.3846 - val_accuracy: 0.8827\n",
      "Epoch 270/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2386 - accuracy: 0.9148\n",
      "Epoch 00270: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2357 - accuracy: 0.9159 - val_loss: 0.3370 - val_accuracy: 0.8929\n",
      "Epoch 271/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2105 - accuracy: 0.9276\n",
      "Epoch 00271: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2131 - accuracy: 0.9273 - val_loss: 0.3628 - val_accuracy: 0.8854\n",
      "Epoch 272/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2079 - accuracy: 0.9295\n",
      "Epoch 00272: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2105 - accuracy: 0.9278 - val_loss: 0.3556 - val_accuracy: 0.8861\n",
      "Epoch 273/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2082 - accuracy: 0.9278\n",
      "Epoch 00273: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2082 - accuracy: 0.9287 - val_loss: 0.3491 - val_accuracy: 0.8909\n",
      "Epoch 274/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2108 - accuracy: 0.9265\n",
      "Epoch 00274: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2094 - accuracy: 0.9268 - val_loss: 0.3606 - val_accuracy: 0.8799\n",
      "Epoch 275/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2082 - accuracy: 0.9267\n",
      "Epoch 00275: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2072 - accuracy: 0.9268 - val_loss: 0.3653 - val_accuracy: 0.8874\n",
      "Epoch 276/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2215 - accuracy: 0.9217\n",
      "Epoch 00276: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2204 - accuracy: 0.9224 - val_loss: 0.3731 - val_accuracy: 0.8799\n",
      "Epoch 277/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2266 - accuracy: 0.9243\n",
      "Epoch 00277: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2220 - accuracy: 0.9258 - val_loss: 0.3312 - val_accuracy: 0.8997\n",
      "Epoch 278/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2115 - accuracy: 0.9275\n",
      "Epoch 00278: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2096 - accuracy: 0.9280 - val_loss: 0.3442 - val_accuracy: 0.8868\n",
      "Epoch 279/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1943 - accuracy: 0.9327\n",
      "Epoch 00279: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1959 - accuracy: 0.9324 - val_loss: 0.3392 - val_accuracy: 0.8895\n",
      "Epoch 280/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2045 - accuracy: 0.9297\n",
      "Epoch 00280: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2094 - accuracy: 0.9282 - val_loss: 0.3653 - val_accuracy: 0.8827\n",
      "Epoch 281/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2073 - accuracy: 0.9293\n",
      "Epoch 00281: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2099 - accuracy: 0.9283 - val_loss: 0.3595 - val_accuracy: 0.8868\n",
      "Epoch 282/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2020 - accuracy: 0.9291\n",
      "Epoch 00282: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2034 - accuracy: 0.9283 - val_loss: 0.3629 - val_accuracy: 0.8861\n",
      "Epoch 283/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2012 - accuracy: 0.9314\n",
      "Epoch 00283: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1998 - accuracy: 0.9323 - val_loss: 0.3490 - val_accuracy: 0.8922\n",
      "Epoch 284/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2011 - accuracy: 0.9280\n",
      "Epoch 00284: val_loss did not improve from 0.33032\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2035 - accuracy: 0.9270 - val_loss: 0.3677 - val_accuracy: 0.8840\n",
      "Epoch 285/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2063 - accuracy: 0.9293\n",
      "Epoch 00285: val_loss improved from 0.33032 to 0.32295, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2069 - accuracy: 0.9300 - val_loss: 0.3230 - val_accuracy: 0.9004\n",
      "Epoch 286/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1943 - accuracy: 0.9330\n",
      "Epoch 00286: val_loss did not improve from 0.32295\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1954 - accuracy: 0.9324 - val_loss: 0.3368 - val_accuracy: 0.8990\n",
      "Epoch 287/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2037 - accuracy: 0.9297\n",
      "Epoch 00287: val_loss did not improve from 0.32295\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2049 - accuracy: 0.9283 - val_loss: 0.3550 - val_accuracy: 0.8888\n",
      "Epoch 288/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2129 - accuracy: 0.9217\n",
      "Epoch 00288: val_loss did not improve from 0.32295\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2165 - accuracy: 0.9196 - val_loss: 0.3342 - val_accuracy: 0.8984\n",
      "Epoch 289/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2118 - accuracy: 0.9263\n",
      "Epoch 00289: val_loss did not improve from 0.32295\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2147 - accuracy: 0.9254 - val_loss: 0.3440 - val_accuracy: 0.8936\n",
      "Epoch 290/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2050 - accuracy: 0.9254\n",
      "Epoch 00290: val_loss did not improve from 0.32295\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2070 - accuracy: 0.9260 - val_loss: 0.3433 - val_accuracy: 0.8950\n",
      "Epoch 291/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1996 - accuracy: 0.9275\n",
      "Epoch 00291: val_loss improved from 0.32295 to 0.31519, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2003 - accuracy: 0.9266 - val_loss: 0.3152 - val_accuracy: 0.9004\n",
      "Epoch 292/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1912 - accuracy: 0.9321\n",
      "Epoch 00292: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1937 - accuracy: 0.9311 - val_loss: 0.3359 - val_accuracy: 0.9052\n",
      "Epoch 293/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.9314\n",
      "Epoch 00293: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1963 - accuracy: 0.9309 - val_loss: 0.3188 - val_accuracy: 0.9031\n",
      "Epoch 294/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.9315\n",
      "Epoch 00294: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1953 - accuracy: 0.9309 - val_loss: 0.3270 - val_accuracy: 0.9018\n",
      "Epoch 295/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.9327\n",
      "Epoch 00295: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1944 - accuracy: 0.9324 - val_loss: 0.3457 - val_accuracy: 0.8922\n",
      "Epoch 296/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1954 - accuracy: 0.9336\n",
      "Epoch 00296: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1999 - accuracy: 0.9312 - val_loss: 0.3365 - val_accuracy: 0.8970\n",
      "Epoch 297/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1930 - accuracy: 0.9328\n",
      "Epoch 00297: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1950 - accuracy: 0.9311 - val_loss: 0.3618 - val_accuracy: 0.8915\n",
      "Epoch 298/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1937 - accuracy: 0.9312\n",
      "Epoch 00298: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1953 - accuracy: 0.9314 - val_loss: 0.3572 - val_accuracy: 0.8929\n",
      "Epoch 299/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1853 - accuracy: 0.9364\n",
      "Epoch 00299: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1860 - accuracy: 0.9353 - val_loss: 0.3605 - val_accuracy: 0.8936\n",
      "Epoch 300/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.9284\n",
      "Epoch 00300: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1904 - accuracy: 0.9304 - val_loss: 0.3336 - val_accuracy: 0.9031\n",
      "Epoch 301/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1864 - accuracy: 0.9336\n",
      "Epoch 00301: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1858 - accuracy: 0.9340 - val_loss: 0.3346 - val_accuracy: 0.8950\n",
      "Epoch 302/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1868 - accuracy: 0.9332\n",
      "Epoch 00302: val_loss did not improve from 0.31519\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1897 - accuracy: 0.9309 - val_loss: 0.3169 - val_accuracy: 0.9100\n",
      "Epoch 303/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1941 - accuracy: 0.9343\n",
      "Epoch 00303: val_loss improved from 0.31519 to 0.31350, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1958 - accuracy: 0.9341 - val_loss: 0.3135 - val_accuracy: 0.9045\n",
      "Epoch 304/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2011 - accuracy: 0.9243\n",
      "Epoch 00304: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2027 - accuracy: 0.9246 - val_loss: 0.3400 - val_accuracy: 0.8909\n",
      "Epoch 305/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1883 - accuracy: 0.9343\n",
      "Epoch 00305: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1913 - accuracy: 0.9336 - val_loss: 0.3500 - val_accuracy: 0.8902\n",
      "Epoch 306/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1908 - accuracy: 0.9302\n",
      "Epoch 00306: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1904 - accuracy: 0.9311 - val_loss: 0.3389 - val_accuracy: 0.8936\n",
      "Epoch 307/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2002 - accuracy: 0.9252\n",
      "Epoch 00307: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1977 - accuracy: 0.9273 - val_loss: 0.3274 - val_accuracy: 0.9052\n",
      "Epoch 308/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1849 - accuracy: 0.9328\n",
      "Epoch 00308: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1865 - accuracy: 0.9335 - val_loss: 0.3477 - val_accuracy: 0.8922\n",
      "Epoch 309/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1854 - accuracy: 0.9334\n",
      "Epoch 00309: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1860 - accuracy: 0.9328 - val_loss: 0.3250 - val_accuracy: 0.8963\n",
      "Epoch 310/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1907 - accuracy: 0.9312\n",
      "Epoch 00310: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1926 - accuracy: 0.9300 - val_loss: 0.3327 - val_accuracy: 0.8977\n",
      "Epoch 311/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1829 - accuracy: 0.9377\n",
      "Epoch 00311: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1847 - accuracy: 0.9374 - val_loss: 0.3364 - val_accuracy: 0.8984\n",
      "Epoch 312/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1879 - accuracy: 0.9328\n",
      "Epoch 00312: val_loss did not improve from 0.31350\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1845 - accuracy: 0.9335 - val_loss: 0.3337 - val_accuracy: 0.9031\n",
      "Epoch 313/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1822 - accuracy: 0.9375\n",
      "Epoch 00313: val_loss improved from 0.31350 to 0.30858, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1810 - accuracy: 0.9394 - val_loss: 0.3086 - val_accuracy: 0.9059\n",
      "Epoch 314/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1843 - accuracy: 0.9330\n",
      "Epoch 00314: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1857 - accuracy: 0.9331 - val_loss: 0.3326 - val_accuracy: 0.8990\n",
      "Epoch 315/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1888 - accuracy: 0.9345\n",
      "Epoch 00315: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1871 - accuracy: 0.9345 - val_loss: 0.3384 - val_accuracy: 0.8909\n",
      "Epoch 316/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1892 - accuracy: 0.9328\n",
      "Epoch 00316: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1923 - accuracy: 0.9321 - val_loss: 0.3182 - val_accuracy: 0.8990\n",
      "Epoch 317/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1782 - accuracy: 0.9375\n",
      "Epoch 00317: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1794 - accuracy: 0.9376 - val_loss: 0.3171 - val_accuracy: 0.9038\n",
      "Epoch 318/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1776 - accuracy: 0.9418\n",
      "Epoch 00318: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1765 - accuracy: 0.9427 - val_loss: 0.3432 - val_accuracy: 0.8950\n",
      "Epoch 319/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1743 - accuracy: 0.9356\n",
      "Epoch 00319: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1758 - accuracy: 0.9357 - val_loss: 0.3509 - val_accuracy: 0.9011\n",
      "Epoch 320/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1909 - accuracy: 0.9304\n",
      "Epoch 00320: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1869 - accuracy: 0.9319 - val_loss: 0.3499 - val_accuracy: 0.9011\n",
      "Epoch 321/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1785 - accuracy: 0.9399\n",
      "Epoch 00321: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1798 - accuracy: 0.9391 - val_loss: 0.3201 - val_accuracy: 0.9059\n",
      "Epoch 322/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1798 - accuracy: 0.9368\n",
      "Epoch 00322: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1784 - accuracy: 0.9360 - val_loss: 0.3371 - val_accuracy: 0.9031\n",
      "Epoch 323/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1740 - accuracy: 0.9414\n",
      "Epoch 00323: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1743 - accuracy: 0.9413 - val_loss: 0.3205 - val_accuracy: 0.9011\n",
      "Epoch 324/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1757 - accuracy: 0.9384\n",
      "Epoch 00324: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1788 - accuracy: 0.9376 - val_loss: 0.3619 - val_accuracy: 0.8915\n",
      "Epoch 325/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1892 - accuracy: 0.9317\n",
      "Epoch 00325: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1940 - accuracy: 0.9299 - val_loss: 0.3240 - val_accuracy: 0.9011\n",
      "Epoch 326/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1777 - accuracy: 0.9377\n",
      "Epoch 00326: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1759 - accuracy: 0.9389 - val_loss: 0.3151 - val_accuracy: 0.9086\n",
      "Epoch 327/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1736 - accuracy: 0.9397\n",
      "Epoch 00327: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1740 - accuracy: 0.9401 - val_loss: 0.3098 - val_accuracy: 0.9038\n",
      "Epoch 328/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1766 - accuracy: 0.9407\n",
      "Epoch 00328: val_loss did not improve from 0.30858\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1782 - accuracy: 0.9398 - val_loss: 0.3240 - val_accuracy: 0.8895\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1676 - accuracy: 0.9436\n",
      "Epoch 00329: val_loss improved from 0.30858 to 0.29506, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1716 - accuracy: 0.9416 - val_loss: 0.2951 - val_accuracy: 0.9093\n",
      "Epoch 330/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1864 - accuracy: 0.9340\n",
      "Epoch 00330: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1859 - accuracy: 0.9335 - val_loss: 0.3163 - val_accuracy: 0.9038\n",
      "Epoch 331/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1658 - accuracy: 0.9408\n",
      "Epoch 00331: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1661 - accuracy: 0.9408 - val_loss: 0.3112 - val_accuracy: 0.9113\n",
      "Epoch 332/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1651 - accuracy: 0.9410\n",
      "Epoch 00332: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1673 - accuracy: 0.9403 - val_loss: 0.3033 - val_accuracy: 0.9072\n",
      "Epoch 333/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1681 - accuracy: 0.9390\n",
      "Epoch 00333: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1684 - accuracy: 0.9393 - val_loss: 0.3266 - val_accuracy: 0.8990\n",
      "Epoch 334/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1748 - accuracy: 0.9393\n",
      "Epoch 00334: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1723 - accuracy: 0.9396 - val_loss: 0.3201 - val_accuracy: 0.9031\n",
      "Epoch 335/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1680 - accuracy: 0.9412\n",
      "Epoch 00335: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1688 - accuracy: 0.9410 - val_loss: 0.3189 - val_accuracy: 0.8997\n",
      "Epoch 336/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1660 - accuracy: 0.9399\n",
      "Epoch 00336: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1707 - accuracy: 0.9389 - val_loss: 0.3011 - val_accuracy: 0.9045\n",
      "Epoch 337/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1718 - accuracy: 0.9399\n",
      "Epoch 00337: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1725 - accuracy: 0.9391 - val_loss: 0.3199 - val_accuracy: 0.9079\n",
      "Epoch 338/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1698 - accuracy: 0.9412\n",
      "Epoch 00338: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1710 - accuracy: 0.9410 - val_loss: 0.3066 - val_accuracy: 0.9113\n",
      "Epoch 339/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1630 - accuracy: 0.9397\n",
      "Epoch 00339: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1634 - accuracy: 0.9405 - val_loss: 0.3158 - val_accuracy: 0.9059\n",
      "Epoch 340/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1659 - accuracy: 0.9390\n",
      "Epoch 00340: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1689 - accuracy: 0.9377 - val_loss: 0.2990 - val_accuracy: 0.9052\n",
      "Epoch 341/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1759 - accuracy: 0.9338\n",
      "Epoch 00341: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1782 - accuracy: 0.9331 - val_loss: 0.3427 - val_accuracy: 0.8909\n",
      "Epoch 342/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1877 - accuracy: 0.9340\n",
      "Epoch 00342: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1898 - accuracy: 0.9328 - val_loss: 0.3293 - val_accuracy: 0.8970\n",
      "Epoch 343/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1829 - accuracy: 0.9356\n",
      "Epoch 00343: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1816 - accuracy: 0.9358 - val_loss: 0.3416 - val_accuracy: 0.8929\n",
      "Epoch 344/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1607 - accuracy: 0.9425\n",
      "Epoch 00344: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1616 - accuracy: 0.9422 - val_loss: 0.3081 - val_accuracy: 0.9093\n",
      "Epoch 345/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1631 - accuracy: 0.9416\n",
      "Epoch 00345: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1658 - accuracy: 0.9410 - val_loss: 0.3034 - val_accuracy: 0.9086\n",
      "Epoch 346/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1640 - accuracy: 0.9414\n",
      "Epoch 00346: val_loss did not improve from 0.29506\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1641 - accuracy: 0.9413 - val_loss: 0.3071 - val_accuracy: 0.9025\n",
      "Epoch 347/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1549 - accuracy: 0.9408\n",
      "Epoch 00347: val_loss improved from 0.29506 to 0.29259, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1552 - accuracy: 0.9408 - val_loss: 0.2926 - val_accuracy: 0.9072\n",
      "Epoch 348/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1634 - accuracy: 0.9446\n",
      "Epoch 00348: val_loss did not improve from 0.29259\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1675 - accuracy: 0.9439 - val_loss: 0.3093 - val_accuracy: 0.9052\n",
      "Epoch 349/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1617 - accuracy: 0.9431\n",
      "Epoch 00349: val_loss did not improve from 0.29259\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1591 - accuracy: 0.9435 - val_loss: 0.2973 - val_accuracy: 0.9086\n",
      "Epoch 350/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1716 - accuracy: 0.9395\n",
      "Epoch 00350: val_loss improved from 0.29259 to 0.29016, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1726 - accuracy: 0.9394 - val_loss: 0.2902 - val_accuracy: 0.9093\n",
      "Epoch 351/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1698 - accuracy: 0.9382\n",
      "Epoch 00351: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1690 - accuracy: 0.9399 - val_loss: 0.3148 - val_accuracy: 0.9059\n",
      "Epoch 352/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1563 - accuracy: 0.9455\n",
      "Epoch 00352: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1603 - accuracy: 0.9447 - val_loss: 0.3113 - val_accuracy: 0.9018\n",
      "Epoch 353/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1606 - accuracy: 0.9423\n",
      "Epoch 00353: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1643 - accuracy: 0.9403 - val_loss: 0.3159 - val_accuracy: 0.9052\n",
      "Epoch 354/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1679 - accuracy: 0.9390\n",
      "Epoch 00354: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1642 - accuracy: 0.9401 - val_loss: 0.3109 - val_accuracy: 0.9011\n",
      "Epoch 355/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1551 - accuracy: 0.9435\n",
      "Epoch 00355: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1551 - accuracy: 0.9434 - val_loss: 0.2960 - val_accuracy: 0.9113\n",
      "Epoch 356/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1536 - accuracy: 0.9487\n",
      "Epoch 00356: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1542 - accuracy: 0.9490 - val_loss: 0.3374 - val_accuracy: 0.9011\n",
      "Epoch 357/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1536 - accuracy: 0.9449\n",
      "Epoch 00357: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1548 - accuracy: 0.9444 - val_loss: 0.2969 - val_accuracy: 0.9052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1619 - accuracy: 0.9392\n",
      "Epoch 00358: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1590 - accuracy: 0.9406 - val_loss: 0.3217 - val_accuracy: 0.9025\n",
      "Epoch 359/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1511 - accuracy: 0.9490\n",
      "Epoch 00359: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1580 - accuracy: 0.9468 - val_loss: 0.3113 - val_accuracy: 0.9072\n",
      "Epoch 360/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.9434\n",
      "Epoch 00360: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1599 - accuracy: 0.9422 - val_loss: 0.3371 - val_accuracy: 0.9045\n",
      "Epoch 361/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1604 - accuracy: 0.9436\n",
      "Epoch 00361: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1630 - accuracy: 0.9420 - val_loss: 0.2945 - val_accuracy: 0.9093\n",
      "Epoch 362/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1544 - accuracy: 0.9438\n",
      "Epoch 00362: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1530 - accuracy: 0.9445 - val_loss: 0.3300 - val_accuracy: 0.9004\n",
      "Epoch 363/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1647 - accuracy: 0.9412\n",
      "Epoch 00363: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1664 - accuracy: 0.9406 - val_loss: 0.3348 - val_accuracy: 0.8990\n",
      "Epoch 364/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1567 - accuracy: 0.9451\n",
      "Epoch 00364: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1551 - accuracy: 0.9464 - val_loss: 0.3115 - val_accuracy: 0.9052\n",
      "Epoch 365/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1448 - accuracy: 0.9511\n",
      "Epoch 00365: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1464 - accuracy: 0.9505 - val_loss: 0.3113 - val_accuracy: 0.9079\n",
      "Epoch 366/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1576 - accuracy: 0.9461\n",
      "Epoch 00366: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1553 - accuracy: 0.9468 - val_loss: 0.3133 - val_accuracy: 0.9025\n",
      "Epoch 367/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1550 - accuracy: 0.9436\n",
      "Epoch 00367: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1574 - accuracy: 0.9420 - val_loss: 0.3530 - val_accuracy: 0.8881\n",
      "Epoch 368/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1581 - accuracy: 0.9429\n",
      "Epoch 00368: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1576 - accuracy: 0.9432 - val_loss: 0.3422 - val_accuracy: 0.8977\n",
      "Epoch 369/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1655 - accuracy: 0.9399\n",
      "Epoch 00369: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1631 - accuracy: 0.9403 - val_loss: 0.3149 - val_accuracy: 0.9038\n",
      "Epoch 370/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1569 - accuracy: 0.9461\n",
      "Epoch 00370: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1550 - accuracy: 0.9471 - val_loss: 0.3212 - val_accuracy: 0.9045\n",
      "Epoch 371/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1591 - accuracy: 0.9449\n",
      "Epoch 00371: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1560 - accuracy: 0.9461 - val_loss: 0.3112 - val_accuracy: 0.9086\n",
      "Epoch 372/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1419 - accuracy: 0.9496\n",
      "Epoch 00372: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1428 - accuracy: 0.9493 - val_loss: 0.3290 - val_accuracy: 0.9052\n",
      "Epoch 373/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1571 - accuracy: 0.9440\n",
      "Epoch 00373: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1561 - accuracy: 0.9444 - val_loss: 0.3006 - val_accuracy: 0.9134\n",
      "Epoch 374/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1559 - accuracy: 0.9436\n",
      "Epoch 00374: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1551 - accuracy: 0.9451 - val_loss: 0.3301 - val_accuracy: 0.8977\n",
      "Epoch 375/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1523 - accuracy: 0.9481\n",
      "Epoch 00375: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1509 - accuracy: 0.9480 - val_loss: 0.3084 - val_accuracy: 0.9100\n",
      "Epoch 376/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1519 - accuracy: 0.9461\n",
      "Epoch 00376: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1529 - accuracy: 0.9452 - val_loss: 0.3419 - val_accuracy: 0.8950\n",
      "Epoch 377/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1512 - accuracy: 0.9470\n",
      "Epoch 00377: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1528 - accuracy: 0.9466 - val_loss: 0.3408 - val_accuracy: 0.8997\n",
      "Epoch 378/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1494 - accuracy: 0.9474\n",
      "Epoch 00378: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1525 - accuracy: 0.9469 - val_loss: 0.3069 - val_accuracy: 0.9120\n",
      "Epoch 379/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1425 - accuracy: 0.9511\n",
      "Epoch 00379: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1426 - accuracy: 0.9507 - val_loss: 0.3291 - val_accuracy: 0.9059\n",
      "Epoch 380/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1486 - accuracy: 0.9429\n",
      "Epoch 00380: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1456 - accuracy: 0.9451 - val_loss: 0.2952 - val_accuracy: 0.9113\n",
      "Epoch 381/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1451 - accuracy: 0.9475\n",
      "Epoch 00381: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1456 - accuracy: 0.9474 - val_loss: 0.3060 - val_accuracy: 0.9072\n",
      "Epoch 382/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1407 - accuracy: 0.9518\n",
      "Epoch 00382: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1429 - accuracy: 0.9512 - val_loss: 0.3145 - val_accuracy: 0.8997\n",
      "Epoch 383/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1439 - accuracy: 0.9494\n",
      "Epoch 00383: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1443 - accuracy: 0.9497 - val_loss: 0.3256 - val_accuracy: 0.9038\n",
      "Epoch 384/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1538 - accuracy: 0.9453\n",
      "Epoch 00384: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1528 - accuracy: 0.9459 - val_loss: 0.3033 - val_accuracy: 0.8977\n",
      "Epoch 385/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1534 - accuracy: 0.9453\n",
      "Epoch 00385: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1539 - accuracy: 0.9456 - val_loss: 0.3085 - val_accuracy: 0.9106\n",
      "Epoch 386/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1439 - accuracy: 0.9503\n",
      "Epoch 00386: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1411 - accuracy: 0.9512 - val_loss: 0.3047 - val_accuracy: 0.9045\n",
      "Epoch 387/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1382 - accuracy: 0.9520\n",
      "Epoch 00387: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1396 - accuracy: 0.9514 - val_loss: 0.2978 - val_accuracy: 0.9113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1561 - accuracy: 0.9466\n",
      "Epoch 00388: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1572 - accuracy: 0.9463 - val_loss: 0.3436 - val_accuracy: 0.9004\n",
      "Epoch 389/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1485 - accuracy: 0.9468\n",
      "Epoch 00389: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1480 - accuracy: 0.9474 - val_loss: 0.3445 - val_accuracy: 0.9031\n",
      "Epoch 390/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1530 - accuracy: 0.9440\n",
      "Epoch 00390: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1508 - accuracy: 0.9444 - val_loss: 0.3203 - val_accuracy: 0.9106\n",
      "Epoch 391/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1392 - accuracy: 0.9513\n",
      "Epoch 00391: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1447 - accuracy: 0.9483 - val_loss: 0.3051 - val_accuracy: 0.9106\n",
      "Epoch 392/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1320 - accuracy: 0.9522\n",
      "Epoch 00392: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1342 - accuracy: 0.9521 - val_loss: 0.3073 - val_accuracy: 0.9045\n",
      "Epoch 393/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1578 - accuracy: 0.9453\n",
      "Epoch 00393: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1572 - accuracy: 0.9454 - val_loss: 0.2919 - val_accuracy: 0.9175\n",
      "Epoch 394/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1484 - accuracy: 0.9464\n",
      "Epoch 00394: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1464 - accuracy: 0.9471 - val_loss: 0.3617 - val_accuracy: 0.8950\n",
      "Epoch 395/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1372 - accuracy: 0.9526\n",
      "Epoch 00395: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1375 - accuracy: 0.9527 - val_loss: 0.3105 - val_accuracy: 0.9100\n",
      "Epoch 396/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1326 - accuracy: 0.9535\n",
      "Epoch 00396: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1331 - accuracy: 0.9531 - val_loss: 0.3215 - val_accuracy: 0.9025\n",
      "Epoch 397/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1415 - accuracy: 0.9496\n",
      "Epoch 00397: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1428 - accuracy: 0.9497 - val_loss: 0.3228 - val_accuracy: 0.9059\n",
      "Epoch 398/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1559 - accuracy: 0.9433\n",
      "Epoch 00398: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1559 - accuracy: 0.9439 - val_loss: 0.3116 - val_accuracy: 0.9045\n",
      "Epoch 399/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1413 - accuracy: 0.9483\n",
      "Epoch 00399: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1409 - accuracy: 0.9497 - val_loss: 0.3002 - val_accuracy: 0.9127\n",
      "Epoch 400/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1452 - accuracy: 0.9455\n",
      "Epoch 00400: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1433 - accuracy: 0.9468 - val_loss: 0.3003 - val_accuracy: 0.9065\n",
      "Epoch 401/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1495 - accuracy: 0.9470\n",
      "Epoch 00401: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1492 - accuracy: 0.9473 - val_loss: 0.3490 - val_accuracy: 0.9018\n",
      "Epoch 402/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1392 - accuracy: 0.9522\n",
      "Epoch 00402: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1398 - accuracy: 0.9519 - val_loss: 0.3286 - val_accuracy: 0.9106\n",
      "Epoch 403/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1324 - accuracy: 0.9533\n",
      "Epoch 00403: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1366 - accuracy: 0.9515 - val_loss: 0.3295 - val_accuracy: 0.9072\n",
      "Epoch 404/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1461 - accuracy: 0.9468\n",
      "Epoch 00404: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1463 - accuracy: 0.9474 - val_loss: 0.3350 - val_accuracy: 0.9065\n",
      "Epoch 405/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1416 - accuracy: 0.9470\n",
      "Epoch 00405: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1403 - accuracy: 0.9480 - val_loss: 0.3275 - val_accuracy: 0.9031\n",
      "Epoch 406/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1414 - accuracy: 0.9515\n",
      "Epoch 00406: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1382 - accuracy: 0.9527 - val_loss: 0.2986 - val_accuracy: 0.9052\n",
      "Epoch 407/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1305 - accuracy: 0.9554\n",
      "Epoch 00407: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1314 - accuracy: 0.9551 - val_loss: 0.3253 - val_accuracy: 0.9052\n",
      "Epoch 408/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1421 - accuracy: 0.9490\n",
      "Epoch 00408: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1423 - accuracy: 0.9488 - val_loss: 0.3329 - val_accuracy: 0.9045\n",
      "Epoch 409/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1417 - accuracy: 0.9485\n",
      "Epoch 00409: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1435 - accuracy: 0.9481 - val_loss: 0.3057 - val_accuracy: 0.9059\n",
      "Epoch 410/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1381 - accuracy: 0.9526\n",
      "Epoch 00410: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1390 - accuracy: 0.9519 - val_loss: 0.3080 - val_accuracy: 0.9106\n",
      "Epoch 411/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1382 - accuracy: 0.9507\n",
      "Epoch 00411: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1378 - accuracy: 0.9507 - val_loss: 0.3146 - val_accuracy: 0.9079\n",
      "Epoch 412/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1311 - accuracy: 0.9535\n",
      "Epoch 00412: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1311 - accuracy: 0.9539 - val_loss: 0.3058 - val_accuracy: 0.9086\n",
      "Epoch 413/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1310 - accuracy: 0.9513\n",
      "Epoch 00413: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1318 - accuracy: 0.9509 - val_loss: 0.3047 - val_accuracy: 0.9113\n",
      "Epoch 414/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1279 - accuracy: 0.9563\n",
      "Epoch 00414: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1281 - accuracy: 0.9556 - val_loss: 0.3137 - val_accuracy: 0.9106\n",
      "Epoch 415/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1304 - accuracy: 0.9509\n",
      "Epoch 00415: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1291 - accuracy: 0.9519 - val_loss: 0.3018 - val_accuracy: 0.9086\n",
      "Epoch 416/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1234 - accuracy: 0.9570\n",
      "Epoch 00416: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1201 - accuracy: 0.9585 - val_loss: 0.3124 - val_accuracy: 0.9127\n",
      "Epoch 417/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1313 - accuracy: 0.9522\n",
      "Epoch 00417: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1326 - accuracy: 0.9526 - val_loss: 0.2971 - val_accuracy: 0.9072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1381 - accuracy: 0.9505\n",
      "Epoch 00418: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1352 - accuracy: 0.9515 - val_loss: 0.3322 - val_accuracy: 0.9065\n",
      "Epoch 419/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1433 - accuracy: 0.9472\n",
      "Epoch 00419: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1451 - accuracy: 0.9474 - val_loss: 0.3303 - val_accuracy: 0.9072\n",
      "Epoch 420/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1354 - accuracy: 0.9511\n",
      "Epoch 00420: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1364 - accuracy: 0.9514 - val_loss: 0.3330 - val_accuracy: 0.9120\n",
      "Epoch 421/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1251 - accuracy: 0.9542\n",
      "Epoch 00421: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1258 - accuracy: 0.9538 - val_loss: 0.3248 - val_accuracy: 0.9086\n",
      "Epoch 422/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1325 - accuracy: 0.9529\n",
      "Epoch 00422: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1317 - accuracy: 0.9538 - val_loss: 0.2976 - val_accuracy: 0.9120\n",
      "Epoch 423/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1223 - accuracy: 0.9594\n",
      "Epoch 00423: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1239 - accuracy: 0.9589 - val_loss: 0.3112 - val_accuracy: 0.9079\n",
      "Epoch 424/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1208 - accuracy: 0.9548\n",
      "Epoch 00424: val_loss did not improve from 0.29016\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1213 - accuracy: 0.9546 - val_loss: 0.3246 - val_accuracy: 0.9086\n",
      "Epoch 425/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1291 - accuracy: 0.9568\n",
      "Epoch 00425: val_loss improved from 0.29016 to 0.28570, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1296 - accuracy: 0.9560 - val_loss: 0.2857 - val_accuracy: 0.9113\n",
      "Epoch 426/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1303 - accuracy: 0.9550\n",
      "Epoch 00426: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1288 - accuracy: 0.9550 - val_loss: 0.3025 - val_accuracy: 0.9086\n",
      "Epoch 427/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1312 - accuracy: 0.9546\n",
      "Epoch 00427: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1326 - accuracy: 0.9543 - val_loss: 0.3272 - val_accuracy: 0.9065\n",
      "Epoch 428/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1426 - accuracy: 0.9481\n",
      "Epoch 00428: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1390 - accuracy: 0.9497 - val_loss: 0.3377 - val_accuracy: 0.9025\n",
      "Epoch 429/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1439 - accuracy: 0.9477\n",
      "Epoch 00429: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1419 - accuracy: 0.9485 - val_loss: 0.3042 - val_accuracy: 0.9059\n",
      "Epoch 430/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1339 - accuracy: 0.9552\n",
      "Epoch 00430: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1340 - accuracy: 0.9544 - val_loss: 0.3007 - val_accuracy: 0.9106\n",
      "Epoch 431/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1263 - accuracy: 0.9589\n",
      "Epoch 00431: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1253 - accuracy: 0.9585 - val_loss: 0.3184 - val_accuracy: 0.9120\n",
      "Epoch 432/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1209 - accuracy: 0.9552\n",
      "Epoch 00432: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1217 - accuracy: 0.9550 - val_loss: 0.3222 - val_accuracy: 0.9093\n",
      "Epoch 433/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1339 - accuracy: 0.9505\n",
      "Epoch 00433: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1316 - accuracy: 0.9507 - val_loss: 0.3302 - val_accuracy: 0.9059\n",
      "Epoch 434/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1215 - accuracy: 0.9567\n",
      "Epoch 00434: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1202 - accuracy: 0.9570 - val_loss: 0.3097 - val_accuracy: 0.9113\n",
      "Epoch 435/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9576\n",
      "Epoch 00435: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1280 - accuracy: 0.9572 - val_loss: 0.3166 - val_accuracy: 0.9113\n",
      "Epoch 436/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9559\n",
      "Epoch 00436: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1252 - accuracy: 0.9555 - val_loss: 0.3143 - val_accuracy: 0.9052\n",
      "Epoch 437/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1358 - accuracy: 0.9518\n",
      "Epoch 00437: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1351 - accuracy: 0.9517 - val_loss: 0.3136 - val_accuracy: 0.9052\n",
      "Epoch 438/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1291 - accuracy: 0.9528\n",
      "Epoch 00438: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1289 - accuracy: 0.9533 - val_loss: 0.3595 - val_accuracy: 0.8990\n",
      "Epoch 439/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1302 - accuracy: 0.9520\n",
      "Epoch 00439: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1310 - accuracy: 0.9515 - val_loss: 0.3264 - val_accuracy: 0.9004\n",
      "Epoch 440/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1310 - accuracy: 0.9513\n",
      "Epoch 00440: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1267 - accuracy: 0.9531 - val_loss: 0.3272 - val_accuracy: 0.9052\n",
      "Epoch 441/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1196 - accuracy: 0.9578\n",
      "Epoch 00441: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1208 - accuracy: 0.9575 - val_loss: 0.3161 - val_accuracy: 0.9086\n",
      "Epoch 442/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1234 - accuracy: 0.9528\n",
      "Epoch 00442: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1225 - accuracy: 0.9533 - val_loss: 0.3298 - val_accuracy: 0.9113\n",
      "Epoch 443/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1142 - accuracy: 0.9570\n",
      "Epoch 00443: val_loss did not improve from 0.28570\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1171 - accuracy: 0.9558 - val_loss: 0.3155 - val_accuracy: 0.9079\n",
      "Epoch 00443: early stopping\n",
      "Training for model  3  completed in time:  0:03:02.526628 seconds\n",
      "Training for model  4  has started.\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 4.8550 - accuracy: 0.1756\n",
      "Epoch 00001: val_loss improved from inf to 2.07937, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 4.8550 - accuracy: 0.1756 - val_loss: 2.0794 - val_accuracy: 0.2503\n",
      "Epoch 2/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.8524 - accuracy: 0.2980\n",
      "Epoch 00002: val_loss improved from 2.07937 to 1.82242, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.8457 - accuracy: 0.3008 - val_loss: 1.8224 - val_accuracy: 0.3267\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 1.6692 - accuracy: 0.3837\n",
      "Epoch 00003: val_loss improved from 1.82242 to 1.61659, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.6642 - accuracy: 0.3878 - val_loss: 1.6166 - val_accuracy: 0.4325\n",
      "Epoch 4/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.4685 - accuracy: 0.4795\n",
      "Epoch 00004: val_loss improved from 1.61659 to 1.40310, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.4650 - accuracy: 0.4800 - val_loss: 1.4031 - val_accuracy: 0.5150\n",
      "Epoch 5/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.3359 - accuracy: 0.5292\n",
      "Epoch 00005: val_loss improved from 1.40310 to 1.26456, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.3386 - accuracy: 0.5305 - val_loss: 1.2646 - val_accuracy: 0.5737\n",
      "Epoch 6/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.2545 - accuracy: 0.5606\n",
      "Epoch 00006: val_loss improved from 1.26456 to 1.24310, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.2441 - accuracy: 0.5673 - val_loss: 1.2431 - val_accuracy: 0.5859\n",
      "Epoch 7/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1765 - accuracy: 0.5871\n",
      "Epoch 00007: val_loss improved from 1.24310 to 1.15996, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.1674 - accuracy: 0.5888 - val_loss: 1.1600 - val_accuracy: 0.6126\n",
      "Epoch 8/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.1157 - accuracy: 0.6105\n",
      "Epoch 00008: val_loss improved from 1.15996 to 1.13697, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.1143 - accuracy: 0.6122 - val_loss: 1.1370 - val_accuracy: 0.6105\n",
      "Epoch 9/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0611 - accuracy: 0.6350\n",
      "Epoch 00009: val_loss improved from 1.13697 to 1.09130, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.0635 - accuracy: 0.6347 - val_loss: 1.0913 - val_accuracy: 0.6467\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 1.0192 - accuracy: 0.6501\n",
      "Epoch 00010: val_loss improved from 1.09130 to 1.04262, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.0189 - accuracy: 0.6492 - val_loss: 1.0426 - val_accuracy: 0.6555\n",
      "Epoch 11/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9644 - accuracy: 0.6657\n",
      "Epoch 00011: val_loss improved from 1.04262 to 1.01639, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.9654 - accuracy: 0.6659 - val_loss: 1.0164 - val_accuracy: 0.6767\n",
      "Epoch 12/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.9241 - accuracy: 0.6780\n",
      "Epoch 00012: val_loss improved from 1.01639 to 0.93830, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.9239 - accuracy: 0.6786 - val_loss: 0.9383 - val_accuracy: 0.6930\n",
      "Epoch 13/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8892 - accuracy: 0.6972\n",
      "Epoch 00013: val_loss improved from 0.93830 to 0.93738, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.8943 - accuracy: 0.6944 - val_loss: 0.9374 - val_accuracy: 0.6971\n",
      "Epoch 14/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8699 - accuracy: 0.7024\n",
      "Epoch 00014: val_loss improved from 0.93738 to 0.90800, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.8666 - accuracy: 0.7055 - val_loss: 0.9080 - val_accuracy: 0.7053\n",
      "Epoch 15/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.8274 - accuracy: 0.7173\n",
      "Epoch 00015: val_loss improved from 0.90800 to 0.86407, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.8278 - accuracy: 0.7163 - val_loss: 0.8641 - val_accuracy: 0.7162\n",
      "Epoch 16/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7993 - accuracy: 0.7217\n",
      "Epoch 00016: val_loss improved from 0.86407 to 0.83140, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.8026 - accuracy: 0.7200 - val_loss: 0.8314 - val_accuracy: 0.7224\n",
      "Epoch 17/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7868 - accuracy: 0.7327\n",
      "Epoch 00017: val_loss improved from 0.83140 to 0.81246, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.7861 - accuracy: 0.7321 - val_loss: 0.8125 - val_accuracy: 0.7476\n",
      "Epoch 18/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7438 - accuracy: 0.7448\n",
      "Epoch 00018: val_loss improved from 0.81246 to 0.74790, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.7399 - accuracy: 0.7459 - val_loss: 0.7479 - val_accuracy: 0.7585\n",
      "Epoch 19/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.7060 - accuracy: 0.7675\n",
      "Epoch 00019: val_loss did not improve from 0.74790\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7068 - accuracy: 0.7666 - val_loss: 0.7530 - val_accuracy: 0.7606\n",
      "Epoch 20/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6925 - accuracy: 0.7664\n",
      "Epoch 00020: val_loss improved from 0.74790 to 0.70901, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.6881 - accuracy: 0.7685 - val_loss: 0.7090 - val_accuracy: 0.7667\n",
      "Epoch 21/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6815 - accuracy: 0.7692\n",
      "Epoch 00021: val_loss improved from 0.70901 to 0.69744, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.6828 - accuracy: 0.7685 - val_loss: 0.6974 - val_accuracy: 0.7701\n",
      "Epoch 22/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6539 - accuracy: 0.7788\n",
      "Epoch 00022: val_loss improved from 0.69744 to 0.67666, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.6632 - accuracy: 0.7753 - val_loss: 0.6767 - val_accuracy: 0.7769\n",
      "Epoch 23/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6286 - accuracy: 0.7870\n",
      "Epoch 00023: val_loss improved from 0.67666 to 0.64632, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.6306 - accuracy: 0.7854 - val_loss: 0.6463 - val_accuracy: 0.7920\n",
      "Epoch 24/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6063 - accuracy: 0.7976\n",
      "Epoch 00024: val_loss did not improve from 0.64632\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6072 - accuracy: 0.7963 - val_loss: 0.6575 - val_accuracy: 0.7913\n",
      "Epoch 25/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5962 - accuracy: 0.7948\n",
      "Epoch 00025: val_loss did not improve from 0.64632\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5982 - accuracy: 0.7954 - val_loss: 0.6765 - val_accuracy: 0.7879\n",
      "Epoch 26/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5974 - accuracy: 0.7961\n",
      "Epoch 00026: val_loss improved from 0.64632 to 0.62422, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.5955 - accuracy: 0.7956 - val_loss: 0.6242 - val_accuracy: 0.8008\n",
      "Epoch 27/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5776 - accuracy: 0.8080\n",
      "Epoch 00027: val_loss improved from 0.62422 to 0.60373, saving model to models/saved_models/best_models_DP_0.1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5778 - accuracy: 0.8063 - val_loss: 0.6037 - val_accuracy: 0.7967\n",
      "Epoch 28/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5651 - accuracy: 0.8116\n",
      "Epoch 00028: val_loss did not improve from 0.60373\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5675 - accuracy: 0.8120 - val_loss: 0.6203 - val_accuracy: 0.7967\n",
      "Epoch 29/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5689 - accuracy: 0.8088\n",
      "Epoch 00029: val_loss improved from 0.60373 to 0.59986, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5671 - accuracy: 0.8106 - val_loss: 0.5999 - val_accuracy: 0.8063\n",
      "Epoch 30/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5303 - accuracy: 0.8198\n",
      "Epoch 00030: val_loss improved from 0.59986 to 0.56098, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.5299 - accuracy: 0.8200 - val_loss: 0.5610 - val_accuracy: 0.8213\n",
      "Epoch 31/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5269 - accuracy: 0.8240\n",
      "Epoch 00031: val_loss improved from 0.56098 to 0.55724, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.5280 - accuracy: 0.8219 - val_loss: 0.5572 - val_accuracy: 0.8267\n",
      "Epoch 32/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5143 - accuracy: 0.8255\n",
      "Epoch 00032: val_loss improved from 0.55724 to 0.54276, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.5177 - accuracy: 0.8243 - val_loss: 0.5428 - val_accuracy: 0.8288\n",
      "Epoch 33/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4891 - accuracy: 0.8404\n",
      "Epoch 00033: val_loss improved from 0.54276 to 0.54263, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.4929 - accuracy: 0.8372 - val_loss: 0.5426 - val_accuracy: 0.8267\n",
      "Epoch 34/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4989 - accuracy: 0.8333\n",
      "Epoch 00034: val_loss improved from 0.54263 to 0.51562, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4956 - accuracy: 0.8347 - val_loss: 0.5156 - val_accuracy: 0.8383\n",
      "Epoch 35/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4763 - accuracy: 0.8413\n",
      "Epoch 00035: val_loss did not improve from 0.51562\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4762 - accuracy: 0.8427 - val_loss: 0.5306 - val_accuracy: 0.8308\n",
      "Epoch 36/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4637 - accuracy: 0.8497\n",
      "Epoch 00036: val_loss improved from 0.51562 to 0.48175, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.4701 - accuracy: 0.8444 - val_loss: 0.4817 - val_accuracy: 0.8547\n",
      "Epoch 37/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4519 - accuracy: 0.8445\n",
      "Epoch 00037: val_loss did not improve from 0.48175\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4534 - accuracy: 0.8429 - val_loss: 0.5369 - val_accuracy: 0.8192\n",
      "Epoch 38/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4623 - accuracy: 0.8462\n",
      "Epoch 00038: val_loss did not improve from 0.48175\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4668 - accuracy: 0.8430 - val_loss: 0.5409 - val_accuracy: 0.8267\n",
      "Epoch 39/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4438 - accuracy: 0.8469\n",
      "Epoch 00039: val_loss did not improve from 0.48175\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4421 - accuracy: 0.8476 - val_loss: 0.4973 - val_accuracy: 0.8308\n",
      "Epoch 40/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4520 - accuracy: 0.8525\n",
      "Epoch 00040: val_loss improved from 0.48175 to 0.47055, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.4463 - accuracy: 0.8541 - val_loss: 0.4705 - val_accuracy: 0.8527\n",
      "Epoch 41/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4059 - accuracy: 0.8640\n",
      "Epoch 00041: val_loss improved from 0.47055 to 0.46827, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4121 - accuracy: 0.8630 - val_loss: 0.4683 - val_accuracy: 0.8445\n",
      "Epoch 42/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4221 - accuracy: 0.8588\n",
      "Epoch 00042: val_loss improved from 0.46827 to 0.43984, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4187 - accuracy: 0.8584 - val_loss: 0.4398 - val_accuracy: 0.8608\n",
      "Epoch 43/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4184 - accuracy: 0.8609\n",
      "Epoch 00043: val_loss did not improve from 0.43984\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4182 - accuracy: 0.8598 - val_loss: 0.4802 - val_accuracy: 0.8404\n",
      "Epoch 44/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3891 - accuracy: 0.8703\n",
      "Epoch 00044: val_loss did not improve from 0.43984\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3913 - accuracy: 0.8705 - val_loss: 0.4467 - val_accuracy: 0.8581\n",
      "Epoch 45/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3852 - accuracy: 0.8707\n",
      "Epoch 00045: val_loss improved from 0.43984 to 0.42035, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.3865 - accuracy: 0.8695 - val_loss: 0.4203 - val_accuracy: 0.8649\n",
      "Epoch 46/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3665 - accuracy: 0.8718\n",
      "Epoch 00046: val_loss improved from 0.42035 to 0.41080, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.3734 - accuracy: 0.8712 - val_loss: 0.4108 - val_accuracy: 0.8697\n",
      "Epoch 47/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3562 - accuracy: 0.8741\n",
      "Epoch 00047: val_loss did not improve from 0.41080\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3569 - accuracy: 0.8746 - val_loss: 0.4444 - val_accuracy: 0.8615\n",
      "Epoch 48/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3736 - accuracy: 0.8728\n",
      "Epoch 00048: val_loss improved from 0.41080 to 0.41052, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.3724 - accuracy: 0.8736 - val_loss: 0.4105 - val_accuracy: 0.8656\n",
      "Epoch 49/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3587 - accuracy: 0.8795\n",
      "Epoch 00049: val_loss did not improve from 0.41052\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3724 - accuracy: 0.8765 - val_loss: 0.4212 - val_accuracy: 0.8670\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3801 - accuracy: 0.8709\n",
      "Epoch 00050: val_loss improved from 0.41052 to 0.40274, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.3765 - accuracy: 0.8717 - val_loss: 0.4027 - val_accuracy: 0.8724\n",
      "Epoch 51/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3444 - accuracy: 0.8860\n",
      "Epoch 00051: val_loss improved from 0.40274 to 0.39997, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3426 - accuracy: 0.8864 - val_loss: 0.4000 - val_accuracy: 0.8752\n",
      "Epoch 52/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3514 - accuracy: 0.8763\n",
      "Epoch 00052: val_loss did not improve from 0.39997\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3535 - accuracy: 0.8763 - val_loss: 0.4243 - val_accuracy: 0.8547\n",
      "Epoch 53/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3297 - accuracy: 0.8873\n",
      "Epoch 00053: val_loss improved from 0.39997 to 0.38830, saving model to models/saved_models/best_models_DP_0.1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 21ms/step - loss: 0.3311 - accuracy: 0.8881 - val_loss: 0.3883 - val_accuracy: 0.8711\n",
      "Epoch 54/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3212 - accuracy: 0.8890\n",
      "Epoch 00054: val_loss did not improve from 0.38830\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3201 - accuracy: 0.8884 - val_loss: 0.4036 - val_accuracy: 0.8629\n",
      "Epoch 55/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3190 - accuracy: 0.8943\n",
      "Epoch 00055: val_loss improved from 0.38830 to 0.37657, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.3175 - accuracy: 0.8944 - val_loss: 0.3766 - val_accuracy: 0.8799\n",
      "Epoch 56/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3063 - accuracy: 0.8949\n",
      "Epoch 00056: val_loss improved from 0.37657 to 0.35190, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.3090 - accuracy: 0.8944 - val_loss: 0.3519 - val_accuracy: 0.8840\n",
      "Epoch 57/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3117 - accuracy: 0.8964\n",
      "Epoch 00057: val_loss did not improve from 0.35190\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.3106 - accuracy: 0.8966 - val_loss: 0.3988 - val_accuracy: 0.8697\n",
      "Epoch 58/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3112 - accuracy: 0.8903\n",
      "Epoch 00058: val_loss did not improve from 0.35190\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.3118 - accuracy: 0.8903 - val_loss: 0.3584 - val_accuracy: 0.8861\n",
      "Epoch 59/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3035 - accuracy: 0.9012\n",
      "Epoch 00059: val_loss improved from 0.35190 to 0.35132, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.3024 - accuracy: 0.9009 - val_loss: 0.3513 - val_accuracy: 0.8854\n",
      "Epoch 60/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2912 - accuracy: 0.8975\n",
      "Epoch 00060: val_loss did not improve from 0.35132\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2929 - accuracy: 0.8958 - val_loss: 0.4012 - val_accuracy: 0.8615\n",
      "Epoch 61/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2980 - accuracy: 0.8951\n",
      "Epoch 00061: val_loss improved from 0.35132 to 0.35034, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2983 - accuracy: 0.8966 - val_loss: 0.3503 - val_accuracy: 0.8854\n",
      "Epoch 62/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2838 - accuracy: 0.9040\n",
      "Epoch 00062: val_loss improved from 0.35034 to 0.33821, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2842 - accuracy: 0.9053 - val_loss: 0.3382 - val_accuracy: 0.8956\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2858 - accuracy: 0.9029\n",
      "Epoch 00063: val_loss did not improve from 0.33821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2850 - accuracy: 0.9036 - val_loss: 0.3491 - val_accuracy: 0.8929\n",
      "Epoch 64/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2802 - accuracy: 0.9079\n",
      "Epoch 00064: val_loss did not improve from 0.33821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2806 - accuracy: 0.9074 - val_loss: 0.3405 - val_accuracy: 0.8943\n",
      "Epoch 65/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2644 - accuracy: 0.9055\n",
      "Epoch 00065: val_loss did not improve from 0.33821\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2669 - accuracy: 0.9050 - val_loss: 0.3444 - val_accuracy: 0.8895\n",
      "Epoch 66/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2648 - accuracy: 0.9061\n",
      "Epoch 00066: val_loss improved from 0.33821 to 0.32854, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2691 - accuracy: 0.9048 - val_loss: 0.3285 - val_accuracy: 0.8922\n",
      "Epoch 67/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2654 - accuracy: 0.9070\n",
      "Epoch 00067: val_loss did not improve from 0.32854\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2623 - accuracy: 0.9072 - val_loss: 0.3418 - val_accuracy: 0.8970\n",
      "Epoch 68/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2543 - accuracy: 0.9074\n",
      "Epoch 00068: val_loss did not improve from 0.32854\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2550 - accuracy: 0.9072 - val_loss: 0.3488 - val_accuracy: 0.8881\n",
      "Epoch 69/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2601 - accuracy: 0.9096\n",
      "Epoch 00069: val_loss did not improve from 0.32854\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2639 - accuracy: 0.9084 - val_loss: 0.3419 - val_accuracy: 0.8922\n",
      "Epoch 70/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2542 - accuracy: 0.9111\n",
      "Epoch 00070: val_loss did not improve from 0.32854\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2552 - accuracy: 0.9109 - val_loss: 0.3563 - val_accuracy: 0.8834\n",
      "Epoch 71/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2553 - accuracy: 0.9102\n",
      "Epoch 00071: val_loss did not improve from 0.32854\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2528 - accuracy: 0.9109 - val_loss: 0.3559 - val_accuracy: 0.8847\n",
      "Epoch 72/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2510 - accuracy: 0.9141\n",
      "Epoch 00072: val_loss improved from 0.32854 to 0.30263, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2500 - accuracy: 0.9137 - val_loss: 0.3026 - val_accuracy: 0.9031\n",
      "Epoch 73/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2323 - accuracy: 0.9172\n",
      "Epoch 00073: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2340 - accuracy: 0.9167 - val_loss: 0.3302 - val_accuracy: 0.8922\n",
      "Epoch 74/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2424 - accuracy: 0.9116\n",
      "Epoch 00074: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2418 - accuracy: 0.9125 - val_loss: 0.3161 - val_accuracy: 0.9018\n",
      "Epoch 75/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2321 - accuracy: 0.9196\n",
      "Epoch 00075: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2324 - accuracy: 0.9191 - val_loss: 0.3438 - val_accuracy: 0.8881\n",
      "Epoch 76/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2308 - accuracy: 0.9200\n",
      "Epoch 00076: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2314 - accuracy: 0.9193 - val_loss: 0.3573 - val_accuracy: 0.8888\n",
      "Epoch 77/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2393 - accuracy: 0.9148\n",
      "Epoch 00077: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2406 - accuracy: 0.9152 - val_loss: 0.3380 - val_accuracy: 0.8922\n",
      "Epoch 78/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2254 - accuracy: 0.9191\n",
      "Epoch 00078: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2253 - accuracy: 0.9195 - val_loss: 0.3104 - val_accuracy: 0.9038\n",
      "Epoch 79/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2413 - accuracy: 0.9191\n",
      "Epoch 00079: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2427 - accuracy: 0.9179 - val_loss: 0.3030 - val_accuracy: 0.9079\n",
      "Epoch 80/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2265 - accuracy: 0.9209\n",
      "Epoch 00080: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2267 - accuracy: 0.9207 - val_loss: 0.3423 - val_accuracy: 0.8922\n",
      "Epoch 81/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2250 - accuracy: 0.9204\n",
      "Epoch 00081: val_loss did not improve from 0.30263\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2301 - accuracy: 0.9190 - val_loss: 0.3352 - val_accuracy: 0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2179 - accuracy: 0.9235\n",
      "Epoch 00082: val_loss improved from 0.30263 to 0.28800, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2182 - accuracy: 0.9239 - val_loss: 0.2880 - val_accuracy: 0.9045\n",
      "Epoch 83/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2103 - accuracy: 0.9273\n",
      "Epoch 00083: val_loss did not improve from 0.28800\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2103 - accuracy: 0.9268 - val_loss: 0.3301 - val_accuracy: 0.8943\n",
      "Epoch 84/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2097 - accuracy: 0.9249\n",
      "Epoch 00084: val_loss improved from 0.28800 to 0.28192, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2091 - accuracy: 0.9258 - val_loss: 0.2819 - val_accuracy: 0.9072\n",
      "Epoch 85/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2075 - accuracy: 0.9299\n",
      "Epoch 00085: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2054 - accuracy: 0.9302 - val_loss: 0.3003 - val_accuracy: 0.9038\n",
      "Epoch 86/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1907 - accuracy: 0.9360\n",
      "Epoch 00086: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1920 - accuracy: 0.9348 - val_loss: 0.2838 - val_accuracy: 0.9120\n",
      "Epoch 87/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1874 - accuracy: 0.9336\n",
      "Epoch 00087: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1883 - accuracy: 0.9328 - val_loss: 0.3263 - val_accuracy: 0.8963\n",
      "Epoch 88/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1894 - accuracy: 0.9336\n",
      "Epoch 00088: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1939 - accuracy: 0.9321 - val_loss: 0.2821 - val_accuracy: 0.9127\n",
      "Epoch 89/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2035 - accuracy: 0.9319\n",
      "Epoch 00089: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2021 - accuracy: 0.9323 - val_loss: 0.2864 - val_accuracy: 0.9141\n",
      "Epoch 90/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2006 - accuracy: 0.9308\n",
      "Epoch 00090: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1996 - accuracy: 0.9309 - val_loss: 0.3033 - val_accuracy: 0.9038\n",
      "Epoch 91/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1975 - accuracy: 0.9289\n",
      "Epoch 00091: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1943 - accuracy: 0.9306 - val_loss: 0.3325 - val_accuracy: 0.8956\n",
      "Epoch 92/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1914 - accuracy: 0.9317\n",
      "Epoch 00092: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1967 - accuracy: 0.9300 - val_loss: 0.3141 - val_accuracy: 0.9045\n",
      "Epoch 93/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2009 - accuracy: 0.9302\n",
      "Epoch 00093: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1951 - accuracy: 0.9329 - val_loss: 0.2947 - val_accuracy: 0.9065\n",
      "Epoch 94/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1740 - accuracy: 0.9390\n",
      "Epoch 00094: val_loss did not improve from 0.28192\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1771 - accuracy: 0.9374 - val_loss: 0.2898 - val_accuracy: 0.9059\n",
      "Epoch 95/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1871 - accuracy: 0.9342\n",
      "Epoch 00095: val_loss improved from 0.28192 to 0.27574, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1869 - accuracy: 0.9341 - val_loss: 0.2757 - val_accuracy: 0.9154\n",
      "Epoch 96/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1854 - accuracy: 0.9368\n",
      "Epoch 00096: val_loss improved from 0.27574 to 0.26962, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1830 - accuracy: 0.9372 - val_loss: 0.2696 - val_accuracy: 0.9100\n",
      "Epoch 97/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1687 - accuracy: 0.9414\n",
      "Epoch 00097: val_loss improved from 0.26962 to 0.25612, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1681 - accuracy: 0.9420 - val_loss: 0.2561 - val_accuracy: 0.9195\n",
      "Epoch 98/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1694 - accuracy: 0.9416\n",
      "Epoch 00098: val_loss did not improve from 0.25612\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1713 - accuracy: 0.9411 - val_loss: 0.2890 - val_accuracy: 0.9106\n",
      "Epoch 99/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1837 - accuracy: 0.9359\n",
      "Epoch 00099: val_loss did not improve from 0.25612\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1855 - accuracy: 0.9343 - val_loss: 0.3037 - val_accuracy: 0.9072\n",
      "Epoch 100/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1738 - accuracy: 0.9388\n",
      "Epoch 00100: val_loss did not improve from 0.25612\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1775 - accuracy: 0.9374 - val_loss: 0.2841 - val_accuracy: 0.9127\n",
      "Epoch 101/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1646 - accuracy: 0.9435\n",
      "Epoch 00101: val_loss did not improve from 0.25612\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1621 - accuracy: 0.9437 - val_loss: 0.2885 - val_accuracy: 0.9113\n",
      "Epoch 102/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1702 - accuracy: 0.9442\n",
      "Epoch 00102: val_loss did not improve from 0.25612\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1698 - accuracy: 0.9447 - val_loss: 0.2657 - val_accuracy: 0.9161\n",
      "Epoch 103/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.9396\n",
      "Epoch 00103: val_loss did not improve from 0.25612\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1722 - accuracy: 0.9401 - val_loss: 0.2666 - val_accuracy: 0.9147\n",
      "Epoch 104/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1619 - accuracy: 0.9451\n",
      "Epoch 00104: val_loss improved from 0.25612 to 0.25379, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1601 - accuracy: 0.9454 - val_loss: 0.2538 - val_accuracy: 0.9188\n",
      "Epoch 105/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1607 - accuracy: 0.9440\n",
      "Epoch 00105: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1637 - accuracy: 0.9427 - val_loss: 0.2678 - val_accuracy: 0.9168\n",
      "Epoch 106/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1695 - accuracy: 0.9407\n",
      "Epoch 00106: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1710 - accuracy: 0.9401 - val_loss: 0.3208 - val_accuracy: 0.9038\n",
      "Epoch 107/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1665 - accuracy: 0.9402\n",
      "Epoch 00107: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1653 - accuracy: 0.9418 - val_loss: 0.2734 - val_accuracy: 0.9175\n",
      "Epoch 108/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1609 - accuracy: 0.9451\n",
      "Epoch 00108: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1624 - accuracy: 0.9437 - val_loss: 0.2992 - val_accuracy: 0.9059\n",
      "Epoch 109/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1577 - accuracy: 0.9446\n",
      "Epoch 00109: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1578 - accuracy: 0.9439 - val_loss: 0.3015 - val_accuracy: 0.9147\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1467 - accuracy: 0.9461\n",
      "Epoch 00110: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1510 - accuracy: 0.9456 - val_loss: 0.2730 - val_accuracy: 0.9202\n",
      "Epoch 111/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9488\n",
      "Epoch 00111: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1461 - accuracy: 0.9488 - val_loss: 0.2724 - val_accuracy: 0.9202\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1377 - accuracy: 0.9496\n",
      "Epoch 00112: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1379 - accuracy: 0.9500 - val_loss: 0.2694 - val_accuracy: 0.9195\n",
      "Epoch 113/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1460 - accuracy: 0.9485\n",
      "Epoch 00113: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1442 - accuracy: 0.9493 - val_loss: 0.2639 - val_accuracy: 0.9141\n",
      "Epoch 114/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1416 - accuracy: 0.9529\n",
      "Epoch 00114: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1442 - accuracy: 0.9509 - val_loss: 0.2733 - val_accuracy: 0.9127\n",
      "Epoch 115/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1496 - accuracy: 0.9467\n",
      "Epoch 00115: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1465 - accuracy: 0.9490 - val_loss: 0.2593 - val_accuracy: 0.9161\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1489 - accuracy: 0.9494\n",
      "Epoch 00116: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1462 - accuracy: 0.9492 - val_loss: 0.2669 - val_accuracy: 0.9141\n",
      "Epoch 117/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1495 - accuracy: 0.9503\n",
      "Epoch 00117: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1468 - accuracy: 0.9507 - val_loss: 0.2579 - val_accuracy: 0.9209\n",
      "Epoch 118/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9472\n",
      "Epoch 00118: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1481 - accuracy: 0.9457 - val_loss: 0.2761 - val_accuracy: 0.9236\n",
      "Epoch 119/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1465 - accuracy: 0.9487\n",
      "Epoch 00119: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1454 - accuracy: 0.9492 - val_loss: 0.2896 - val_accuracy: 0.9072\n",
      "Epoch 120/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1358 - accuracy: 0.9509\n",
      "Epoch 00120: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1379 - accuracy: 0.9503 - val_loss: 0.2693 - val_accuracy: 0.9209\n",
      "Epoch 121/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1251 - accuracy: 0.9570\n",
      "Epoch 00121: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1252 - accuracy: 0.9563 - val_loss: 0.3036 - val_accuracy: 0.9059\n",
      "Epoch 122/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1403 - accuracy: 0.9486\n",
      "Epoch 00122: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1396 - accuracy: 0.9498 - val_loss: 0.2689 - val_accuracy: 0.9195\n",
      "Epoch 123/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1241 - accuracy: 0.9576\n",
      "Epoch 00123: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1223 - accuracy: 0.9579 - val_loss: 0.2688 - val_accuracy: 0.9209\n",
      "Epoch 124/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1362 - accuracy: 0.9528\n",
      "Epoch 00124: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1388 - accuracy: 0.9527 - val_loss: 0.2911 - val_accuracy: 0.9120\n",
      "Epoch 125/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1454 - accuracy: 0.9461\n",
      "Epoch 00125: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1436 - accuracy: 0.9469 - val_loss: 0.2678 - val_accuracy: 0.9168\n",
      "Epoch 126/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1300 - accuracy: 0.9518\n",
      "Epoch 00126: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1314 - accuracy: 0.9512 - val_loss: 0.3482 - val_accuracy: 0.8943\n",
      "Epoch 127/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1311 - accuracy: 0.9526\n",
      "Epoch 00127: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1327 - accuracy: 0.9517 - val_loss: 0.2631 - val_accuracy: 0.9209\n",
      "Epoch 128/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1175 - accuracy: 0.9568\n",
      "Epoch 00128: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1197 - accuracy: 0.9562 - val_loss: 0.2696 - val_accuracy: 0.9188\n",
      "Epoch 129/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1272 - accuracy: 0.9548\n",
      "Epoch 00129: val_loss did not improve from 0.25379\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1272 - accuracy: 0.9546 - val_loss: 0.2650 - val_accuracy: 0.9175\n",
      "Epoch 130/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1187 - accuracy: 0.9576\n",
      "Epoch 00130: val_loss improved from 0.25379 to 0.24901, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1182 - accuracy: 0.9584 - val_loss: 0.2490 - val_accuracy: 0.9250\n",
      "Epoch 131/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9611\n",
      "Epoch 00131: val_loss did not improve from 0.24901\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1118 - accuracy: 0.9608 - val_loss: 0.2618 - val_accuracy: 0.9216\n",
      "Epoch 132/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1231 - accuracy: 0.9572\n",
      "Epoch 00132: val_loss did not improve from 0.24901\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1210 - accuracy: 0.9580 - val_loss: 0.2746 - val_accuracy: 0.9188\n",
      "Epoch 133/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1170 - accuracy: 0.9600\n",
      "Epoch 00133: val_loss improved from 0.24901 to 0.24385, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1163 - accuracy: 0.9592 - val_loss: 0.2439 - val_accuracy: 0.9263\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9602\n",
      "Epoch 00134: val_loss improved from 0.24385 to 0.23766, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1118 - accuracy: 0.9602 - val_loss: 0.2377 - val_accuracy: 0.9222\n",
      "Epoch 135/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1112 - accuracy: 0.9613\n",
      "Epoch 00135: val_loss did not improve from 0.23766\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1115 - accuracy: 0.9608 - val_loss: 0.2766 - val_accuracy: 0.9181\n",
      "Epoch 136/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1096 - accuracy: 0.9589\n",
      "Epoch 00136: val_loss did not improve from 0.23766\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1100 - accuracy: 0.9585 - val_loss: 0.2377 - val_accuracy: 0.9311\n",
      "Epoch 137/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1178 - accuracy: 0.9572\n",
      "Epoch 00137: val_loss improved from 0.23766 to 0.23755, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1240 - accuracy: 0.9543 - val_loss: 0.2375 - val_accuracy: 0.9297\n",
      "Epoch 138/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1186 - accuracy: 0.9583\n",
      "Epoch 00138: val_loss did not improve from 0.23755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1171 - accuracy: 0.9592 - val_loss: 0.2547 - val_accuracy: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1092 - accuracy: 0.9604\n",
      "Epoch 00139: val_loss did not improve from 0.23755\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1072 - accuracy: 0.9613 - val_loss: 0.2754 - val_accuracy: 0.9161\n",
      "Epoch 140/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1074 - accuracy: 0.9630\n",
      "Epoch 00140: val_loss improved from 0.23755 to 0.23480, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.1047 - accuracy: 0.9640 - val_loss: 0.2348 - val_accuracy: 0.9291\n",
      "Epoch 141/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0999 - accuracy: 0.9658\n",
      "Epoch 00141: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1035 - accuracy: 0.9642 - val_loss: 0.2604 - val_accuracy: 0.9284\n",
      "Epoch 142/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0950 - accuracy: 0.9660\n",
      "Epoch 00142: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0964 - accuracy: 0.9652 - val_loss: 0.2525 - val_accuracy: 0.9243\n",
      "Epoch 143/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1008 - accuracy: 0.9652\n",
      "Epoch 00143: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0998 - accuracy: 0.9650 - val_loss: 0.2369 - val_accuracy: 0.9263\n",
      "Epoch 144/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0988 - accuracy: 0.9648\n",
      "Epoch 00144: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0967 - accuracy: 0.9657 - val_loss: 0.2586 - val_accuracy: 0.9256\n",
      "Epoch 145/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9626\n",
      "Epoch 00145: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1084 - accuracy: 0.9620 - val_loss: 0.2633 - val_accuracy: 0.9236\n",
      "Epoch 146/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1061 - accuracy: 0.9622\n",
      "Epoch 00146: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1063 - accuracy: 0.9626 - val_loss: 0.2595 - val_accuracy: 0.9209\n",
      "Epoch 147/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1018 - accuracy: 0.9654\n",
      "Epoch 00147: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1018 - accuracy: 0.9650 - val_loss: 0.2427 - val_accuracy: 0.9263\n",
      "Epoch 148/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1091 - accuracy: 0.9605\n",
      "Epoch 00148: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1069 - accuracy: 0.9620 - val_loss: 0.2683 - val_accuracy: 0.9270\n",
      "Epoch 149/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1024 - accuracy: 0.9672\n",
      "Epoch 00149: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1016 - accuracy: 0.9666 - val_loss: 0.2630 - val_accuracy: 0.9270\n",
      "Epoch 150/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.1027 - accuracy: 0.9623\n",
      "Epoch 00150: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.1029 - accuracy: 0.9623 - val_loss: 0.2403 - val_accuracy: 0.9318\n",
      "Epoch 151/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0998 - accuracy: 0.9664\n",
      "Epoch 00151: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0985 - accuracy: 0.9671 - val_loss: 0.2405 - val_accuracy: 0.9345\n",
      "Epoch 152/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0947 - accuracy: 0.9654\n",
      "Epoch 00152: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0958 - accuracy: 0.9654 - val_loss: 0.2602 - val_accuracy: 0.9263\n",
      "Epoch 153/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1011 - accuracy: 0.9630\n",
      "Epoch 00153: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1014 - accuracy: 0.9628 - val_loss: 0.2751 - val_accuracy: 0.9229\n",
      "Epoch 154/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.1127 - accuracy: 0.9613\n",
      "Epoch 00154: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1149 - accuracy: 0.9602 - val_loss: 0.2617 - val_accuracy: 0.9195\n",
      "Epoch 155/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0977 - accuracy: 0.9663\n",
      "Epoch 00155: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0971 - accuracy: 0.9660 - val_loss: 0.2371 - val_accuracy: 0.9304\n",
      "Epoch 156/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0892 - accuracy: 0.9691\n",
      "Epoch 00156: val_loss did not improve from 0.23480\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0888 - accuracy: 0.9693 - val_loss: 0.2374 - val_accuracy: 0.9325\n",
      "Epoch 157/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0844 - accuracy: 0.9704\n",
      "Epoch 00157: val_loss improved from 0.23480 to 0.23117, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0845 - accuracy: 0.9703 - val_loss: 0.2312 - val_accuracy: 0.9345\n",
      "Epoch 158/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0877 - accuracy: 0.9695\n",
      "Epoch 00158: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0898 - accuracy: 0.9688 - val_loss: 0.2509 - val_accuracy: 0.9256\n",
      "Epoch 159/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0809 - accuracy: 0.9695\n",
      "Epoch 00159: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0826 - accuracy: 0.9693 - val_loss: 0.2453 - val_accuracy: 0.9277\n",
      "Epoch 160/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0815 - accuracy: 0.9727\n",
      "Epoch 00160: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0819 - accuracy: 0.9725 - val_loss: 0.2346 - val_accuracy: 0.9291\n",
      "Epoch 161/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0887 - accuracy: 0.9676\n",
      "Epoch 00161: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0892 - accuracy: 0.9678 - val_loss: 0.2509 - val_accuracy: 0.9256\n",
      "Epoch 162/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0913 - accuracy: 0.9693\n",
      "Epoch 00162: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0941 - accuracy: 0.9688 - val_loss: 0.2526 - val_accuracy: 0.9325\n",
      "Epoch 163/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0844 - accuracy: 0.9689\n",
      "Epoch 00163: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0865 - accuracy: 0.9688 - val_loss: 0.2488 - val_accuracy: 0.9284\n",
      "Epoch 164/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0863 - accuracy: 0.9705\n",
      "Epoch 00164: val_loss did not improve from 0.23117\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0840 - accuracy: 0.9717 - val_loss: 0.2604 - val_accuracy: 0.9263\n",
      "Epoch 165/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0856 - accuracy: 0.9695\n",
      "Epoch 00165: val_loss improved from 0.23117 to 0.22714, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 0.0868 - accuracy: 0.9693 - val_loss: 0.2271 - val_accuracy: 0.9332\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0858 - accuracy: 0.9691\n",
      "Epoch 00166: val_loss improved from 0.22714 to 0.22688, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0840 - accuracy: 0.9696 - val_loss: 0.2269 - val_accuracy: 0.9325\n",
      "Epoch 167/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0887 - accuracy: 0.9701\n",
      "Epoch 00167: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0885 - accuracy: 0.9698 - val_loss: 0.2364 - val_accuracy: 0.9277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0807 - accuracy: 0.9738\n",
      "Epoch 00168: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0818 - accuracy: 0.9734 - val_loss: 0.2659 - val_accuracy: 0.9236\n",
      "Epoch 169/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0883 - accuracy: 0.9676\n",
      "Epoch 00169: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0869 - accuracy: 0.9684 - val_loss: 0.2991 - val_accuracy: 0.9256\n",
      "Epoch 170/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0793 - accuracy: 0.9725\n",
      "Epoch 00170: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0812 - accuracy: 0.9713 - val_loss: 0.2394 - val_accuracy: 0.9304\n",
      "Epoch 171/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0775 - accuracy: 0.9736\n",
      "Epoch 00171: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0780 - accuracy: 0.9737 - val_loss: 0.3155 - val_accuracy: 0.9209\n",
      "Epoch 172/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0934 - accuracy: 0.9669\n",
      "Epoch 00172: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0960 - accuracy: 0.9666 - val_loss: 0.2525 - val_accuracy: 0.9277\n",
      "Epoch 173/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0866 - accuracy: 0.9688\n",
      "Epoch 00173: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0852 - accuracy: 0.9693 - val_loss: 0.2411 - val_accuracy: 0.9284\n",
      "Epoch 174/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0858 - accuracy: 0.9712\n",
      "Epoch 00174: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0858 - accuracy: 0.9708 - val_loss: 0.2390 - val_accuracy: 0.9297\n",
      "Epoch 175/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0792 - accuracy: 0.9708\n",
      "Epoch 00175: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0788 - accuracy: 0.9701 - val_loss: 0.2656 - val_accuracy: 0.9263\n",
      "Epoch 176/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0763 - accuracy: 0.9725\n",
      "Epoch 00176: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0789 - accuracy: 0.9720 - val_loss: 0.2412 - val_accuracy: 0.9256\n",
      "Epoch 177/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0823 - accuracy: 0.9715\n",
      "Epoch 00177: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0825 - accuracy: 0.9715 - val_loss: 0.2912 - val_accuracy: 0.9229\n",
      "Epoch 178/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0734 - accuracy: 0.9734\n",
      "Epoch 00178: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0743 - accuracy: 0.9739 - val_loss: 0.2565 - val_accuracy: 0.9256\n",
      "Epoch 179/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0709 - accuracy: 0.9741\n",
      "Epoch 00179: val_loss did not improve from 0.22688\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0716 - accuracy: 0.9741 - val_loss: 0.2284 - val_accuracy: 0.9325\n",
      "Epoch 180/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0673 - accuracy: 0.9767\n",
      "Epoch 00180: val_loss improved from 0.22688 to 0.22212, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0676 - accuracy: 0.9759 - val_loss: 0.2221 - val_accuracy: 0.9304\n",
      "Epoch 181/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0726 - accuracy: 0.9740\n",
      "Epoch 00181: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0731 - accuracy: 0.9747 - val_loss: 0.2377 - val_accuracy: 0.9332\n",
      "Epoch 182/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0728 - accuracy: 0.9749\n",
      "Epoch 00182: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0719 - accuracy: 0.9751 - val_loss: 0.2737 - val_accuracy: 0.9243\n",
      "Epoch 183/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0866 - accuracy: 0.9701\n",
      "Epoch 00183: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0862 - accuracy: 0.9700 - val_loss: 0.2986 - val_accuracy: 0.9202\n",
      "Epoch 184/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0799 - accuracy: 0.9691\n",
      "Epoch 00184: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0781 - accuracy: 0.9703 - val_loss: 0.2873 - val_accuracy: 0.9202\n",
      "Epoch 185/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0623 - accuracy: 0.9766\n",
      "Epoch 00185: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0618 - accuracy: 0.9771 - val_loss: 0.2315 - val_accuracy: 0.9318\n",
      "Epoch 186/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0640 - accuracy: 0.9786\n",
      "Epoch 00186: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0653 - accuracy: 0.9780 - val_loss: 0.2685 - val_accuracy: 0.9154\n",
      "Epoch 187/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0711 - accuracy: 0.9732\n",
      "Epoch 00187: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0717 - accuracy: 0.9736 - val_loss: 0.2830 - val_accuracy: 0.9216\n",
      "Epoch 188/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0633 - accuracy: 0.9782\n",
      "Epoch 00188: val_loss did not improve from 0.22212\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0641 - accuracy: 0.9782 - val_loss: 0.2661 - val_accuracy: 0.9263\n",
      "Epoch 189/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0537 - accuracy: 0.9828\n",
      "Epoch 00189: val_loss improved from 0.22212 to 0.21301, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0550 - accuracy: 0.9814 - val_loss: 0.2130 - val_accuracy: 0.9393\n",
      "Epoch 190/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0664 - accuracy: 0.9760\n",
      "Epoch 00190: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0672 - accuracy: 0.9763 - val_loss: 0.2475 - val_accuracy: 0.9338\n",
      "Epoch 191/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0671 - accuracy: 0.9775\n",
      "Epoch 00191: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0671 - accuracy: 0.9773 - val_loss: 0.2247 - val_accuracy: 0.9352\n",
      "Epoch 192/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0589 - accuracy: 0.9795\n",
      "Epoch 00192: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0622 - accuracy: 0.9790 - val_loss: 0.2723 - val_accuracy: 0.9236\n",
      "Epoch 193/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0784 - accuracy: 0.9747\n",
      "Epoch 00193: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0795 - accuracy: 0.9739 - val_loss: 0.2675 - val_accuracy: 0.9270\n",
      "Epoch 194/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0626 - accuracy: 0.9756\n",
      "Epoch 00194: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0618 - accuracy: 0.9766 - val_loss: 0.2839 - val_accuracy: 0.9277\n",
      "Epoch 195/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0555 - accuracy: 0.9814\n",
      "Epoch 00195: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0570 - accuracy: 0.9807 - val_loss: 0.2491 - val_accuracy: 0.9291\n",
      "Epoch 196/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0589 - accuracy: 0.9784\n",
      "Epoch 00196: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0581 - accuracy: 0.9782 - val_loss: 0.2405 - val_accuracy: 0.9352\n",
      "Epoch 197/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0636 - accuracy: 0.9781\n",
      "Epoch 00197: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0615 - accuracy: 0.9787 - val_loss: 0.2448 - val_accuracy: 0.9386\n",
      "Epoch 198/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0670 - accuracy: 0.9779\n",
      "Epoch 00198: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0681 - accuracy: 0.9775 - val_loss: 0.2577 - val_accuracy: 0.9297\n",
      "Epoch 199/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0672 - accuracy: 0.9781\n",
      "Epoch 00199: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0671 - accuracy: 0.9783 - val_loss: 0.2619 - val_accuracy: 0.9297\n",
      "Epoch 200/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0676 - accuracy: 0.9754\n",
      "Epoch 00200: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0683 - accuracy: 0.9751 - val_loss: 0.2576 - val_accuracy: 0.9236\n",
      "Epoch 201/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0664 - accuracy: 0.9766\n",
      "Epoch 00201: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0698 - accuracy: 0.9756 - val_loss: 0.2609 - val_accuracy: 0.9311\n",
      "Epoch 202/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0623 - accuracy: 0.9783\n",
      "Epoch 00202: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0614 - accuracy: 0.9790 - val_loss: 0.2530 - val_accuracy: 0.9318\n",
      "Epoch 203/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0577 - accuracy: 0.9801\n",
      "Epoch 00203: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0577 - accuracy: 0.9799 - val_loss: 0.2718 - val_accuracy: 0.9332\n",
      "Epoch 204/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0634 - accuracy: 0.9771\n",
      "Epoch 00204: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0657 - accuracy: 0.9773 - val_loss: 0.2770 - val_accuracy: 0.9284\n",
      "Epoch 205/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0605 - accuracy: 0.9786\n",
      "Epoch 00205: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0598 - accuracy: 0.9788 - val_loss: 0.2497 - val_accuracy: 0.9297\n",
      "Epoch 206/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0595 - accuracy: 0.9785\n",
      "Epoch 00206: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0602 - accuracy: 0.9773 - val_loss: 0.2143 - val_accuracy: 0.9407\n",
      "Epoch 207/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0595 - accuracy: 0.9792\n",
      "Epoch 00207: val_loss did not improve from 0.21301\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0574 - accuracy: 0.9802 - val_loss: 0.2626 - val_accuracy: 0.9318\n",
      "Epoch 208/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0574 - accuracy: 0.9810\n",
      "Epoch 00208: val_loss improved from 0.21301 to 0.20914, saving model to models/saved_models/best_models_DP_0.1.hdf5\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.0580 - accuracy: 0.9812 - val_loss: 0.2091 - val_accuracy: 0.9441\n",
      "Epoch 209/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0574 - accuracy: 0.9790\n",
      "Epoch 00209: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0599 - accuracy: 0.9785 - val_loss: 0.2567 - val_accuracy: 0.9338\n",
      "Epoch 210/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0707 - accuracy: 0.9758\n",
      "Epoch 00210: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0696 - accuracy: 0.9759 - val_loss: 0.2823 - val_accuracy: 0.9270\n",
      "Epoch 211/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0549 - accuracy: 0.9808\n",
      "Epoch 00211: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0563 - accuracy: 0.9807 - val_loss: 0.2316 - val_accuracy: 0.9359\n",
      "Epoch 212/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0691 - accuracy: 0.9771\n",
      "Epoch 00212: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0702 - accuracy: 0.9761 - val_loss: 0.2674 - val_accuracy: 0.9277\n",
      "Epoch 213/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0640 - accuracy: 0.9777\n",
      "Epoch 00213: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0648 - accuracy: 0.9775 - val_loss: 0.2414 - val_accuracy: 0.9325\n",
      "Epoch 214/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0591 - accuracy: 0.9784\n",
      "Epoch 00214: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0581 - accuracy: 0.9783 - val_loss: 0.2417 - val_accuracy: 0.9379\n",
      "Epoch 215/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0522 - accuracy: 0.9823\n",
      "Epoch 00215: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0570 - accuracy: 0.9809 - val_loss: 0.2572 - val_accuracy: 0.9284\n",
      "Epoch 216/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0512 - accuracy: 0.9814\n",
      "Epoch 00216: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0528 - accuracy: 0.9811 - val_loss: 0.2687 - val_accuracy: 0.9270\n",
      "Epoch 217/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0553 - accuracy: 0.9812\n",
      "Epoch 00217: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0540 - accuracy: 0.9811 - val_loss: 0.2373 - val_accuracy: 0.9400\n",
      "Epoch 218/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0532 - accuracy: 0.9801\n",
      "Epoch 00218: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0550 - accuracy: 0.9792 - val_loss: 0.2409 - val_accuracy: 0.9318\n",
      "Epoch 219/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0600 - accuracy: 0.9792\n",
      "Epoch 00219: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0582 - accuracy: 0.9797 - val_loss: 0.2411 - val_accuracy: 0.9352\n",
      "Epoch 220/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0535 - accuracy: 0.9836\n",
      "Epoch 00220: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0540 - accuracy: 0.9831 - val_loss: 0.2235 - val_accuracy: 0.9420\n",
      "Epoch 221/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0555 - accuracy: 0.9788\n",
      "Epoch 00221: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0549 - accuracy: 0.9788 - val_loss: 0.2553 - val_accuracy: 0.9332\n",
      "Epoch 222/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0506 - accuracy: 0.9833\n",
      "Epoch 00222: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0496 - accuracy: 0.9836 - val_loss: 0.2532 - val_accuracy: 0.9352\n",
      "Epoch 223/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0525 - accuracy: 0.9810\n",
      "Epoch 00223: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0501 - accuracy: 0.9823 - val_loss: 0.2420 - val_accuracy: 0.9447\n",
      "Epoch 224/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0509 - accuracy: 0.9807\n",
      "Epoch 00224: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0499 - accuracy: 0.9811 - val_loss: 0.2646 - val_accuracy: 0.9297\n",
      "Epoch 225/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0531 - accuracy: 0.9808\n",
      "Epoch 00225: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0520 - accuracy: 0.9812 - val_loss: 0.2199 - val_accuracy: 0.9407\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0477 - accuracy: 0.9840\n",
      "Epoch 00226: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0464 - accuracy: 0.9845 - val_loss: 0.2557 - val_accuracy: 0.9325\n",
      "Epoch 227/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0467 - accuracy: 0.9840\n",
      "Epoch 00227: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0457 - accuracy: 0.9841 - val_loss: 0.2466 - val_accuracy: 0.9359\n",
      "Epoch 228/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0454 - accuracy: 0.9825\n",
      "Epoch 00228: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0458 - accuracy: 0.9824 - val_loss: 0.2533 - val_accuracy: 0.9359\n",
      "Epoch 229/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0571 - accuracy: 0.9807\n",
      "Epoch 00229: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0584 - accuracy: 0.9802 - val_loss: 0.2564 - val_accuracy: 0.9372\n",
      "Epoch 230/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0544 - accuracy: 0.9791\n",
      "Epoch 00230: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.9799 - val_loss: 0.2259 - val_accuracy: 0.9393\n",
      "Epoch 231/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0566 - accuracy: 0.9805\n",
      "Epoch 00231: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.2519 - val_accuracy: 0.9332\n",
      "Epoch 232/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0489 - accuracy: 0.9833\n",
      "Epoch 00232: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0467 - accuracy: 0.9841 - val_loss: 0.2289 - val_accuracy: 0.9427\n",
      "Epoch 233/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0490 - accuracy: 0.9840\n",
      "Epoch 00233: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0484 - accuracy: 0.9843 - val_loss: 0.2423 - val_accuracy: 0.9332\n",
      "Epoch 234/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0508 - accuracy: 0.9810\n",
      "Epoch 00234: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0501 - accuracy: 0.9812 - val_loss: 0.2712 - val_accuracy: 0.9332\n",
      "Epoch 235/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0485 - accuracy: 0.9821\n",
      "Epoch 00235: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0478 - accuracy: 0.9824 - val_loss: 0.2530 - val_accuracy: 0.9386\n",
      "Epoch 236/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0528 - accuracy: 0.9821\n",
      "Epoch 00236: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0540 - accuracy: 0.9821 - val_loss: 0.2360 - val_accuracy: 0.9407\n",
      "Epoch 237/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0419 - accuracy: 0.9838\n",
      "Epoch 00237: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0419 - accuracy: 0.9838 - val_loss: 0.2246 - val_accuracy: 0.9372\n",
      "Epoch 238/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0427 - accuracy: 0.9834\n",
      "Epoch 00238: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0426 - accuracy: 0.9836 - val_loss: 0.2378 - val_accuracy: 0.9372\n",
      "Epoch 239/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0529 - accuracy: 0.9812\n",
      "Epoch 00239: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 0.2653 - val_accuracy: 0.9359\n",
      "Epoch 240/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0554 - accuracy: 0.9794\n",
      "Epoch 00240: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0535 - accuracy: 0.9799 - val_loss: 0.2699 - val_accuracy: 0.9250\n",
      "Epoch 241/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0600 - accuracy: 0.9812\n",
      "Epoch 00241: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0584 - accuracy: 0.9817 - val_loss: 0.2344 - val_accuracy: 0.9434\n",
      "Epoch 242/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0528 - accuracy: 0.9816\n",
      "Epoch 00242: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0575 - accuracy: 0.9804 - val_loss: 0.2374 - val_accuracy: 0.9372\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9763\n",
      "Epoch 00243: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0650 - accuracy: 0.9763 - val_loss: 0.2715 - val_accuracy: 0.9372\n",
      "Epoch 244/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0614 - accuracy: 0.9790\n",
      "Epoch 00244: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0612 - accuracy: 0.9788 - val_loss: 0.2379 - val_accuracy: 0.9372\n",
      "Epoch 245/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0546 - accuracy: 0.9824\n",
      "Epoch 00245: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0533 - accuracy: 0.9828 - val_loss: 0.2543 - val_accuracy: 0.9338\n",
      "Epoch 246/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0472 - accuracy: 0.9842\n",
      "Epoch 00246: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0501 - accuracy: 0.9833 - val_loss: 0.2223 - val_accuracy: 0.9441\n",
      "Epoch 247/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 0.9849\n",
      "Epoch 00247: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0467 - accuracy: 0.9845 - val_loss: 0.2543 - val_accuracy: 0.9386\n",
      "Epoch 248/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0454 - accuracy: 0.9840\n",
      "Epoch 00248: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.2447 - val_accuracy: 0.9420\n",
      "Epoch 249/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0342 - accuracy: 0.9871\n",
      "Epoch 00249: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0359 - accuracy: 0.9869 - val_loss: 0.2702 - val_accuracy: 0.9304\n",
      "Epoch 250/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0441 - accuracy: 0.9855\n",
      "Epoch 00250: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0436 - accuracy: 0.9857 - val_loss: 0.2978 - val_accuracy: 0.9297\n",
      "Epoch 251/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0414 - accuracy: 0.9857\n",
      "Epoch 00251: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0406 - accuracy: 0.9858 - val_loss: 0.2618 - val_accuracy: 0.9372\n",
      "Epoch 252/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0438 - accuracy: 0.9851\n",
      "Epoch 00252: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.2724 - val_accuracy: 0.9372\n",
      "Epoch 253/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0497 - accuracy: 0.9830\n",
      "Epoch 00253: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0506 - accuracy: 0.9828 - val_loss: 0.2610 - val_accuracy: 0.9352\n",
      "Epoch 254/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0420 - accuracy: 0.9844\n",
      "Epoch 00254: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0415 - accuracy: 0.9850 - val_loss: 0.2584 - val_accuracy: 0.9359\n",
      "Epoch 255/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.0453 - accuracy: 0.9838\n",
      "Epoch 00255: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0454 - accuracy: 0.9838 - val_loss: 0.2290 - val_accuracy: 0.9372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0397 - accuracy: 0.9857\n",
      "Epoch 00256: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0396 - accuracy: 0.9860 - val_loss: 0.2804 - val_accuracy: 0.9352\n",
      "Epoch 257/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0443 - accuracy: 0.9836\n",
      "Epoch 00257: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0420 - accuracy: 0.9846 - val_loss: 0.2443 - val_accuracy: 0.9359\n",
      "Epoch 258/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 0.9892\n",
      "Epoch 00258: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0348 - accuracy: 0.9894 - val_loss: 0.2916 - val_accuracy: 0.9284\n",
      "Epoch 259/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0417 - accuracy: 0.9851\n",
      "Epoch 00259: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0412 - accuracy: 0.9852 - val_loss: 0.2886 - val_accuracy: 0.9270\n",
      "Epoch 260/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0436 - accuracy: 0.9879\n",
      "Epoch 00260: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0427 - accuracy: 0.9879 - val_loss: 0.2652 - val_accuracy: 0.9332\n",
      "Epoch 261/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0444 - accuracy: 0.9821\n",
      "Epoch 00261: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0427 - accuracy: 0.9828 - val_loss: 0.2544 - val_accuracy: 0.9332\n",
      "Epoch 262/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 0.9872\n",
      "Epoch 00262: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0334 - accuracy: 0.9869 - val_loss: 0.2512 - val_accuracy: 0.9379\n",
      "Epoch 263/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0338 - accuracy: 0.9890\n",
      "Epoch 00263: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0341 - accuracy: 0.9891 - val_loss: 0.2467 - val_accuracy: 0.9407\n",
      "Epoch 264/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0348 - accuracy: 0.9900\n",
      "Epoch 00264: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0343 - accuracy: 0.9901 - val_loss: 0.2655 - val_accuracy: 0.9366\n",
      "Epoch 265/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0469 - accuracy: 0.9833\n",
      "Epoch 00265: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0462 - accuracy: 0.9838 - val_loss: 0.2550 - val_accuracy: 0.9413\n",
      "Epoch 266/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0402 - accuracy: 0.9857\n",
      "Epoch 00266: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0407 - accuracy: 0.9857 - val_loss: 0.2733 - val_accuracy: 0.9332\n",
      "Epoch 267/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0420 - accuracy: 0.9851\n",
      "Epoch 00267: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0430 - accuracy: 0.9852 - val_loss: 0.2496 - val_accuracy: 0.9366\n",
      "Epoch 268/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0440 - accuracy: 0.9842\n",
      "Epoch 00268: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0429 - accuracy: 0.9846 - val_loss: 0.2550 - val_accuracy: 0.9386\n",
      "Epoch 269/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0445 - accuracy: 0.9849\n",
      "Epoch 00269: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0439 - accuracy: 0.9853 - val_loss: 0.2854 - val_accuracy: 0.9325\n",
      "Epoch 270/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0510 - accuracy: 0.9807\n",
      "Epoch 00270: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0500 - accuracy: 0.9816 - val_loss: 0.2515 - val_accuracy: 0.9379\n",
      "Epoch 271/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 0.9842\n",
      "Epoch 00271: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0457 - accuracy: 0.9838 - val_loss: 0.2850 - val_accuracy: 0.9332\n",
      "Epoch 272/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0427 - accuracy: 0.9857\n",
      "Epoch 00272: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0430 - accuracy: 0.9855 - val_loss: 0.2637 - val_accuracy: 0.9332\n",
      "Epoch 273/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.0398 - accuracy: 0.9857\n",
      "Epoch 00273: val_loss did not improve from 0.20914\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0401 - accuracy: 0.9857 - val_loss: 0.2453 - val_accuracy: 0.9407\n",
      "Epoch 00273: early stopping\n",
      "Training for model  4  completed in time:  0:01:58.519345 seconds\n"
     ]
    }
   ],
   "source": [
    "histories, durations = model.train(X_train, X_test, y_train, y_test, num_epochs=500, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES FOR MODEL WITH  1  LAYERS : \n",
      "Training Accuracy:  0.5359153747558594\n",
      "Testing Accuracy:  0.48703956604003906\n",
      "Duration of training:  0:02:36.865201 \n",
      "\n",
      "SCORES FOR MODEL WITH  2  LAYERS : \n",
      "Training Accuracy:  0.7515782117843628\n",
      "Testing Accuracy:  0.7435197830200195\n",
      "Duration of training:  0:01:45.864726 \n",
      "\n",
      "SCORES FOR MODEL WITH  3  LAYERS : \n",
      "Training Accuracy:  0.9619519114494324\n",
      "Testing Accuracy:  0.9079126715660095\n",
      "Duration of training:  0:03:02.526628 \n",
      "\n",
      "SCORES FOR MODEL WITH  4  LAYERS : \n",
      "Training Accuracy:  0.9955639243125916\n",
      "Testing Accuracy:  0.9406548142433167\n",
      "Duration of training:  0:01:58.519345 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'training_score_1': [1.3689161539077759, 0.5359153747558594],\n",
       "  'training_score_2': [0.7255287766456604, 0.7515782117843628],\n",
       "  'training_score_3': [0.10200401395559311, 0.9619519114494324],\n",
       "  'training_score_4': [0.015453903004527092, 0.9955639243125916]},\n",
       " {'testing_score_1': [1.4290132522583008, 0.48703956604003906],\n",
       "  'testing_score_2': [0.773709774017334, 0.7435197830200195],\n",
       "  'testing_score_3': [0.31546688079833984, 0.9079126715660095],\n",
       "  'testing_score_4': [0.2452804148197174, 0.9406548142433167]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Plots for Model  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Plots for Model  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Plots for Model  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Plots for Model  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x450 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.plot_all_histories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 850x510 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.accuracies_vs_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 850x510 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.duration_vs_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}